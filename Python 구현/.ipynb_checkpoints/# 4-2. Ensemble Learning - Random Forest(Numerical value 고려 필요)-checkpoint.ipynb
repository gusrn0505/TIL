{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dff1c11c",
   "metadata": {},
   "source": [
    "# Random Forest \n",
    "\n",
    "### 알고리즘은 강의 자료를 참고하겠음. \n",
    "\n",
    "1. For b =1 to B : \n",
    "- Draw a bootstrap sample $Z^*$ of size N from the training data. \n",
    "- Grow a random-forest tree $T_b$ to the bootstrapped data, by recursively repeating the following steps for each ternimal node of the tree, until the mininum node size $n_{min}$ is reached. \n",
    "\n",
    "> 1). Select m variables at random from the p variables \n",
    "\n",
    "> 2). Pick the best variable/split-point among the m \n",
    "\n",
    "> 3). Split the node into two daughter nodes \n",
    "\n",
    "2. Output the ensemble of trees {$T_b$}$|_1^B$ \n",
    "- To make a prediction at a new point x \n",
    "- Regression : $\\hat f_{rf}^B(x) = \\frac{1}{B} \\sum_{b=1}^B T_b(x)$ \n",
    "- Classification : Let $\\hat C_b(x)$ be the class prediction of the bth random-forest tree. Then $\\hat C_{rf}^B(x)$ = majority vote{$\\hat C_b(x)$}$|_1^B$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4743cdc",
   "metadata": {},
   "source": [
    "**구현해야하는 것**\n",
    "- 사이즈 N인 Bootstrap 샘플 B개 뽑기 \n",
    "- 샘플 데이터 셋 B개에 대해서 Random-forest tree 만들기 \n",
    "- result aggregation 진행. Regression이면 평균 값을, Classification이면 Majority vote 적용 \n",
    "\n",
    "**필요한 것**\n",
    "- M : 랜덤 트리 개수  \n",
    "- m : 선정할 변수 개수\n",
    "- X : input 데이터 \n",
    "- y : Output 데이터 \n",
    "- Treenode \n",
    "\n",
    "**함수의 형태** \n",
    "- def __init__(self, X, y, num_tree, num_var) \n",
    "- def make_tree(self, num_tree, num_var): \n",
    "\n",
    "*Regression 간에는 SVR을, classification 간에는 SVC를 적용하겠음*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a9e0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rand\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "X = load_iris()['data']\n",
    "y = load_iris()[\"target\"].reshape(-1,1)\n",
    "D = np.concatenate((X,y), axis=1)\n",
    "A = load_iris()['feature_names']\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f74ab9",
   "metadata": {},
   "source": [
    "### Decision Tree 수도 알고리즘 \n",
    "\n",
    "\n",
    "**알고리즘 과정**\n",
    "1. 주어진 샘플에 대해서 복원추출을 통해 다수의 Tree에 적용할 샘플들을 형성한다. \n",
    "\n",
    "2. 각 주어진 샘플 별로 고려할 속성 집합을 랜덤하게 선택한다. \n",
    "\n",
    "3. 각 샘플별로 선택한 속성만을 고려하여 깊이 1의 Decision Tree를 통해 데이터를 구분해낸다. \n",
    "\n",
    "4. 분류된 하위 샘플들이 단일 레이블로 분류될 때까지 2~3번 과정을 반복한다. \n",
    "\n",
    "**Input**\n",
    "- data : X와 y가 결합된 데이터\n",
    "- num_tree : 몇 개의 트리를 적용할지\n",
    "- num_var : 데이터 별 한번에 고려할 변수의 개수 \n",
    "- max_depth : DT 적용간 깊이 \n",
    "\n",
    "**구현할 것**\n",
    "- 랜덤 샘플 생성 \n",
    "- 트리 형성 : 주어진 샘플 데이터에 대해 DT 적용하여 하위 샘플들 구분할 것. \n",
    "- 터미널 노드가 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "29759612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_forest(): \n",
    "    def __init__(self, data,  num_tree, num_var, max_depth =1) : \n",
    "        self.data = np.array(data) \n",
    "        self.X = np.array(data[:, :-1])\n",
    "        self.y = np.array(data[:, -1])\n",
    "        self.n = np.shape(self.X)[0]\n",
    "        self.m = np.shape(self.X)[1]\n",
    "        \n",
    "        self.num_tree = num_tree\n",
    "        \n",
    "        self.sample, self.sample_y, self.oob = self.random_sample()\n",
    "        \n",
    "        \n",
    "        self.num_var = num_var \n",
    "        self.max_depth = max_depth \n",
    "        \n",
    "    def random_sample(self) : \n",
    "        sample_list = []\n",
    "        sample_y_list = [] \n",
    "        oob_list = [] \n",
    "    \n",
    "        for i in range(self.num_tree) : \n",
    "            np.random.seed(i)\n",
    "            sample_index = np.random.choice(self.n, size = self.n, replace = True)\n",
    "            sample = self.X[sample_index]\n",
    "            sample_y = self.y[sample_index]\n",
    "            \n",
    "            oob_index = np.array(list(set(range(self.n)) - set(sample_index))) \n",
    "            oob = self.data[oob_index, :]\n",
    "            \n",
    "            sample_list.append(sample)\n",
    "            sample_y_list.append(sample_y)\n",
    "            oob_list.append(oob)\n",
    "\n",
    "        return sample_list, sample_y_list, oob_list\n",
    "    \n",
    "        \n",
    "    def make_tree(self, X, y) : \n",
    "        # X는 np.array 여야 함. \n",
    "        \n",
    "        tree = DecisionTreeClassifier(max_depth = self.max_depth, random_state = 0)\n",
    "        chosen_att = sorted(np.random.choice(range(self.m), self.num_var, replace=False))\n",
    "\n",
    "        if len(np.unique(y)) == 0 : \n",
    "            return None\n",
    "        \n",
    "        if len(np.unique(y)) == 1 :\n",
    "            tree.results = y[0]\n",
    "            return tree\n",
    "        \n",
    "\n",
    "        chosen_X = X[:, chosen_att]\n",
    "        \n",
    "        tree.fit(chosen_X,y) \n",
    "        tree_result= tree.predict(chosen_X)\n",
    "        \n",
    "        left_index = np.where(tree_result == 0)\n",
    "        right_index = np.where(tree_result == 1)\n",
    "        \n",
    "        if len(left_index) > 0 : \n",
    "            tree.left = self.make_tree(X[left_index], y[left_index])\n",
    "        if len(right_index) > 0 : \n",
    "            tree.right = self.make_tree(X[right_index], y[right_index])\n",
    "        \n",
    "        return tree\n",
    "    \n",
    "    \n",
    "    # 재귀함수 형태로 트리를 만들어 자동으로 트리가 형성될 수 있도록 해야함. \n",
    "    # 또한 트리의 마지막 Terminal node에 대해서 weight를 부여하여 최종 값을 생성할 수 있도록 해야함. \n",
    "    def random_forest(self) : \n",
    "        tree_lst = [] \n",
    "        for i in range(self.num_tree) :\n",
    "            sample = self.sample[i]\n",
    "            sample_y = self.sample_y[i]\n",
    "            \n",
    "            tree = self.make_tree(sample, sample_y)\n",
    "            tree_list.append(tree)\n",
    "            \n",
    "            #while len(np.unique(left_y)) > 1 or np.unique(right_y) > 1 :\n",
    "                \n",
    "        \n",
    "    def tree_score(self, tree, x,y) : \n",
    "        results = [] \n",
    "        for row in x : \n",
    "            result.append(tree.predict(row)) \n",
    "        \n",
    "        n_true = 0 \n",
    "        for i in range(len(results)) : \n",
    "            if results[i] == y[i] : \n",
    "                n_true += 1 \n",
    "        return (n_true/len(results))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dcb8182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Random_forest(D, 5, 3)\n",
    "test.random_sample()\n",
    "\n",
    "\n",
    "a = test.make_tree(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1590f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4781b5a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 4 features, but DecisionTreeClassifier is expecting 3 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-475619ab3e66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m    466\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 467\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;34m\"\"\"Validate the training data on predict (probabilities).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             if issparse(X) and (\n\u001b[0;32m    435\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    401\u001b[0m                 \u001b[1;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m                 \u001b[1;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: X has 4 features, but DecisionTreeClassifier is expecting 3 features as input."
     ]
    }
   ],
   "source": [
    "a.predict([[5.1, 3.5, 1.4, 0.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c8043c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고 : 고봉선 님 코드 구현 블로그 \n",
    "class TreeNode:\n",
    "        def __init__(self, col=-1, value=None, results=None, left=None, right=None):\n",
    "            self.col = col          # 분류에 사용된 variable 정보\n",
    "            self.results = results  # 최종 분류값 (for leaf node)\n",
    "            self.left = left   # true인 경우 branch\n",
    "            self.right = right  # false인 경우 branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5472a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진 분류 모델 \n",
    "def bagging(X, x_new, model, T, N, kernel, agg_type) : \n",
    "    sample_list = []\n",
    "    ensemble_list = [] \n",
    "    X1 = np.array([X[i][:-1] for i in range(len(X)) if X[i][-1] == 0]) \n",
    "    X2 = np.array([X[i][:-1] for i in range(len(X)) if X[i][-1] == 1]) \n",
    "    \n",
    "    for i in range(T) : \n",
    "        np.random.seed(i)\n",
    "        sample_index = np.random.choice(np.shape(X1)[0], size = N, replace = True)\n",
    "        sample_1 = X1[sample_index]\n",
    "        sample_2 = X2[sample_index]\n",
    "        ensemble_list.append(model(sample_1,sample_2,kernel)) \n",
    "    \n",
    "    result = [] \n",
    "    for i in range(T) : \n",
    "        result.append(ensemble_list[i].classify(x_new)) \n",
    "    \n",
    "    return aggregation(result, agg_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef328aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregation(result, agg_type) : \n",
    "    \n",
    "    if agg_type == \"Majority\" : \n",
    "        agg_result = argmax([result.count(0), result.count(1)]) \n",
    "        return agg_result\n",
    "    \n",
    "    elif agg_type == \"Accuracy Weighted\" : \n",
    "        return 0\n",
    "        # 모델별 정확도가 구현되어 있지 않아 여기선 생략 \n",
    "    \n",
    "    elif agg_type == \"Prediction Weighted\" : \n",
    "        return 0 \n",
    "        # 모델별 예상 확률이 구현되어 있지 않아 여기선 생략 \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ada1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e24c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a235f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc918d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "abcb39d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5번 항목과 연계하는 과정에서 막힘. \n",
    "# 특정 속성 값의 개수에 맞춰 Child 생성하는데 막힘. \n",
    "\n",
    "class treenode() : \n",
    "    def __init__(self, num_child=None) : \n",
    "        self.label = None\n",
    "        self.child = self.make_child(num_child)\n",
    "        \n",
    "    def make_child(self, num_child) : \n",
    "        for i in range(num_child) : \n",
    "            globals()['self.child_{}'.format(i)] = None\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb198d4",
   "metadata": {},
   "source": [
    "##### 4. 최적의 분할 속성 정하는 방법 : 정보 이득율 사용 \n",
    "- Gain_ratio(D,a) = $\\frac{Gain(D,a)}{IV(a)}$ \n",
    "- IV(a) = - $\\sum_{v=1}^V \\frac{D^v}{D} log2 \\frac{D^v}{D}$\n",
    "- Gain(D,a) = Ent(D) - $\\sum_{v=1}^V \\frac{|D^v|}{|D|} Ent(D^v)$\n",
    "- Ent(D) = -$\\sum_{k=1}^{|Y|}p_k log2(p_k) $\n",
    "> |Y| : 클래스의 종류 개수 \n",
    "> $p_k$ : k번째 클래스의 비율 \n",
    "\n",
    "\n",
    "\n",
    "##### 5. for $a_*$의 각 value에 따라 for 절을 수행할 것. 문제 - Category value가 아니라 Numerical Value로 주어지면 어떻게 분류하지? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc9c26cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#엔트로피 설정\n",
    "\n",
    "from math import log \n",
    "from collections import defaultdict\n",
    "\n",
    "def entropy(dataset) : \n",
    "    datasize = len(dataset) \n",
    "    \n",
    "    label_count = defaultdict(int)\n",
    "    entropy = 0.0 \n",
    "    \n",
    "    for data_line in dataset : \n",
    "        cur_label = data_line[-1]\n",
    "        label_count[cur_label] += 1 \n",
    "        \n",
    "    for key in label_count : \n",
    "        prop = float(label_count[key]) / datasize \n",
    "        entropy -= prop*log(prop,2) \n",
    "    return entropy\n",
    "\n",
    "# Dv, Ent(Dv) 계산하기 위한 토대. 특정 속성값을 제외한 나머지 값들을 결과로 반환 \n",
    "def pick(dataset, index, value) : \n",
    "    result = [] \n",
    "    for dataline in dataset : \n",
    "        dataline = list(dataline)\n",
    "        if dataline[index] == value : \n",
    "            temp_vec = dataline[:index]\n",
    "            temp_vec.extend(dataline[index+1:])\n",
    "            result.append(temp_vec)\n",
    "    return result \n",
    "\n",
    "# 최고의 선별 속성을 결정하는 함수 \n",
    "def best_feat(dataset): \n",
    "    base_entropy = entropy(dataset)\n",
    "    best_infogain = 0.0 \n",
    "    index = 0 \n",
    "    num_feat = len(dataset[0]) -1 \n",
    "    for i in range(num_feat) : \n",
    "        feat_value_set = set([value[i] for value in dataset])\n",
    "        new_entropy = 0.0 \n",
    "        \n",
    "        # 정보 이득 계산하기 \n",
    "        for element in feat_value_set : \n",
    "            sub_dataset = pick(dataset, i, element) \n",
    "            prop = len(sub_dataset) / float(len(dataset))\n",
    "            new_entropy += prop*entropy(sub_dataset)\n",
    "            \n",
    "        info_gain = base_entropy - new_entropy \n",
    "        \n",
    "        if best_infogain < info_gain : \n",
    "            best_infogain = info_gain \n",
    "            index = i \n",
    "    return index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101d02a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c00ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd6973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de222363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a9246a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd75ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
