{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c50eec",
   "metadata": {},
   "source": [
    "# Ensemble Learning - XGBoost \n",
    "\n",
    "### 강의 자료 속 나온 3개의 수도 코드 구현에 집중 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a11da",
   "metadata": {},
   "source": [
    "### Algorithm 1 : Exact Greedy Algorithm for Split Finding \n",
    "\n",
    "Input : I, instance set of current node \n",
    "Input : d, feature dimension \n",
    "\n",
    "1. gain <- 0 \n",
    "2. G $\\leftarrow \\sum_{i \\in I}g_i$, H $\\leftarrow \\sum_{i \\in I} h_i$ \n",
    "3. for k = 1 to m do :\n",
    "- $G_L \\leftarrow 0, H_L \\leftarrow 0$ \n",
    "- for j in sorted(I, by $x_{jk}$) do \n",
    "\n",
    "> $G_L \\leftarrow G_L + g_j, H_L \\leftarrow H_L + h_j$\n",
    "\n",
    "> $G_R \\leftarrow G - G_L, H_R \\leftarrow H - H_L$ \n",
    "\n",
    "> score $\\leftarrow$ max(score, $\\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G^2}{H + \\lambda}) $ \n",
    "\n",
    "> end \n",
    "- end \n",
    "\n",
    "output : Split with max score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1558c4c",
   "metadata": {},
   "source": [
    "**구현해야 하는 것**\n",
    "- Loss function : $l(y_i, \\hat y_i^{t-1} + f_t(x_i))$ \n",
    "- $G_L, G_R$ : 손실함수의 1차 미분의 좌측 합, 1차 미분의 나머지 합 \n",
    "- $H_L, H_R$ : 손실함수의 2차 미분의 좌측 합, 2차 미분의 나머지 합 \n",
    "- $\\lambda$ : $\\rho(f_k) = \\gamma T + \\frac{1}{2} \\lambda ||w||^2$ 에서의 $\\lambda$ 값\n",
    "> $\\gamma, \\lambda$ : Parameter.\n",
    "\n",
    "**필요한 것** \n",
    "- CART Tree : DT Regression 적용하겠음. \n",
    "- Loss 함수 정의 : 여기선 간단한 OLS을 채택하겠음. \n",
    "\n",
    "> g = f(x) - y : 예측값 - 실제값 \n",
    "\n",
    "> h = 1 : 상수 \n",
    "\n",
    "> f(x) = DT Regression으로 구하기.  \n",
    "\n",
    "**함수의 형태** \n",
    "- def __init__(self, X, y, gamma, lambda) : \n",
    "\n",
    "- def altorithm_1(self) : > 모든 Split point 계산하기\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6de1a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "\n",
    "X = load_iris()['data']\n",
    "y = load_iris()[\"target\"]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0c0c4564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 데이터 셋 체크 \n",
    "\n",
    "from sklearn.datasets import load_diabetes \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X,y = load_diabetes(return_X_y = True)\n",
    "train_x, test_x, train_y, test_y = train_test_split(X,y, test_size = 0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c9d17ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoost() : \n",
    "    def __init__(self, X, y, num_repeat,  gamma, lamda) : \n",
    "        self.X = X \n",
    "        self.n = np.shape(X)[0]\n",
    "        self.d = np.shape(X)[1]\n",
    "        \n",
    "        self.y = y \n",
    "        self.num_label = len(np.unique(y))\n",
    "        \n",
    "        self.num_repeat = num_repeat\n",
    "        \n",
    "        self.gamma = gamma \n",
    "        self.lamda = lamda\n",
    "    \n",
    "    \n",
    "    def algorithm_1(self, X, y) : \n",
    "        # gain 변수는 사용하지 않아 Score 변수로 변경하여 사용 \n",
    "        score = 0 \n",
    "        target_value = y\n",
    "    \n",
    "        g_lst = [] \n",
    "        h_lst = [] \n",
    "        \n",
    "        for i in range(self.num_repeat) : \n",
    "            model = tree.DecisionTreeRegressor()\n",
    "            model.fit(self.X, self.y)\n",
    "            pred_y = model.predict(X)\n",
    "            \n",
    "            grad = np.array(pred_y) - np.array(target_value)\n",
    "            h = np.ones(len(X))\n",
    "            g_lst.append(grad)\n",
    "            h_lst.append(h)\n",
    "            target_value = grad \n",
    "        \n",
    "        G = np.sum(g_lst) \n",
    "        H = np.sum(h_lst)\n",
    "        \n",
    "        \n",
    "        split_point = 0 \n",
    "        att = 0\n",
    "        \n",
    "        for k in range(self.d) : \n",
    "            G_left = 0 \n",
    "            H_left = 0 \n",
    "\n",
    "            # j 는 각 특성별 크기에 sorting 된 순서\n",
    "            for j in np.argsort(np.array(X)[:,k]) : \n",
    "            # 각 객체별로 g, h의 값을 더하기. \n",
    "                G_left += np.sum([g_lst[k][j] for k in range(len(g_lst))]) \n",
    "                H_left += np.sum([h_lst[k][j] for k in range(len(h_lst))])\n",
    "                G_right = G - G_left\n",
    "                H_right = H - H_left \n",
    "            \n",
    "                cur_score = (G_left**2 /(H_left + self.lamda)) + (G_right**2/(H_right + self.lamda)) - (G**2/(H+self.lamda))\n",
    "                if score <= cur_score : \n",
    "                    score = cur_score\n",
    "                    split_point= j \n",
    "                    att = k\n",
    "            \n",
    "        index = np.argsort(np.array(X)[:, att])\n",
    "        X,y = X[index], y[index] \n",
    "        return X[:split_point+1], y[:split_point+1], X[split_point+1:], y[split_point+1:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f2dc7e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.9, 3. , 4.2, 1.5],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.2, 2.8, 4.8, 1.8]]),\n",
       " array([1, 1, 1, 2]),\n",
       " array([[6.3, 2.5, 4.9, 1.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [7.7, 3.8, 6.7, 2.2]]),\n",
       " array([1, 2, 2, 2, 2, 2, 2]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = XGBoost(train_x,train_y, 5, 1, 2)\n",
    "a_input, a_target, b_input, b_target = test.algorithm_1(test_x,test_y)\n",
    "test.algorithm_1(b_input, b_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655b1ca",
   "metadata": {},
   "source": [
    "### Algorithm 2 : Approximate Algorithm for split finding \n",
    "\n",
    "1. for k = 1 to m do \n",
    "- Propose $S_k$ = ${s_{k1}, s_{k2}, ... s_{kl}}$ by percentiles on feature k. \n",
    "- proposal can be done per tree (global), or per split(local) \n",
    "- end \n",
    "\n",
    "2. for k =1 to m do \n",
    "- $G_{kv}$ = $\\sum_{j \\in {j|s_k, v >= x_{jk} > s_k, v-1}} g_j$ \n",
    "\n",
    "- $H_{kv} = \\sum_{j \\in {j|s_k, v >= x_{jk} > s_k, v-1}} h_j$ \n",
    "- end \n",
    "\n",
    "3. Follow same step as in previous section to find max score only among proposed splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "58e1fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoost() : \n",
    "    def __init__(self, X, y, num_repeat,  gamma, lamda) : \n",
    "        self.X = X \n",
    "        self.n = np.shape(X)[0]\n",
    "        self.d = np.shape(X)[1]\n",
    "        \n",
    "        self.y = y \n",
    "        self.num_label = len(np.unique(y))\n",
    "        \n",
    "        self.num_repeat = num_repeat\n",
    "        \n",
    "        self.gamma = gamma \n",
    "        self.lamda = lamda\n",
    "\n",
    "    def algorithm_1(self, X, y) : \n",
    "        # gain 변수는 사용하지 않아 Score 변수로 변경하여 사용 \n",
    "        score = 0 \n",
    "        target_value = y\n",
    "    \n",
    "        g_lst = [] \n",
    "        h_lst = [] \n",
    "        \n",
    "        for i in range(self.num_repeat) : \n",
    "            model = tree.DecisionTreeRegressor()\n",
    "            model.fit(self.X, self.y)\n",
    "            pred_y = model.predict(X)\n",
    "            \n",
    "            grad = np.array(pred_y) - np.array(target_value)\n",
    "            h = np.ones(len(X))\n",
    "            g_lst.append(grad)\n",
    "            h_lst.append(h)\n",
    "            target_value = grad \n",
    "        \n",
    "        G = np.sum(g_lst) \n",
    "        H = np.sum(h_lst)\n",
    "        \n",
    "        \n",
    "        split_point = 0 \n",
    "        att = 0\n",
    "        \n",
    "        for k in range(self.d) : \n",
    "            G_left = 0 \n",
    "            H_left = 0 \n",
    "\n",
    "            # j 는 각 특성별 크기에 sorting 된 순서\n",
    "            for j in np.argsort(np.array(X)[:,k]) : \n",
    "            # 각 객체별로 g, h의 값을 더하기. \n",
    "                G_left += np.sum([g_lst[k][j] for k in range(len(g_lst))]) \n",
    "                H_left += np.sum([h_lst[k][j] for k in range(len(h_lst))])\n",
    "                G_right = G - G_left\n",
    "                H_right = H - H_left \n",
    "            \n",
    "                cur_score = (G_left**2 /(H_left + self.lamda)) + (G_right**2/(H_right + self.lamda)) - (G**2/(H+self.lamda))\n",
    "                if score <= cur_score : \n",
    "                    score = cur_score\n",
    "                    split_point= j \n",
    "                    att = k\n",
    "            \n",
    "        index = np.argsort(np.array(X)[:, att])\n",
    "        X,y = X[index], y[index] \n",
    "        return X[:split_point+1], y[:split_point+1], X[split_point+1:], y[split_point+1:] \n",
    "        \n",
    " \n",
    "\n",
    "    def algorithm_2(self,X,y, typ, bucket_size) :\n",
    "        score = 0 \n",
    "        target_value = y\n",
    "    \n",
    "        g_lst = [] \n",
    "        h_lst = [] \n",
    "        \n",
    "        for i in range(self.num_repeat) : \n",
    "            model = tree.DecisionTreeRegressor()\n",
    "            model.fit(self.X, self.y)\n",
    "            pred_y = model.predict(X)\n",
    "            \n",
    "            grad = np.array(pred_y) - np.array(target_value)\n",
    "            h = np.ones(len(X))\n",
    "            g_lst.append(grad)\n",
    "            h_lst.append(h)\n",
    "            target_value = grad \n",
    "        \n",
    "        G = np.sum(g_lst) \n",
    "        H = np.sum(h_lst)\n",
    "        \n",
    "        # Sk list를 표현할 index 뽑기 \n",
    "        index_lst = [] \n",
    "        split_size = len(X) / bucket_size\n",
    "        for k in range(self.d) :\n",
    "            index = np.argsort(np.array(X)[:,k])\n",
    "            index_att_lst = [] \n",
    "            for per in range(bucket_size) : \n",
    "                per_index = list(np.where((split_size*(per+1) > index) &(index >= split_size*(per)))[0])\n",
    "                index_att_lst.append(per_index)\n",
    "            index_lst.append(index_att_lst)\n",
    "        \n",
    "        #G_kv, H_kv 구하기 \n",
    "        G_kv = [] \n",
    "        H_kv = [] \n",
    "        bucket_lst = [] \n",
    "        for bucket in range(bucket_size) : \n",
    "            bucket_index = [index_lst[k][bucket] for k in range(self.d)]\n",
    "            bucket_lst.append(bucket_index)\n",
    "        \n",
    "        g_sum_lst = np.array(np.sum(g_lst,axis =0))\n",
    "        h_sum_lst = np.array(np.sum(h_lst, axis =0 ))\n",
    "        for k in range(self.d) :\n",
    "            bucket_index = [bucket_lst[bucket][k] for bucket in range(bucket_size)]\n",
    "            G_kv.append([np.sum(g_sum_lst[np.array(bucket_index[bucket])]) for bucket in range(bucket_size)])\n",
    "            H_kv.append([np.sum(h_sum_lst[np.array(bucket_index[bucket])]) for bucket in range(bucket_size)])\n",
    "                        \n",
    "    # 뒷 과정은 생략. Sk 로 구분한 데이터 셋을 algorithm1에 적용해도 됨. \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73763dc",
   "metadata": {},
   "source": [
    "### Algorithm 3 : Sparsity-Aware Split finding \n",
    "\n",
    "Missing Value를 좌, 우 측에 붙여야 한다. \n",
    "\n",
    "이를 위해서 Missing value 데이터와 그렇지 않은 데이터를 구분한다. \n",
    "그리고 모든 특성 값이 있는 데이터를 오름차순, 내림차순으로 정렬한 다음, Missing value 데이터를 각각 좌, 우측으로 붙인다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05fd406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoost() : \n",
    "    def __init__(self, X, y, num_repeat,  gamma, lamda) : \n",
    "        self.X = X \n",
    "        self.n = np.shape(X)[0]\n",
    "        self.d = np.shape(X)[1]\n",
    "        \n",
    "        self.y = y \n",
    "        self.num_label = len(np.unique(y))\n",
    "        \n",
    "        self.num_repeat = num_repeat\n",
    "        \n",
    "        self.gamma = gamma \n",
    "        self.lamda = lamda\n",
    "\n",
    "    def algorithm_1(self, X, y) : \n",
    "        # gain 변수는 사용하지 않아 Score 변수로 변경하여 사용 \n",
    "        score = 0 \n",
    "        target_value = y\n",
    "    \n",
    "        g_lst = [] \n",
    "        h_lst = [] \n",
    "        \n",
    "        for i in range(self.num_repeat) : \n",
    "            model = tree.DecisionTreeRegressor()\n",
    "            model.fit(self.X, self.y)\n",
    "            pred_y = model.predict(X)\n",
    "            \n",
    "            grad = np.array(pred_y) - np.array(target_value)\n",
    "            h = np.ones(len(X))\n",
    "            g_lst.append(grad)\n",
    "            h_lst.append(h)\n",
    "            target_value = grad \n",
    "        \n",
    "        G = np.sum(g_lst) \n",
    "        H = np.sum(h_lst)\n",
    "        \n",
    "        \n",
    "        split_point = 0 \n",
    "        att = 0\n",
    "        \n",
    "        for k in range(self.d) : \n",
    "            G_left = 0 \n",
    "            H_left = 0 \n",
    "\n",
    "            # j 는 각 특성별 크기에 sorting 된 순서\n",
    "            for j in np.argsort(np.array(X)[:,k]) : \n",
    "            # 각 객체별로 g, h의 값을 더하기. \n",
    "                G_left += np.sum([g_lst[k][j] for k in range(len(g_lst))]) \n",
    "                H_left += np.sum([h_lst[k][j] for k in range(len(h_lst))])\n",
    "                G_right = G - G_left\n",
    "                H_right = H - H_left \n",
    "            \n",
    "                cur_score = (G_left**2 /(H_left + self.lamda)) + (G_right**2/(H_right + self.lamda)) - (G**2/(H+self.lamda))\n",
    "                if score <= cur_score : \n",
    "                    score = cur_score\n",
    "                    split_point= j \n",
    "                    att = k\n",
    "            \n",
    "        index = np.argsort(np.array(X)[:, att])\n",
    "        X,y = X[index], y[index] \n",
    "        return X[:split_point+1], y[:split_point+1], X[split_point+1:], y[split_point+1:] \n",
    "        \n",
    " \n",
    "\n",
    "    def algorithm_2(self,X,y, typ, bucket_size) :\n",
    "        score = 0 \n",
    "        target_value = y\n",
    "    \n",
    "        g_lst = [] \n",
    "        h_lst = [] \n",
    "        \n",
    "        for i in range(self.num_repeat) : \n",
    "            model = tree.DecisionTreeRegressor()\n",
    "            model.fit(self.X, self.y)\n",
    "            pred_y = model.predict(X)\n",
    "            \n",
    "            grad = np.array(pred_y) - np.array(target_value)\n",
    "            h = np.ones(len(X))\n",
    "            g_lst.append(grad)\n",
    "            h_lst.append(h)\n",
    "            target_value = grad \n",
    "        \n",
    "        G = np.sum(g_lst) \n",
    "        H = np.sum(h_lst)\n",
    "        \n",
    "        # Sk list를 표현할 index 뽑기 \n",
    "        index_lst = [] \n",
    "        split_size = len(X) / bucket_size\n",
    "        for k in range(self.d) :\n",
    "            index = np.argsort(np.array(X)[:,k])\n",
    "            index_att_lst = [] \n",
    "            for per in range(bucket_size) : \n",
    "                per_index = list(np.where((split_size*(per+1) > index) &(index >= split_size*(per)))[0])\n",
    "                index_att_lst.append(per_index)\n",
    "            index_lst.append(index_att_lst)\n",
    "        \n",
    "        #G_kv, H_kv 구하기 \n",
    "        G_kv = [] \n",
    "        H_kv = [] \n",
    "        bucket_lst = [] \n",
    "        for bucket in range(bucket_size) : \n",
    "            bucket_index = [index_lst[k][bucket] for k in range(self.d)]\n",
    "            bucket_lst.append(bucket_index)\n",
    "        \n",
    "        g_sum_lst = np.array(np.sum(g_lst,axis =0))\n",
    "        h_sum_lst = np.array(np.sum(h_lst, axis =0 ))\n",
    "        for k in range(self.d) :\n",
    "            bucket_index = [bucket_lst[bucket][k] for bucket in range(bucket_size)]\n",
    "            G_kv.append([np.sum(g_sum_lst[np.array(bucket_index[bucket])]) for bucket in range(bucket_size)])\n",
    "            H_kv.append([np.sum(h_sum_lst[np.array(bucket_index[bucket])]) for bucket in range(bucket_size)])\n",
    "                        \n",
    "    # 뒷 과정은 생략. Sk 로 구분한 데이터 셋을 algorithm1에 적용해도 됨. \n",
    "    \n",
    "    def algorithm_3(self, X, y, missing_X, missing_y) : \n",
    "        # gain 변수는 사용하지 않아 Score 변수로 변경하여 사용 \n",
    "        score = 0 \n",
    "        target_value = y + missing_y\n",
    "    \n",
    "        g_lst = [] \n",
    "        h_lst = [] \n",
    "        \n",
    "        for i in range(self.num_repeat) : \n",
    "            model = tree.DecisionTreeRegressor()\n",
    "            model.fit(self.X, self.y)\n",
    "            \n",
    "            S_X = X + missing_X \n",
    "            S_y = y + missing_y\n",
    "            pred_y = model.predict(S_X)\n",
    "            \n",
    "            grad = np.array(pred_y) - np.array(target_value)\n",
    "            h = np.ones(len(S_X))\n",
    "            g_lst.append(grad)\n",
    "            h_lst.append(h)\n",
    "            target_value = grad \n",
    "        \n",
    "        G = np.sum(g_lst) \n",
    "        H = np.sum(h_lst)\n",
    "        \n",
    "        \n",
    "        split_point = 0 \n",
    "        att = 0\n",
    "\n",
    "        #  Missing value 오른쪽으로 몰기 \n",
    "        for k in range(self.d) : \n",
    "            G_left = 0 \n",
    "            H_left = 0 \n",
    "\n",
    "            for j in np.argsort(np.array(X)[:,k]) : \n",
    "            # 각 객체별로 g, h의 값을 더하기. \n",
    "                G_left += np.sum([g_lst[k][j] for k in range(len(g_lst))]) \n",
    "                H_left += np.sum([h_lst[k][j] for k in range(len(h_lst))])\n",
    "                G_right = G - G_left\n",
    "                H_right = H - H_left \n",
    "            \n",
    "                cur_score = (G_left**2 /(H_left + self.lamda)) + (G_right**2/(H_right + self.lamda)) - (G**2/(H+self.lamda))\n",
    "                if score <= cur_score : \n",
    "                    score = cur_score\n",
    "                    split_point= j \n",
    "                    att = k\n",
    "\n",
    "        for k in range(self.d) : \n",
    "            G_right = 0 \n",
    "            H_right = 0 \n",
    "            \n",
    "            for j in np.argsort(np.array(X)[:,k]) : \n",
    "            # 각 객체별로 g, h의 값을 더하기. \n",
    "                G_right += np.sum([g_lst[k][-j] for k in range(len(g_lst))]) \n",
    "                H_right += np.sum([h_lst[k][-j] for k in range(len(h_lst))])\n",
    "                G_left = G - G_right\n",
    "                H_left = H - H_right\n",
    "            \n",
    "                cur_score = (G_left**2 /(H_left + self.lamda)) + (G_right**2/(H_right + self.lamda)) - (G**2/(H+self.lamda))\n",
    "                if score <= cur_score : \n",
    "                    score = cur_score\n",
    "                    split_point= j \n",
    "                    att = k\n",
    "                    \n",
    "            \n",
    "            \n",
    "        index = np.argsort(np.array(X)[:, att])\n",
    "        X,y = X[index], y[index] \n",
    "        return X[:split_point+1], y[:split_point+1], X[split_point+1:], y[split_point+1:] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c7696334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [2, 3, 4]]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2,3]]\n",
    "b = [[2,3,4]]\n",
    "\n",
    "a + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2459b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bdb41d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c14f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
