{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dff1c11c",
   "metadata": {},
   "source": [
    "# Random Forest \n",
    "\n",
    "### 알고리즘은 강의 자료를 참고하겠음. \n",
    "\n",
    "1. For b =1 to B : \n",
    "- Draw a bootstrap sample $Z^*$ of size N from the training data. \n",
    "- Grow a random-forest tree $T_b$ to the bootstrapped data, by recursively repeating the following steps for each ternimal node of the tree, until the mininum node size $n_{min}$ is reached. \n",
    "\n",
    "> 1). Select m variables at random from the p variables \n",
    "\n",
    "> 2). Pick the best variable/split-point among the m \n",
    "\n",
    "> 3). Split the node into two daughter nodes \n",
    "\n",
    "2. Output the ensemble of trees {$T_b$}$|_1^B$ \n",
    "- To make a prediction at a new point x \n",
    "- Regression : $\\hat f_{rf}^B(x) = \\frac{1}{B} \\sum_{b=1}^B T_b(x)$ \n",
    "- Classification : Let $\\hat C_b(x)$ be the class prediction of the bth random-forest tree. Then $\\hat C_{rf}^B(x)$ = majority vote{$\\hat C_b(x)$}$|_1^B$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4743cdc",
   "metadata": {},
   "source": [
    "**구현해야하는 것**\n",
    "- 사이즈 N인 Bootstrap 샘플 B개 뽑기 \n",
    "- 샘플 데이터 셋 B개에 대해서 Random-forest tree 만들기 \n",
    "- result aggregation 진행. Regression이면 평균 값을, Classification이면 Majority vote 적용 \n",
    "\n",
    "**필요한 것**\n",
    "- M : 랜덤 트리 개수  \n",
    "- m : 선정할 변수 개수\n",
    "- X : input 데이터 \n",
    "- y : Output 데이터 \n",
    "- Treenode \n",
    "- Split variable 및 기준 설정 \n",
    "\n",
    "**함수의 형태** \n",
    "- def __init__(self, X, y, num_tree, num_var) \n",
    "- def make_tree(self, num_tree, num_var): \n",
    "\n",
    "*Regression 간에는 SVR을, classification 간에는 SVC를 적용하겠음*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a9e0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rand\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "X = load_iris()['data']\n",
    "y = load_iris()[\"target\"].reshape(-1,1)\n",
    "D = np.concatenate((X,y), axis=1)\n",
    "A = load_iris()['feature_names']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sc\n",
    "from scipy.stats import norm\n",
    "from sys import maxsize\n",
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f74ab9",
   "metadata": {},
   "source": [
    "### Decision Tree 수도 알고리즘 \n",
    "출처 - 단단한 머신러닝 챕터 4\n",
    "\n",
    "입력 : 훈련세트 D = {(x1,y1), (x2,y2), ..., (xm,ym)}, 속성집합 A = {a1, a2, ..., ad} \n",
    "과정 : 함수 TreeGenerate(D,A) \n",
    "\n",
    "1. node 생성 \n",
    "2. if D의 샘플이 모두 같은 클래스(y값)에 속하면 then \n",
    "- 해당 node를 레이블이 C인 터미널 노드로 정한다. return \n",
    "- end if \n",
    "\n",
    "3. (Leaf 설정) if A가 없거나, D의 샘플이 A 속성에 같은 값을 취한다면 then \n",
    "- 해당 node를 터미널 노드로 정하고, 해당 클래스는 D 샘플 중 가장 많은 샘풀의 수가 속한 속성으로 정한다. return \n",
    "- end if \n",
    "\n",
    "4. A에서 최적의 분할 속성 $a_*$를 선택한다. \n",
    "\n",
    "5. for $a_*$의 각 $a_*^v$에 대해 다음을 행한다. \n",
    "- node에서 하나의 가지를 생선한다. $D_v$는 D는 $a_*^v$ 속성값을 가지는 샘플의 하위 집합으로 표기한다. \n",
    "- if $D_v$가 0이면 then \n",
    "> 해당 가지 node를 터미널 노드로 정하고, 해당 클래스는 D 샘플 중 가장 많은 클래스로 정한다. return \n",
    "- else \n",
    "> TreeGenerate($D_v$, A/{$a_*$} 를 가지 노드로 정한다. \n",
    "> end if \n",
    "- end for\n",
    "\n",
    "출력 : node를 루트 노드로 하는 의사결정 트리 \n",
    "\n",
    "\n",
    "\n",
    "### 4. 최적의 분할 속성 정하는 방법 : 정보 이득율 사용 \n",
    "- Gain_ratio(D,a) = $\\frac{Gain(D,a)}{IV(a)}$ \n",
    "- IV(a) = - $\\sum_{v=1}^V \\frac{D^v}{D} log2 \\frac{D^v}{D}$\n",
    "- Gain(D,a) = Ent(D) - $\\sum_{v=1}^V \\frac{|D^v|}{|D|} Ent(D^v)$\n",
    "- Ent(D) = -$\\sum_{k=1}^{|Y|}p_k log2(p_k) $\n",
    "> |Y| : 클래스의 종류 개수 \n",
    "> $p_k$ : k번째 클래스의 비율 \n",
    "\n",
    "\n",
    "\n",
    "### 5. for $a_*$의 각 value에 따라 for 절을 수행할 것. 문제 - Category value가 아니라 Numerical Value로 주어지면 어떻게 분류하지? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc9c26cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#엔트로피 설정\n",
    "\n",
    "from math import log \n",
    "from collections import defaultdict\n",
    "\n",
    "def entropy(dataset) : \n",
    "    datasize = len(dataset) \n",
    "    \n",
    "    label_count = defaultdict(int)\n",
    "    entropy = 0.0 \n",
    "    \n",
    "    for data_line in dataset : \n",
    "        cur_label = data_line[-1]\n",
    "        label_count[cur_label] += 1 \n",
    "        \n",
    "    for key in label_count : \n",
    "        prop = float(label_count[key]) / datasize \n",
    "        entropy -= prop*log(prop,2) \n",
    "    return entropy\n",
    "\n",
    "# Dv, Ent(Dv) 계산하기 위한 토대. 특정 속성값을 제외한 나머지 값들을 결과로 반환 \n",
    "def pick(dataset, index, value) : \n",
    "    result = [] \n",
    "    for dataline in dataset : \n",
    "        dataline = list(dataline)\n",
    "        if dataline[index] == value : \n",
    "            temp_vec = dataline[:index]\n",
    "            temp_vec.extend(dataline[index+1:])\n",
    "            result.append(temp_vec)\n",
    "    return result \n",
    "\n",
    "# 최고의 선별 속성을 결정하는 함수 \n",
    "def best_feat(dataset): \n",
    "    base_entropy = entropy(dataset)\n",
    "    best_infogain = 0.0 \n",
    "    index = 0 \n",
    "    num_feat = len(dataset[0]) -1 \n",
    "    for i in range(num_feat) : \n",
    "        feat_value_set = set([value[i] for value in dataset])\n",
    "        new_entropy = 0.0 \n",
    "        \n",
    "        # 정보 이득 계산하기 \n",
    "        for element in feat_value_set : \n",
    "            sub_dataset = pick(dataset, i, element) \n",
    "            prop = len(sub_dataset) / float(len(dataset))\n",
    "            new_entropy += prop*entropy(sub_dataset)\n",
    "            \n",
    "        info_gain = base_entropy - new_entropy \n",
    "        \n",
    "        if best_infogain < info_gain : \n",
    "            best_infogain = info_gain \n",
    "            index = i \n",
    "    return index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e35dc07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_feat(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "abcb39d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 속성 값의 개수에 맞춰 Child 생성하는데 막힘. \n",
    "\n",
    "class treenode() : \n",
    "    def __init__(self, num_child=None) : \n",
    "        self.label = None\n",
    "        self.child = self.make_child(num_child)\n",
    "        \n",
    "    def make_child(self, num_child) : \n",
    "        for i in range(num_child) : \n",
    "            globals()['self.child_{}'.format(i)] = None\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2714740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomTreeGenerate(D,A,m) : \n",
    "    node = treenode()\n",
    "    num_att = len(D[0, :-1])\n",
    "    chosen_att = np.choice(range(num_att), m)\n",
    "    \n",
    "    # \"모든 샘플이 같은 클래스에 속하면\"을 y값에 대한 분산값이 0인 것으로 확인 \n",
    "    if len(D) != 0 and  np.var(D[:, -1]) ==0 : \n",
    "        node.label = D[0,-1] \n",
    "        return node\n",
    "    \n",
    "    # \"속성 집합이 0일 경우\"를  샘플의 개수로, 속성 값이 전부 같은 경우는 x값에 대한 분산값이 0인 것으로 확인 \n",
    "    if len(D) == 0 or np.var(D[:, :-1]) == 0 : \n",
    "        # np.unique를 통해서 lable의 개수 세기. [1]은 갯수 list로, 그중 가장 큰 값을 가진 것의 index 반환  \n",
    "        node.label = np.argmax(np.unique(D[:, -1], return_counts=True)[1])\n",
    "        return node \n",
    "    \n",
    "\n",
    "    best_att = best_feat(D)\n",
    "\"\"\"\n",
    "# 5번쨰 항목. Numerical Value의 이산화 작업 : 이분법 적용 \n",
    "# 가장 가까운 split 포인트로 값을 재할당 \n",
    "    value_list = np.uniqu(np.sort(D[:, best_att]))\n",
    "    split_point = [] \n",
    "    pre = value_list[0]\n",
    "    for i in range(len(value_list)-1) :\n",
    "        split_point.append((pre+value_list[i+1])/2) \n",
    "        pre = value_list[i+1]\n",
    "    \n",
    "    for value in dataset[:, best_att] : \n",
    "        index = np.argmin(split_point - value)\n",
    "\"\"\"\n",
    "\n",
    "    for value in dataset[:,best_att] : \n",
    "        new_node = treenode()\n",
    "        node.child = new_node\n",
    "        new_dataset = [dataset[i] for i in len(D) if dataset[i,best_att] == value]\n",
    "        if len(new_dataset) == 0 : \n",
    "            new_node.label = np.argmax(np.unique(D[:, -1], return_counts=True)[1])\n",
    "            return new_node\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c42c81a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Random_forest(): \n",
    "    def __init__(self, data, num_sample, num_tree, num_var) : \n",
    "        self.data = data \n",
    "        self.X = data[:, :-1]\n",
    "        self.y = data[:. -1]\n",
    "        self.n = np.shape(self.X)[0]\n",
    "        self.m = np.shape(self.X)[1]\n",
    "        \n",
    "        self.num_sample = num_sample\n",
    "        self.num_tree = num_tree\n",
    "        \n",
    "        self.sample = self.random_sample() \n",
    "        \n",
    "        self.num_var = num_var \n",
    "        \n",
    "    def random_sample(self) : \n",
    "        sample_list = []\n",
    "    \n",
    "        for i in range(self.num_tree) : \n",
    "            np.random.seed(i)\n",
    "            sample_index = np.random.choice(self.n, size = self.num_sample, replace = True)\n",
    "            sample = X1[sample_index]\n",
    "            sample_list.append(sample)\n",
    "\n",
    "        return sample_list\n",
    "\n",
    "        \n",
    "    def make_tree(self) : \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5472a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진 분류 모델 \n",
    "def bagging(X, x_new, model, T, N, kernel, agg_type) : \n",
    "    sample_list = []\n",
    "    ensemble_list = [] \n",
    "    X1 = np.array([X[i][:-1] for i in range(len(X)) if X[i][-1] == 0]) \n",
    "    X2 = np.array([X[i][:-1] for i in range(len(X)) if X[i][-1] == 1]) \n",
    "    \n",
    "    for i in range(T) : \n",
    "        np.random.seed(i)\n",
    "        sample_index = np.random.choice(np.shape(X1)[0], size = N, replace = True)\n",
    "        sample_1 = X1[sample_index]\n",
    "        sample_2 = X2[sample_index]\n",
    "        ensemble_list.append(model(sample_1,sample_2,kernel)) \n",
    "    \n",
    "    result = [] \n",
    "    for i in range(T) : \n",
    "        result.append(ensemble_list[i].classify(x_new)) \n",
    "    \n",
    "    return aggregation(result, agg_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef328aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregation(result, agg_type) : \n",
    "    \n",
    "    if agg_type == \"Majority\" : \n",
    "        agg_result = argmax([result.count(0), result.count(1)]) \n",
    "        return agg_result\n",
    "    \n",
    "    elif agg_type == \"Accuracy Weighted\" : \n",
    "        return 0\n",
    "        # 모델별 정확도가 구현되어 있지 않아 여기선 생략 \n",
    "    \n",
    "    elif agg_type == \"Prediction Weighted\" : \n",
    "        return 0 \n",
    "        # 모델별 예상 확률이 구현되어 있지 않아 여기선 생략 \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ada1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e24c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a235f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc918d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101d02a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c00ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd6973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de222363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a9246a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd75ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
