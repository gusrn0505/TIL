{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85f9f94",
   "metadata": {},
   "source": [
    "<최초 MC 예측 수도코드 - 출처 : 단단한 강화학습 챕터 5> \n",
    "\n",
    "입력 : 평가 대상인 정책 $\\pi$ \n",
    "\n",
    "초기화 \n",
    "- 모든 $s \\in S$ 에 대해 임의의 값으로 $V(s) \\in R$를 초기화 \n",
    "- 모든 $s \\in S$ 에 대해 값이 채워지지 않는 리스트를 Returns(s) 변수에 대입\n",
    "\n",
    "(각 에피소드에 대해) 무한 루프 :\n",
    "- 정책 $\\pi$를 따르는 하나의 에피소드를 생성 : $S_0, A_0, R_1, S_1, A_1, R_2, ... , S_{T-1}, A_{T-1}, R_T$ \n",
    "- 변수 G에 0을 대입 \n",
    "- 에피소드의 각 단계에 대해 반복 수행, t= T-1, T-2, ..., 0 : \n",
    "\n",
    " - 변수 G에 $\\gamma G + R_{t+1}$을 대입 \n",
    " \n",
    " - $S_t$가 $S_o, S_1, ... S_{t-1}$ 안에 나타나지 않는다면 : \n",
    " \n",
    "  > 리스트 Returns($S_T$)에 변수 G를 새 항목으로 추가 \n",
    "  \n",
    "  > 리스트 Returns($S_T$)에 대한 평균을 $V(S_t)$에 대입 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c530c15",
   "metadata": {},
   "source": [
    "**구현해야 하는 것** \n",
    "- 에피소드 생성 \n",
    "- Returns(s) : list \n",
    "- V(s) : 각 s를 넣었을 때 가치 값이 나오도록 구현(dic) \n",
    "- R_T : 가치 함수 \n",
    "- G : (갱신됨) $\\gamma G + R_{t+1}$ \n",
    "\n",
    "- 확률값에 따라 random하게 샘플을 뽑아내는 함수(정책 $\\pi$에 따라 s' 선택)  \n",
    "\n",
    "- 정책 $\\pi$에 따르는 에피소드 생성 함수\n",
    "\n",
    "- 각 에피소드별 루프를 돌며 Returns(S_t)를 갱신할 함수 \n",
    "\n",
    "**필요한 것** \n",
    "- $\\gamma$ : 입력 값으로 받기 \n",
    "- S : 모든 상태값의 집합 \n",
    "- A : 모든 행동값의 집합 \n",
    "\n",
    "\n",
    "**함수/데이터의 형태** \n",
    "- class First_Visit_MC : \n",
    "\n",
    "> def __init__(self, S, A, reward_func, pi, gamma = 0.9, num_episode = 10, len_episode = 20) \n",
    "\n",
    "> def choice_sample(self, list, prob) : => 단일 s' 값 반환 \n",
    "\n",
    "> def make_episode(self, start_s, T, pi) : \n",
    "=> 최초 s값과 에피소드 길이 T, 정책 $\\pi$를 넣으면 각 시간 단계에 따른 S,A,R을 계산하여 dic 형태로 반환. \n",
    "\n",
    "> def update_returns(self) : \n",
    "\n",
    "**전제**\n",
    "- 상태 s에서 행동 a를 했을 때 결정론적으로 s' 상태로 변화한다. \n",
    "\n",
    "<외부 함수> \n",
    "- def reward_func(s',a,s) : \n",
    "\n",
    "- def pi(a,s) :\n",
    "\n",
    "\n",
    "<고민점>\n",
    "- 정책 $\\pi$의 값이 확률론적으로 나왔을 때 A_t를 어떻게 정의할 것인가? 확률에 따른 random한 값으로? 이게 맞는 것 같은데. \n",
    "\n",
    "> 확률값에 따라 랜덤하게 s'를 생성한 다음, return_func(s',a,s)에 대입하면 수치상으로 값을 얻을 수 있겠다. \n",
    "\n",
    "- 에피소드 생성 간 데이터 형식은 어떻게? S,A,R을 한 번에 넣을 것인가, 아니면 따로 따로 구현할 것인가? \n",
    "\n",
    "> dic{\"S\": []. \"A\" : [], \"R\" : []} 형식이면 보기도 깔끔할 것 같다. \n",
    "\n",
    "- 추후에 정책 $\\pi$를 변경하기 위해선, $\\pi$가 class 내부 함수여야 할 것 같다. \n",
    "\n",
    "> 아! 아예 처음부터 pi를 외부에서 받으라고 문제에서 정의했기 때문에 고민할 필요 x\n",
    "\n",
    "- '(각 에피소드에 대해) 무한 루프' 라면, 진짜 무한히 돌리라는 건가? 에피소드의 개수는 어떻게 하고? \n",
    "\n",
    "> 일단 임의로 총 에피소드의 개수는 정해야 겠다. \n",
    "> 하지만 루프를 멈출 조건이 필요하겠는걸. 모든 에피소드에 대한 갱신이 끝난 다음 마무리 하는 것으로? \n",
    "\n",
    "- Returns(S_t) 의 데이터 형식은 어떻게 해야하나? \n",
    "\n",
    "> s를 인덱스로 받아, 변수 G를 반환할 수 있는 list여야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d55d4cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9fdcb303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 용 임시 데이터 \n",
    "S = list(range(100)) \n",
    "A = list(range(-5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a1d5884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외부용 함수 reward_func, pi 간략 구현 \n",
    "def reward_func(next_s, a, s) : \n",
    "    # next_s 와 s의 차이가 짝수이면 +1, 홀수면 -1 \n",
    "    # 단, a의 크기에 반비례함. \n",
    "    if abs(next_s - s) %2 == 0 : reward = 1 \n",
    "    else : reward = -1\n",
    "    \n",
    "    if a == 0 : \n",
    "        return 0 \n",
    "    else : \n",
    "        return reward / a # 즉, a가 양수이며 짝수이며, 가능한 작을 때 (=2) 일 때 최대의 보상이 주어지도록 설정 \n",
    "    \n",
    "def pi(a,s) : #s 상황에서 a를 선택할 확률. 전체 합은 1이여야 한다. \n",
    "    # 확률은 모두 동일하게 설정 \n",
    "    \n",
    "    return 1/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed351cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 설정 및 초기화 테스트 \n",
    "\n",
    "class First_Visit_MC : \n",
    "    def __init__(self, S, A, reward_func, pi, gamma = 0.9, num_episode = 10, len_episode = 20) : \n",
    "        self.S = S \n",
    "        self.A = A \n",
    "        self.reward_func = reward_func\n",
    "        self.pi = pi \n",
    "        self.gamma = 0.9 \n",
    "        self.num_episode = num_episode\n",
    "        self.T = len_episode\n",
    "        \n",
    "        self.set_episode = [] # 추후에 어떤 episode들이 있었나 확인용 \n",
    "        \n",
    "        self.V, self.return_lst = self.initiate() \n",
    "    \n",
    "    def initiate(self) : # V(s)와 returns(s) 초기화 함수 \n",
    "        V = defaultdict(float)\n",
    "        for s in self.S : \n",
    "            V[s] = 0 \n",
    "        return_lst = [0]*len(self.S)\n",
    "        return V, return_lst\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "894df6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'float'>, {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0}) [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "test = First_Visit_MC(S,A,reward_func, pi) \n",
    "print(test.V, test.return_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "30ccb93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice_sample, make_episode 함수 구현 \n",
    "\n",
    "class First_Visit_MC : \n",
    "    def __init__(self, S, A, reward_func, pi, gamma = 0.9, num_episode = 10, len_episode = 20) : \n",
    "        self.S = S \n",
    "        self.A = A \n",
    "        self.reward_func = reward_func\n",
    "        self.pi = pi \n",
    "        self.gamma = 0.9 \n",
    "        self.num_episode = num_episode\n",
    "        self.T = len_episode\n",
    "        \n",
    "        self.set_episode = [] # 추후에 어떤 episode들이 있었나 확인용 \n",
    "        \n",
    "        self.V, self.return_lst, self.s_prob = self.initiate() \n",
    "    \n",
    "    def initiate(self) : # 수정. 각 상태 s별 pi(a,s)의 값을 가지고 있는 dic 추가 생성 \n",
    "        V = defaultdict(float)\n",
    "        s_prob = defaultdict(list)\n",
    "        for s in self.S : \n",
    "            V[s] = 0\n",
    "            s_prob[s] = [0]*len(self.A)\n",
    "            for index, a in enumerate(self.A) : \n",
    "                s_prob[s][index] = self.pi(a,s)\n",
    "\n",
    "        return_lst = [0]*len(self.S)\n",
    "        return V, return_lst, s_prob\n",
    "    \n",
    "    def choice_sample(self, s) : #수정. s_prob 에 대해 정의한 후 random.choice 함수 사용\n",
    "        a= random.choices(self.A, weights = self.s_prob[s])\n",
    "        return a[0], max(s+a[0],0) # a[0] +s 가 음수인 경우는 0으로 조정 \n",
    "    \n",
    "    def make_episode(self, start_s, T) :\n",
    "        s = start_s\n",
    "        episode = {\"S\" : [], \"A\" : [], \"R\" : []}\n",
    "        episode[\"R\"].append(0) # R_0 값 부여 \n",
    "        for _ in range(T) : \n",
    "            episode[\"S\"].append(s)\n",
    "            a, next_s = self.choice_sample(s)\n",
    "            r = self.reward_func(next_s, a, s)\n",
    "            episode[\"A\"].append(a)\n",
    "            episode[\"R\"].append(r)\n",
    "            s = next_s\n",
    "        return episode\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5a3b05a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'S': [10, 10, 11, 8, 6, 8, 6, 2, 0, 0, 1, 0, 0, 0, 2, 2, 3, 0, 0, 4], 'A': [0, 1, -3, -2, 2, -2, -4, -5, -3, 1, -4, -3, -4, 2, 0, 1, -4, 0, 4, -4], 'R': [0, 0, -1.0, 0.3333333333333333, -0.5, 0.5, -0.5, -0.25, -0.2, -0.3333333333333333, -1.0, 0.25, -0.3333333333333333, -0.25, 0.5, 0, -1.0, 0.25, 0, 0.25, -0.25]}\n"
     ]
    }
   ],
   "source": [
    "# choice_sample, make_episode 함수 테스트\n",
    "test = First_Visit_MC(S,A,reward_func, pi) \n",
    "print(test.make_episode(10,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9bf4bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_returns 함수 구현. \n",
    "\n",
    "class First_Visit_MC : \n",
    "    def __init__(self, S, A, reward_func, pi, gamma = 0.9, num_episode = 10, len_episode = 20) : \n",
    "        self.S = S \n",
    "        self.A = A \n",
    "        self.reward_func = reward_func\n",
    "        self.pi = pi \n",
    "        self.gamma = 0.9 \n",
    "        self.num_episode = num_episode\n",
    "        self.T = len_episode\n",
    "        \n",
    "        self.set_episode = [] # 추후에 어떤 episode들이 있었나 확인용 \n",
    "        \n",
    "        self.V, self.return_lst, self.s_prob = self.initiate() \n",
    "    \n",
    "    def initiate(self) : # 수정. 각 상태 s별 pi(a,s)의 값을 가지고 있는 dic 추가 생성 \n",
    "        V = defaultdict(float)\n",
    "        s_prob = defaultdict(list)\n",
    "        for s in self.S : \n",
    "            V[s] = 0\n",
    "            s_prob[s] = [0]*len(self.A)\n",
    "            for index, a in enumerate(self.A) : \n",
    "                s_prob[s][index] = self.pi(a,s)\n",
    "\n",
    "        return_lst = [0]*len(self.S)\n",
    "        return V, return_lst, s_prob\n",
    "    \n",
    "    def choice_sample(self, s) : \n",
    "        a= random.choices(self.A, weights = self.s_prob[s])\n",
    "        \n",
    "        # 수정. a[0] +s 가 s의 범위 안에 들어오도록 수정 \n",
    "        return a[0], min(max(s+a[0],0), 99)  \n",
    "    \n",
    "    def make_episode(self, start_s, T) :\n",
    "        s = start_s\n",
    "        episode = {\"S\" : [], \"A\" : [], \"R\" : []}\n",
    "        episode[\"R\"].append(0) # R_0 값 부여 \n",
    "        for _ in range(T) : \n",
    "            episode[\"S\"].append(s)\n",
    "            a, next_s = self.choice_sample(s)\n",
    "            r = self.reward_func(next_s, a, s)\n",
    "            episode[\"A\"].append(a)\n",
    "            episode[\"R\"].append(r)\n",
    "            s = next_s\n",
    "        return episode\n",
    "            \n",
    "    def update_returns(self) :\n",
    "        for _ in range(self.num_episode) :\n",
    "            # 시작 s는 시작 가정에 따라 랜덤하게 산출하겠음. \n",
    "            start_s = random.choice(self.S)\n",
    "            set_s = set() # t 단계까지 나왔던 s 확인 \n",
    "            set_s.add(start_s)\n",
    "            episode = self.make_episode(start_s, self.T)\n",
    "            G = 0 \n",
    "            G_lst = [0]*self.T\n",
    "            \n",
    "            # G와 returns(S_t)를 업데이트 하는 순서가 각각 역순이라 따로 구현\n",
    "            for t in reversed(range(self.T)) : \n",
    "                G = self.gamma *G + episode[\"R\"][t]\n",
    "                G_lst[t] = G\n",
    "                \n",
    "            for t in range(self.T): \n",
    "                s = episode[\"S\"][t]\n",
    "                if s not in set_s : \n",
    "                    self.return_lst[s] = G_lst[self.T-t-1]\n",
    "                    self.V[s] = sum(self.return_lst) / len([i for i in self.return_lst if i != 0])\n",
    "                set_s.add(s) \n",
    "        \n",
    "        \n",
    "        \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6af73498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "# update_returns 구현 테스트 \n",
    "test = First_Visit_MC(S,A,reward_func, pi) \n",
    "test.update_returns()\n",
    "value = list(test.V.values())\n",
    "print(value.index(max(value)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f2230",
   "metadata": {},
   "source": [
    "위의 상태 가치로만은 계속 값이 변하는 모습을 보인다. \n",
    "행동 가치 함수에 대해서 알 때 정확한 정책을 산출할 수 있을 것으로 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d457845",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
