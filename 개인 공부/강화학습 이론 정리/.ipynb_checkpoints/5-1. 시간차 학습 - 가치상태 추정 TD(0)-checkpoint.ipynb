{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34a63a6d",
   "metadata": {},
   "source": [
    "### <$v_\\pi$를 추정하기 위한 표 형태로 된 TD(0)>\n",
    "\n",
    "입력 : 평가 대상이 될 정책 $\\pi$\n",
    "\n",
    "알고리즘 파라미터 : 시간 간격 $\\alpha \\in (0,1] $ \n",
    "\n",
    "모든 $s \\in S^+$ 에 대한 V(s)를 임의의 값으로 초기화. 단, V(종단) = 0 \n",
    "\n",
    "각 에피소드에 대한 루프 : \n",
    "- S를 초기화 \n",
    "- 에피소드의 각 단계에 대한 루프 : \n",
    "\n",
    "> A ← S에 대해 $\\pi$에 따라 도출된 행동 \n",
    "\n",
    "> 행동 A를 취하고 $R,S'$ 을 관측 \n",
    "\n",
    "> V(s) ← $V(S) + \\alpha[R+ \\gamma V(S') - V(S)] $\n",
    "\n",
    "> S ← S' \n",
    "\n",
    "S가 종단이면 종료 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a5c15",
   "metadata": {},
   "source": [
    "<구현해야 하는 것> \n",
    "- V(S) : 상태 s에 대한 가치 함수 계산. 챕터 4의 코드 참고 \n",
    "- R(s',a,s) : 보상함수. 챕터 4의 코드 참고 \n",
    "- next_S(s,a) : 상태 s와 행동 a를 했을 때의 s'. 챕터 4의 코드 참고  \n",
    "- make_episode : 챕터 4의 코드 참고\n",
    "\n",
    "<필요한 것> \n",
    "- $\\alpha$ : class 제작시 입력 값으로 부여 \n",
    "\n",
    "\n",
    "<함수 / 데이터 형식> \n",
    "- class evaluate_TD : # alpha 값 추가 \n",
    "> def __init__(self, S, A, pi, alpha reward_func, epsilon = 0.001, gamma = 0.9, num_episode = 10, len_episode = 20) : \n",
    "\n",
    "\n",
    "\n",
    "<외부함수> \n",
    "- $\\pi(s,a) $ : 상태 s 하에서 선택할 a. 확률론적으로 구현할 것  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8592d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 용 임시 데이터 \n",
    "S = list(range(100)) \n",
    "A = list(range(-5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3907e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b2cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외부용 함수 reward_func 간략 구현 (이전 예시 활용)\n",
    "def reward_func(next_s, a, s) : \n",
    "    # next_s 와 s의 차이가 짝수이면 +1, 홀수면 -1 \n",
    "    # 단, a의 크기에 반비례함. \n",
    "    if abs(next_s - s) %2 == 0 : reward = 1 \n",
    "    else : reward = -1\n",
    "    \n",
    "    if a == 0 : \n",
    "        return 0 \n",
    "    else : \n",
    "        return reward / a # 즉, a가 양수이며 짝수이며, 가능한 작을 때 (=2) 일 때 최대의 보상이 주어지도록 설정 \n",
    "    \n",
    "    \n",
    "def pi(s,a) : #s 상황에서 a를 선택할 확률. 전체 합은 1이여야 한다. \n",
    "    # 확률은 모두 동일하게 설정 \n",
    "    \n",
    "    return 1/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4067b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (2956634012.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [4]\u001b[1;36m\u001b[0m\n\u001b[1;33m    def __init__(self, S, A, pi, alpha=0.1, reward_func, epsilon = 0.001, gamma = 0.9, num_episode = 10, len_episode = 20) :\u001b[0m\n\u001b[1;37m                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "# 비활성 MC 정책 코드 참고  \n",
    "\n",
    "class evaluate_TD :  \n",
    "    def __init__(self, S, A, pi, alpha=0.1, reward_func, epsilon = 0.001, gamma = 0.9, num_episode = 10, len_episode = 20) : \n",
    "        self.S = S \n",
    "        self.A = A \n",
    "        self.pi = pi #정책은 (s,a) 형식으로 입력받았을 때 확률 값을 산출\n",
    "        self.alpha = alpha\n",
    "        self.reward_func = reward_func\n",
    "\n",
    "        self.epsilon = epsilon \n",
    "        self.gamma = 0.9 \n",
    "        self.num_episode = num_episode\n",
    "        self.T = len_episode\n",
    "\n",
    "#        self.b, self.C = self.initiate_b()\n",
    "#        self.Q, self.return_lst, self.s_prob = self.initiate() \n",
    "        self.V = self.initiate() \n",
    "    \n",
    "\"\"\"    \n",
    "    def initiate_b(self) : \n",
    "        # pi를 보증하는 b 정책 초기화하기. 예측이기 때문에 b 정책이 바뀔 일은 없음.\n",
    "        # C(s,a) 도 같이 초기화 \n",
    "        pi_dic = defaultdict(float)\n",
    "        c_dic = defaultdict(float)\n",
    "        for s in self.S :\n",
    "            for a in self.A :  \n",
    "                pi_dic[(s,a)] = 1/len(self.A)\n",
    "                c_dic[(s,a)] = 0\n",
    "        return pi_dic, c_dic\n",
    "    \n",
    "    def initiate(self) :  \n",
    "        Q = defaultdict(float)\n",
    "        s_prob = defaultdict(list)\n",
    "        for s in self.S : \n",
    "            s_prob[s] = [0]*len(self.A)\n",
    "            for index, a in enumerate(self.A) : \n",
    "                Q[(s,a)] = 0 \n",
    "                s_prob[s][index] = self.pi(s,a)\n",
    "\n",
    "        return_lst = [[0]*len(self.A)]*len(self.S) # return_lst의 데이터 형식 변경\n",
    "        return Q, return_lst, s_prob\n",
    "\"\"\"\n",
    "\n",
    "    def initiate(self) : # V 값을 초기화 \n",
    "        pass\n",
    "    \n",
    "\n",
    "\n",
    "    def choice_action(self, s, policy) : #정책 기반으로 상황 s에 있을 때 선택할 행동 a 산출 \n",
    "        policy_a_list = [] \n",
    "        for _ in self.A :\n",
    "            policy_a_list.append(policy(s,_)) \n",
    "            \n",
    "        a= random.choices(self.A, weights = policy_a_list[self.A.index(s)]) # 제한 점 : s는 index와 동일해야 한다. \n",
    "        # 뭔가 이상하게 적용되어 있는데 \n",
    "        \n",
    "        # 수정. a[0] +s 가 s의 범위 안에 들어오도록 수정 \n",
    "        return a[0], min(max(s+a[0],0), max(self.S))  \n",
    "\n",
    "\n",
    "    def make_episode(self, start_s, T) :\n",
    "        s = start_s\n",
    "        episode = {\"S\" : [], \"A\" : [], \"R\" : []}\n",
    "        episode[\"R\"].append(0) # R_0 값 부여 \n",
    "        for _ in range(T) : \n",
    "            episode[\"S\"].append(s)\n",
    "            a, next_s = self.choice_sample(s, self.pi) \n",
    "            r = self.reward_func(next_s, a, s)\n",
    "            episode[\"A\"].append(a)\n",
    "            episode[\"R\"].append(r)\n",
    "            s = next_s\n",
    "        return episode \n",
    " \n",
    "    def update_returns(self) : # TD(0) 형식에 맞춰 대거 수정 \n",
    "        pass\n",
    "\n",
    "        \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daa6c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = off_policy_MC(S,A,reward_func, pi) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f6096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a6cd28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5247f6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7401b1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647fdd66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f59abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56311ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc698a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dbb205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
