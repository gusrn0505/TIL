{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c59076a",
   "metadata": {},
   "source": [
    "### <최적 정책을 추정하기 위핸 Q학습(비활성 정책 TD 제어)> \n",
    "출처 : 단단한 머신러닝 챕터 6 \n",
    "\n",
    "알고리즘 파라미터 : 시간 간격 $\\alpha \\in (0,1] $, 작은 양수 $\\epsilon > 0$ \n",
    "\n",
    "모든 $s \\in S^+$ 에 대한 Q(s,a)를 임의의 값으로 초기화. 단, Q(종단, -) = 0 \n",
    "\n",
    "각 에피소드에 대한 루프 : \n",
    "- S를 초기화 \n",
    "- 에피소드의 각 단계에 대한 루프 : \n",
    "\n",
    "> Q로부터 유도된 정책을 사용하여 S'으로부터 A'를 선택 \n",
    "\n",
    "> 행동 A를 취하고 $R,S'$ 을 관측 \n",
    "\n",
    "> Q(s,a) ← $Q(s,a) + \\alpha[R+ \\gamma max_a Q(s',a') - Q(s,a)] $\n",
    "\n",
    "> S ← S', A ← A' \n",
    "\n",
    "S가 종단이면 종료 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e01fe2",
   "metadata": {},
   "source": [
    "**<구현해야 하는 것>**\n",
    "- Q(a,s) :  \n",
    "\n",
    "**<필요한 것>** \n",
    "- $\\alpha$ : class 제작시 입력 값으로 부여 \n",
    "- $\\epsilon$ : 충분히 작은 값으로 \n",
    "\n",
    "**<함수 / 데이터 형식>** \n",
    "- class evaluate_TD : # alpha 값 추가 \n",
    "> def __init__(self, S, A, alpha reward_func, epsilon = 0.001, gamma = 0.9, num_episode = 10, len_episode = 20) : \n",
    "\n",
    "\n",
    "**<외부함수>** \n",
    "- R(s',a,s) : 보상함수. 챕터 4의 코드 참고 \n",
    "- choose_random_max(lst) : lst 중 가장 값이 큰 것을 반환. 혹시 max 값이 중복된다면 임의의 하나 산출\n",
    "\n",
    "**<고민점>** \n",
    "- max_a Q(s',a')를 어떻게 구현할 것인가? \n",
    "\n",
    "> list comprehension 과 max 함수로 표현 가능하겠다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8385e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 용 임시 데이터 \n",
    "S = list(range(100)) \n",
    "A = list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0440ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "258951b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외부용 함수 reward_func 간략 구현 \n",
    "def reward_func(next_s, a, s) : \n",
    "    # next_s 와 s의 차이가 짝수이면 +1, 홀수면 -1 \n",
    "    if abs(next_s - s) %2 == 0 and next_s > s : reward = 1 \n",
    "    else : reward = -1\n",
    "    \n",
    "    return reward \n",
    "    \n",
    "#최대값이 2개 이상인 경우, 임의로 1개의 최대값을 만들어낸 행동 a를 산출 \n",
    "def choose_random_max(lst) :\n",
    "    max_arg = np.where(np.array(lst) >= max(lst))\n",
    "    return random.choice(list(max_arg)[0]) #max_arg가 array 형태로 안에 있는 list를 꺼내기 위해 [0] 사용 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "934985b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_Q_TD 코드 참고\n",
    "# 마지막 Q(s,a) 갱신 부분만 변경 \n",
    "\n",
    "class evaluate_Q_learning :  \n",
    "    def __init__(self, S, A, reward_func, alpha=0.1,  epsilon = 0.001, gamma = 0.9, num_episode = 10, len_episode = 20) : \n",
    "        self.S = S \n",
    "        self.A = A \n",
    "        self.reward_func = reward_func\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon \n",
    "        self.gamma = gamma \n",
    "        self.num_episode = num_episode\n",
    "        self.T = len_episode\n",
    "        self.Q = self.initiate_Q()\n",
    "#        self.b, self.C = self.initiate_b()\n",
    "#        self.pi = pi \n",
    "#        self.V = self.initiate_V() \n",
    "    \n",
    "    def initiate_Q(self) : # Q(s,a) 값을 초기화 \n",
    "        Q_dict = defaultdict(float)\n",
    "        for s in self.S : \n",
    "            for a in self.A : \n",
    "                Q_dict[(s,a)] = 0         \n",
    "        \n",
    "        return Q_dict\n",
    "    \n",
    "    def pi(self, s) : #Q 값을 기반해서 가장 가치가 높은 행동 a 산출 \n",
    "        lst = [] \n",
    "        for a in self.A : lst.append(self.Q[(s,a)])\n",
    "        return choose_random_max(lst)\n",
    "    \n",
    "\n",
    "    def choice_action(self, s, policy) : #일반화. 정책 기반으로 상황 s에 있을 때 선택할 행동 a 산출 \n",
    "        policy_a_list = [] \n",
    "        for _ in self.A :\n",
    "            policy_a_list.append(policy(s,_)) \n",
    "\n",
    "        a= random.choices(self.A, weights = policy_a_list)\n",
    "        a = a[0]\n",
    "        return a\n",
    "    \n",
    "    def next_s(self, s,a) : # 상태 s에서 a 행동을 했을 때 다음 상태 s'. 정책, S,A 에 따라 달라짐. \n",
    "        return min(max(s+a, 0), max(self.S)) \n",
    "\n",
    "    def make_episode(self, start_s, T) :\n",
    "        s = start_s\n",
    "        episode = {\"S\" : [], \"A\" : [], \"R\" : []}\n",
    "        episode[\"R\"].append(0) # R_0 값 부여 \n",
    "        for _ in range(T) : \n",
    "            episode[\"S\"].append(s)\n",
    "            a = self.pi(s)  # pi 함수가 결정론적으로 a 값을 반환함에 따라 수정 \n",
    "            next_s = self.next_s(s, a)\n",
    "            r = self.reward_func(next_s, a, s)\n",
    "            episode[\"A\"].append(a)\n",
    "            episode[\"R\"].append(r)\n",
    "            s = next_s\n",
    "        return episode \n",
    " \n",
    "    def update_returns(self) : # Q 추정 및 제어를 위해 수정 \n",
    "        \n",
    "        for _ in range(self.num_episode) : \n",
    "            start_s = random.choice(self.S) # 시작 탐험 가정 \n",
    "            episode = self.make_episode(start_s, self.T) \n",
    "            S,R,A = episode['S'],episode['R'], episode['A'] \n",
    "            # make_episode 에서 이미 a,r를 계산해 두었기 때문에 Q(s,a)만 갱신하겠음. \n",
    "            for index, s in enumerate(S[:-1]) :  \n",
    "                index_origin_s = self.S.index(s)\n",
    "                next_s = S[index+1]\n",
    "                max_a = max([self.Q[(next_s,a)] for a in self.A])\n",
    "                self.Q[(s, R[index])] = self.Q[(s,R[index])] + self.alpha*(R[index+1] + self.gamma*max_a - self.Q[(s, R[index])]) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "515ce02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 1, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "test = evaluate_Q_learning(S,A, reward_func)\n",
    "lst = [] \n",
    "\n",
    "for _ in range(10000) : \n",
    "    m_lst = [] \n",
    "    for s in S :\n",
    "        q_lst = [test.Q[(s,a)] for a in A]\n",
    "        max_a = A[q_lst.index(max(q_lst))]\n",
    "        m_lst.append(max_a)\n",
    "    lst.append(m_lst)\n",
    "    test.update_returns()\n",
    "\n",
    "print(lst[0])\n",
    "print(lst[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d23bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0fa952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4d2850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36f0d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf10d2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
