{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc5dbdc",
   "metadata": {},
   "source": [
    "### <Q1, Q2를 추정하기 위한 이중 Q 학습> \n",
    "출처 : 단단한 머신러닝 챕터 6 \n",
    "\n",
    "알고리즘 파라미터 : 시간 간격 $\\alpha \\in (0,1] $, 작은 양수 $\\epsilon > 0$ \n",
    "\n",
    "모든 $s \\in S^+$ 에 대한 Q1(s,a), Q2(s,a)를 임의의 값으로 초기화. 단, Q(종단, -) = 0 \n",
    "\n",
    "각 에피소드에 대한 루프 : \n",
    "- S를 초기화 \n",
    "- 에피소드의 각 단계에 대한 루프 : \n",
    "\n",
    "> Q1 +Q2 에 있어서 입실론 탐욕적인 정책을 사용하여 S'으로부터 A'를 선택 \n",
    "\n",
    "> 행동 A를 취하고 $R,S'$ 을 관측 \n",
    "\n",
    "> 0.5의 확률로:\n",
    "  >Q1(s,a) ← $Q_1(s,a) + \\alpha[R+ \\gamma  Q_2(s',argmax_a Q_1(S',a)) - Q_1(s,a)] $\n",
    "\n",
    "> 그 밖의 경우 : \n",
    "  >Q2(s,a) ← $Q_2(s,a) + \\alpha[R+ \\gamma Q_1(s',argmax_a Q_2(S',a)) - Q_2(s,a)] $ \n",
    "\n",
    "> S ← S', A ← A' \n",
    "\n",
    "S가 종단이면 종료 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5187f8b2",
   "metadata": {},
   "source": [
    "**<구현해야 하는 것>**\n",
    "- Q1(s,a) : \n",
    "- Q2(s,a) : \n",
    "- 입실론 탐욕적인 정책 \n",
    "\n",
    "\n",
    "**<필요한 것>** \n",
    "- $\\alpha$ : class 제작시 입력 값으로 부여 \n",
    "- $\\epsilon$ : 충분히 작은 값으로 \n",
    "\n",
    "**<함수 / 데이터 형식>** \n",
    "- class evaluate_TD : # alpha 값 추가 \n",
    "> def __init__(self, S, A, alpha reward_func, epsilon = 0.001, gamma = 0.9, num_episode = 10, len_episode = 20) : \n",
    "\n",
    "\n",
    "**<외부함수>** \n",
    "- R(s',a,s) : 보상함수. 챕터 4의 코드 참고 \n",
    "- choose_random_max(lst) : lst 중 가장 값이 큰 것을 반환. 혹시 max 값이 중복된다면 임의의 하나 산출\n",
    "\n",
    "**<고민점>** \n",
    "- Q1 + Q2 를 어떻게 표현해야 하는가? \n",
    "\n",
    "- $pi(s,a) = \\pi(a|s)$ 를 표현할 데이터 형식 필요. 갱신을 위해서는 함수 형태가 아니라 데이터 형식이 필요함 \n",
    "\n",
    "> 이중 리스트, dict가 있음. 이 중에서 dict 사용 \n",
    "\n",
    "- pi(s,a)를 언제마다 갱신할 것인가? \n",
    "\n",
    "> update_return 함수 시작 부분에 하면 매번 상호 영향을 줄 수 있을 듯 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af0f567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 용 임시 데이터 \n",
    "S = list(range(100)) \n",
    "A = list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35f37b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee43c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외부용 함수 reward_func 간략 구현 \n",
    "def reward_func(next_s, a, s) : \n",
    "    # next_s 와 s의 차이가 짝수이면 +1, 홀수면 -1 \n",
    "    if abs(next_s - s) %2 == 0 and next_s > s : reward = 1 \n",
    "    else : reward = -1\n",
    "    \n",
    "    return reward \n",
    "    \n",
    "#최대값이 2개 이상인 경우, 임의로 1개의 최대값을 만들어낸 행동 a를 산출 \n",
    "def choose_random_max(lst) :\n",
    "    max_arg = np.where(np.array(lst) >= max(lst))\n",
    "    return random.choice(list(max_arg)[0]) #max_arg가 array 형태로 안에 있는 list를 꺼내기 위해 [0] 사용 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6ccecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_Q_TD 코드 참고\n",
    "# 마지막 Q(s,a) 갱신 부분만 변경 \n",
    "\n",
    "class evaluate_double_Q :  \n",
    "    def __init__(self, S, A, reward_func, alpha=0.1,  epsilon = 0.001, gamma = 0.9, num_episode = 10, len_episode = 20) : \n",
    "        self.S = S \n",
    "        self.A = A \n",
    "        self.reward_func = reward_func\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon \n",
    "        self.gamma = gamma \n",
    "        self.num_episode = num_episode\n",
    "        self.T = len_episode\n",
    "        self.Q1, self.Q2 = self.initiate_Q(), self.initiate_Q() \n",
    "        self.pi = self.initiate_pi() \n",
    "\n",
    "#        self.b, self.C = self.initiate_b()\n",
    "#        self.V = self.initiate_V() \n",
    "    \n",
    "    def initiate_Q(self) : # Q(s,a) 값을 초기화 \n",
    "        Q_dict = defaultdict(float)\n",
    "        for s in self.S : \n",
    "            for a in self.A : \n",
    "                Q_dict[(s,a)] = 0         \n",
    "        \n",
    "        return Q_dict\n",
    "    \n",
    "    def initiate_pi(self) : # pi(s,a) 값을 초기화 \n",
    "        pi_dict = defaultdict(float)\n",
    "        for s in self.S : \n",
    "            for a in self.A : \n",
    "                pi_dict[(s,a)] = 1/len(self.A)         \n",
    "        \n",
    "        return pi_dict\n",
    "    \n",
    "    def update_pi(self, s) : # 입실론 탐욕적 정책에 맞게 값 변경. 확률값 list로 반환할 것. \n",
    "        lst = [] \n",
    "        opt_a = choose_random_max([self.Q1[(s,a)]+ self.Q2[(s,a)] for a in self.A])\n",
    "        for a in self.A : \n",
    "            if a == opt_a : self.pi[(s,a)] = (1-self.epsilon) + (1/self.epsilon)/len(self.A) \n",
    "            else : self.pi[(s,a)] = (1/self.epsilon)/len(self.A)\n",
    "                \n",
    "\n",
    "    def choice_action(self, s, policy) : #일반화. next_s 까지 반환하도록 수정 \n",
    "        policy_a_list = [] \n",
    "        for _ in self.A :\n",
    "            policy_a_list.append(policy[(s,_)])  \n",
    "        a= random.choices(self.A, weights = policy_a_list)\n",
    "        a = a[0]\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def next_s(self, s,a) : # 상태 s에서 a 행동을 했을 때 다음 상태 s'. 정책, S,A 에 따라 달라짐. \n",
    "        return min(max(s+a, 0), max(self.S)) \n",
    "\n",
    "    def make_episode(self, start_s, T) :\n",
    "        s = start_s\n",
    "        episode = {\"S\" : [], \"A\" : [], \"R\" : []}\n",
    "        episode[\"R\"].append(0) # R_0 값 부여 \n",
    "        for _ in range(T) : \n",
    "            episode[\"S\"].append(s)\n",
    "            a = self.choice_action(s, self.pi)   \n",
    "            next_s = self.next_s(s, a)\n",
    "            r = self.reward_func(next_s, a, s)\n",
    "            episode[\"A\"].append(a)\n",
    "            episode[\"R\"].append(r)\n",
    "            s = next_s\n",
    "        return episode \n",
    " \n",
    "    def update_returns(self) : # Q 추정 및 제어를 위해 수정 \n",
    "        for s in self.S : self.update_pi(s)\n",
    "        \n",
    "        for _ in range(self.num_episode) : \n",
    "            start_s = random.choice(self.S) # 시작 탐험 가정 \n",
    "            episode = self.make_episode(start_s, self.T) \n",
    "            S,R,A = episode['S'],episode['R'], episode['A'] \n",
    "            # make_episode 에서 이미 a,r를 계산해 두었기 때문에 Q(s,a)만 갱신하겠음. \n",
    "            rand = random.choice([0,1])\n",
    "                \n",
    "            for index, s in enumerate(S[:-1]) :  \n",
    "                index_origin_s = self.S.index(s)\n",
    "                next_s = S[index+1]\n",
    "                if rand == 0 : \n",
    "                    max_a = self.A[np.argmax([self.Q1[(next_s,a)] for a in self.A])] # 행동 값으로 나와야 함. \n",
    "                    self.Q1[(s, R[index])] = self.Q1[(s,R[index])] + self.alpha*(R[index+1] + self.gamma*self.Q2[(next_s, max_a)] - self.Q1[(s, R[index])]) \n",
    "                    \n",
    "                else : \n",
    "                    max_a = self.A[np.argmax([self.Q2[(next_s,a)] for a in self.A])] # 행동 값으로 나와야 함. \n",
    "                    self.Q2[(s, R[index])] = self.Q2[(s,R[index])] + self.alpha*(R[index+1] + self.gamma*self.Q1[(next_s, max_a)] - self.Q2[(s, R[index])]) \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22ff89a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
      "[[1, 1], [1, 1], [1, 1], [1, 2], [2, 1], [1, 1], [2, 0], [1, 2], [1, 0], [1, 0], [0, 1], [0, 0], [0, 0], [0, 2], [2, 2], [0, 2], [0, 2], [0, 1], [1, 2], [0, 0], [0, 2], [2, 2], [0, 2], [2, 2], [1, 0], [2, 2], [1, 2], [1, 1], [2, 2], [0, 2], [0, 2], [2, 2], [0, 2], [2, 0], [2, 2], [1, 1], [2, 0], [0, 0], [2, 0], [0, 1], [2, 0], [0, 2], [2, 0], [2, 1], [0, 2], [2, 2], [2, 1], [2, 2], [0, 2], [2, 0], [2, 0], [2, 2], [2, 2], [0, 2], [2, 2], [0, 1], [0, 2], [0, 2], [0, 2], [0, 2], [2, 1], [2, 2], [0, 2], [0, 2], [2, 1], [0, 0], [2, 1], [2, 2], [2, 0], [1, 0], [0, 2], [1, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 1], [2, 1], [1, 0], [2, 2], [0, 2], [0, 0], [0, 2], [2, 1], [2, 1], [2, 2], [2, 2], [2, 2], [0, 2], [1, 2], [1, 1], [2, 2], [1, 1], [2, 2], [0, 1], [2, 2], [1, 1], [2, 2], [2, 2]]\n"
     ]
    }
   ],
   "source": [
    "test = evaluate_double_Q(S,A, reward_func)\n",
    "lst = [] \n",
    "\n",
    "for _ in range(100) : \n",
    "    m_lst = [] \n",
    "    for s in S :\n",
    "        q1_lst = [test.Q1[(s,a)] for a in A]\n",
    "        q2_lst = [test.Q2[(s,a)] for a in A]\n",
    "\n",
    "        max_a = A[q1_lst.index(max(q1_lst))]\n",
    "        max_b = A[q2_lst.index(max(q2_lst))]\n",
    "        m_lst.append([max_a, max_b])\n",
    "    lst.append(m_lst)\n",
    "    test.update_returns()\n",
    "\n",
    "print(lst[0])\n",
    "print(lst[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df529b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "rand = random.choice([0,1])\n",
    "print(rand)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
