{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48fe2d59",
   "metadata": {},
   "source": [
    "### <최적 정책을 추정하기 위한 비활성 정책 MC 제어> \n",
    "\n",
    "모든 $s \\in S, a \\in A(s)$에 대해 초기화 : \n",
    "- $Q(s,a) \\in R$ (임의의 값으로 설정) \n",
    "- $C(s,a)$ <- 0 \n",
    "- $\\pi(s)$ <- $argmax_aQ(s,a)$ (최대가 되는 a가 여러 개인 경우 한 가지 행동만 선택) \n",
    "\n",
    "(각 에피소드에 대해) 무한 루프 : \n",
    "- b <- 임의의 소프트 정책 \n",
    "- 정책 b를 활용하여 에피소드를 생성 : $S_0, A_0, R_1, ..., S_{T-1}, A_{T-1}, R_T$ \n",
    "- G <- 0 \n",
    "- W <- 1 \n",
    "- 에피소드의 각 단계에 대한 루프, t = T-1, T-2, ..., 0 : \n",
    "\n",
    "> G <- $\\gamma G + R_{t+1}$ \n",
    "\n",
    "> $C(S_t, A_t) <- C(S_t, A_t) + W$\n",
    "\n",
    "> $Q(S_t, A_t)$ <- $Q(S_t, A_t) + \\frac {W}{C(S_t, A_t)} (G-Q(S_t, A_t))$\n",
    "\n",
    "> $\\pi(S_t)$ <- $argmax_a Q(S_t,a)$\n",
    "\n",
    "> $A_t \\neq \\pi(S_t)$ 이면 루프를 종료 \n",
    "\n",
    "> W <- $W \\frac{1}{b(A_t|S_t)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9796b1",
   "metadata": {},
   "source": [
    "**구현해야 하는 것** \n",
    "- Q(s,a) : 기존 형식 유지 \n",
    "- C(s,a) : 기존 형식 유지\n",
    "\n",
    "- b <- 정책 $\\pi$ 가 보증된 임의의 정책 : 모든 확률을 동일하게 설ㅈ어  \n",
    "\n",
    "- $\\pi(s,a)$ : 내부 함수로 구현할 것. dic 형태로 구현 \n",
    "\n",
    "\n",
    "**필요한 것** \n",
    "- 외부함수 reward func\n",
    "\n",
    "- def choose_random_max(lst) : 최대값이 2개 이상인 경우, 임의로 1개의 최대값을 만들어낸 행동 a를 산출 \n",
    " \n",
    "\n",
    "**함수/데이터의 형태** \n",
    "- class epsilon_soft_MC : \n",
    "\n",
    "> def __init__(self, S, A, reward_func,  epsilon=0.001, gamma = 0.9, num_episode = 10, len_episode = 20) \n",
    "\n",
    "> def choice_sample(self, list, prob) : 기존 함수 유지 \n",
    "\n",
    "> def make_episode(self, start_s, T, pi) : 기존 함수 유지  \n",
    "\n",
    "> def update_returns(self) : 비활성 MC에 따라서 구현 \n",
    "\n",
    "**전제**\n",
    "- 상태 s에서 행동 a를 했을 때 결정론적으로 s' 상태로 변화한다. \n",
    "\n",
    "<외부 함수> \n",
    "- def reward_func(s',a,s) : \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f8b821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 용 임시 데이터 \n",
    "S = list(range(100)) \n",
    "A = list(range(-5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2470bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c6a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외부용 함수 reward_func 간략 구현 (이전 예시 활용)\n",
    "def reward_func(next_s, a, s) : \n",
    "    # next_s 와 s의 차이가 짝수이면 +1, 홀수면 -1 \n",
    "    # 단, a의 크기에 반비례함. \n",
    "    if abs(next_s - s) %2 == 0 : reward = 1 \n",
    "    else : reward = -1\n",
    "    \n",
    "    if a == 0 : \n",
    "        return 0 \n",
    "    else : \n",
    "        return reward / a # 즉, a가 양수이며 짝수이며, 가능한 작을 때 (=2) 일 때 최대의 보상이 주어지도록 설정 \n",
    "    \n",
    "#최대값이 2개 이상인 경우, 임의로 1개의 최대값을 만들어낸 행동 a를 산출 \n",
    "def choose_random_max(lst) :\n",
    "    max_arg = np.where(np.array(lst) >= max(lst))\n",
    "    return random.choice(list(max_arg)[0]) #max_arg가 array 형태로 안에 있는 list를 꺼내기 위해 [0] 사용 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb477ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성 접촉 MC 제어 코드 참고 \n",
    "\n",
    "class epsilon_soft_MC : \n",
    "    def __init__(self, S, A, reward_func, epsilon = 0.001, gamma = 0.9, num_episode = 10, len_episode = 20) : \n",
    "        self.S = S \n",
    "        self.A = A \n",
    "        self.epsilon = epsilon \n",
    "        self.reward_func = reward_func\n",
    "        self.pi = [0]*len(self.S)   # 값을 수정할 수 있는 데이터 형식으로 바꿔야 함. 결정론 방식이라 list로 구현 \n",
    "        self.b, self.C = self.initiate_b() \n",
    "        self.gamma = 0.9 \n",
    "        self.num_episode = num_episode\n",
    "        self.T = len_episode\n",
    "        \n",
    "        self.set_episode = [] # 추후에 어떤 episode들이 있었나 확인용 \n",
    "        \n",
    "        self.Q, self.return_lst, self.s_prob = self.initiate() \n",
    "    \n",
    "    \n",
    "    def initiate_b(self) : \n",
    "        # pi를 보증하는 b 정책 초기화하기. 예측이기 때문에 b 정책이 바뀔 일은 없음.\n",
    "        # C(s,a) 도 같이 초기화 \n",
    "        b_dic = defaultdict(float)\n",
    "        c_dic = defaultdict(float)\n",
    "        for s in self.S :\n",
    "            for a in self.A :  \n",
    "                b_dic[(s,a)] = 1/len(self.A)\n",
    "                c_dic[(s,a)] = 0\n",
    "        return b_dic, c_dic\n",
    "    \n",
    "    def initiate(self) : # V 대신 Q(a,s)를 초기화해야 함. \n",
    "        Q = defaultdict(float)\n",
    "        s_prob = defaultdict(list)\n",
    "        for s in self.S : \n",
    "            s_prob[s] = [0]*len(self.A)\n",
    "            for index, a in enumerate(self.A) : \n",
    "                Q[(a,s)] = 0 \n",
    "                s_prob[s][index] = self.b[(s,a)] # pi에 대한 식 수정 필요\n",
    "\n",
    "        return_lst = [[0]*len(self.A)]*len(self.S) # return_lst의 데이터 형식 변경\n",
    "        return Q, return_lst, s_prob\n",
    "    \n",
    "    def choice_sample(self, s) : \n",
    "        a= random.choices(self.A, weights = self.s_prob[s])\n",
    "        \n",
    "        # 수정. a[0] +s 가 s의 범위 안에 들어오도록 수정 \n",
    "        return a[0], min(max(s+a[0],0), 99)  \n",
    "    \n",
    "    def make_episode(self, start_s, T) :\n",
    "        s = start_s\n",
    "        episode = {\"S\" : [], \"A\" : [], \"R\" : []}\n",
    "        episode[\"R\"].append(0) # R_0 값 부여 \n",
    "        for _ in range(T) : \n",
    "            episode[\"S\"].append(s)\n",
    "            a, next_s = self.choice_sample(s)\n",
    "            r = self.reward_func(next_s, a, s)\n",
    "            episode[\"A\"].append(a)\n",
    "            episode[\"R\"].append(r)\n",
    "            s = next_s\n",
    "        return episode \n",
    " \n",
    "    def update_returns(self) : # epsilon - soft 행동 가치 함수에 맞춰 변경\n",
    "        for _ in range(self.num_episode) :\n",
    "            # 시작 s는 시작 가정에 따라 랜덤하게 산출하겠음. \n",
    "            start_s = random.choice(self.S)\n",
    "            episode = self.make_episode(start_s, self.T)\n",
    "            G = 0 \n",
    "            W = 1 \n",
    "            \n",
    "            # G와 returns(S_t)를 업데이트 하는 순서가 각각 역순이라 따로 구현\n",
    "            for t in reversed(range(self.T)) : \n",
    "                s, a = episode[\"S\"][t], episode[\"A\"][t]\n",
    "                G = self.gamma *G + episode[\"R\"][t]\n",
    "                self.C[(s,a)] = self.C[(s,a)] + W \n",
    "                self.Q[(s,a)] = self.Q[(s,a)] + W/self.C[(s,a)] *(G - self.Q[(s,a)])\n",
    "                q_a_list = [] \n",
    "                for _ in self.A : q_a_list.append(self.Q[(s,_)])\n",
    "                self.pi[s] = choose_random_max(q_a_list)\n",
    "                if a != self.pi[s] : break\n",
    "                W = W/self.b[(s,a)]\n",
    "\n",
    "                \n",
    "\n",
    "        \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8593815b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 9, 9, 8, 9, 9, 7, 4, 4, 4, 7, 4, 4, 2, 4, 4, 4, 2, 0, 4, 7, 2, 7, 7, 4, 0, 4, 7, 2, 4, 4, 7, 4, 4, 4, 8, 7, 9, 3, 5, 5, 1, 5, 1, 9, 1, 2, 4, 0, 8, 5, 4, 9, 6, 3, 7, 6, 0, 3, 0, 1, 1, 8, 6, 3, 7, 1, 5, 2, 0, 5, 8, 1, 4, 0, 0, 0, 3, 0, 8, 0, 0, 0, 0, 5, 1, 0, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 점검 \n",
    "test = epsilon_soft_MC(S,A,reward_func, num_episode = 100000, len_episode = 1000) \n",
    "test.update_returns() \n",
    "#print(test.Q.values())\n",
    "print(test.pi)\n",
    "#print(test.return_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230182c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65001268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61486e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc8303c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28afe521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0878022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5164a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c11bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
