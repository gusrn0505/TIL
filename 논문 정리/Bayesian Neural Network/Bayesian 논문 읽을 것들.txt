<Bayesian 의의>
17 : 모르는 걸 모른다고 하자 
> A baseline for detecting misclassified and out of distribution examples in neural networks

27 : Bayesian 근사라도 충분히 모델을 구체화할 수 있다. 
> Being Bayesian, even just a bit, fixes overconfidence in ReLU network

<Bayesian 방법론>
46 : How to represent learning algorithms using general PGM
> Operations for learning with graphical models 
22 : Unidentifiability in BNN 이해하기 위해! 
> What are Bayesian neural network posteriors really like? 
81 : HMC 이해
> MCMC using Hamiltonian dynamics
85 : SVI 이해 
> Stochastic variational inference
90 : probabilistic backpropagation 이해
> Probabilistic backpropagation for scalable learning of Bayesian neural networks 
111 : Rather than storing MCMC samples using a generative model G 
> Adversarial distilation of Bayesian neural network posteriors 

<Bayesian 기반 Inference method>
24 : Generative model에 사용된 Bayesian 
> Stochastic gradient VB and the variational autoencoder
41, 42 : Active learning 
> Deep Bayesian active learning with image data
> Bayesian generative active deep learning 
74 : Meta learning in Bayesian statistics 
> Recasting gradient-based meta-learning as hierarchical Bayes
98 : MC-dropout
> Unlabelled data improves Bayesian uncertainty calibration under covariate shift
99 : SGD can be reinterpreted as a Markov Chain algorithm 
> Stochastic gradient descent as approximate Bayesian inference
100 : Stochacstic gradient Langevin dynamic 
> Bayesian learning via stochastic gradient Langevin dynamics 
101 : Warm restart of the altorithm 
> Towards cakubrated and scalable uncertainty representations for neural networks 
108 : train non-stochastic ANN to predict the marginal probability p using a BNN as a teacher 
> Bayesian dark knowledge 

<측정수단>
112 : The predictive performance for Bayesian
> On loss functinos for deep neural networks in classification 
113 : Calibration curve for measuring confidence
 > Accurate uncertainties for deep learning using calibrated regression 


<Bayesian 관점 해석>
Gal & Ghahramain, 2015 / 2016 : Dropout에 대해서 Baysian approximation으로 해석함 
> Bayesian convolutional neural networks with bernoulli appromate variational inference 
>Dropout as a Bayesian approximation : Representing model uncertainty in deep learning 

 Gal et al., 2017 ; Boluko et al., 2020 : model parameter W은 Bayesian inference based 로 여겨질 수 있다. 
>Concrete dropout  [o]
>Learnable Bernoulli dropout for Bayesian deep learning  

Zhang et al (2019) : Bayesian framework로 관측된 graph을 재해석하여 의미를 도출함. 
> Bayesian graph convolutional neural networks for semi-supervised classification 

<Direct Optimization>
필수 : ARM 논문 확인필요  Augment-REINFORCE-Merge(
=> Stochastic binary layer을 Backprobagation 할 때 unbiased / low variance / low computational complexity를 가져온다
> ARM: augment-Reinforcement gradient for stochastic binary networks 

<CGN 새로운 해석>
Kipf & Welling, 2017 : GCN은 Laplacian smoothing의 Special form이다. 
> Semi-supervised classification wigh graph convolution networks

