참고할 것 
1. Distance matrix 제작 방법 
2. auto encoder 적용 필요 

문제점 발견 
- 차원이 높을수록, 접하는 subgraph의 개수가 많아진다 => 조건을 만족시키기 어렵다. 
- 현재로선 접하는 subgraph의 수를 적정양으로 줄여내는 것이 필요하다. 이를 줄이기 위해선 반지름을 줄이거나, 차원을 줄여야 한다.  

- 반지름을 줄이는 것은 Sample size가 더욱 커져야 하기 때문에 취지와 맞지 않다. 
- 따라서 정보양을 유지하면서 차원 축소를 해내는 것이 필요하다. 
unsupervised dim-reduction에는 PCA, AE등이 있다. 

단, 차원 축소가 loss되는 정도가 있다. 이것과 balance를 잘 맞춰야할듯
reconstriction error을 줄이는 것이 중요하다. 
=> 관련 방법들을 찾아봅시닷. 


적정한 차원 축소 방안을 찾을 수 있나? 
2차원 이여야 최대 6개로 줄일 수 있었다만, 
3차원이면 주변에 최대 몇개까지 있을지는... 모르겠쿤.. 너무 복잡해진다. 
n 차원이면 계산양이 미쳐날뛰기 시작한다.

1개 차원이 추가될 때마다 접하는 subgraph의 개수가 최대 6배씩 늘어난다. 

=> 그냥 coreset을 적용할 순 없다. 



기존의 코드가 NN에 맞춰져 있기 때문에 코드를 대거 수정할 필요가 있다. 

<구현할 것>
Class 형태로 표현을 해야겠다. 

def coreset selection (input : Unlabel Dataset, or distance matrix, sampling size s  output : Sampling index, delta) 
=> 기존 코드 참고 
 * sampling index와 delta 값을 

self.original_distance_matrix  (n x n matrix) - tensor 등 데이터 형식 고려 필요  
self.original_dataset (n x d matrix) 
- Label Data와 Unlabel Data의 거리값도 구할 수 있어야 함. 
- 1차 Pseudo labeling 된 Data와 2차 Pseudo labeling 된 것을 구분하자 .
* 이건 1차 Pseudo labeling 된 것의 정확도를 한번 확인하고나서 고민할 지점 

self.uu_distance_matrix  (n-s x n-s matrix)

self.ll_distance_matrix (s x s matrix) 

self.la_distance_matrix(s x n matrix) 

self.label_dataset ( s x d matrix) 
self.unlabel_dataset ( n-s x d matrix) 

# 이걸 self.num_selection으로 하는 게 맞으려나~
self.num_selection (input : p, output : sampling index, N, delta)
- p번째 Coreset selection 했을 때의 Sampling point들(또는 Sampling size)과 반지름이 무엇인지, 
=> Label Data에 p 회차 Sampling point 에 대해서 순서대로 들어갈 것이니, Sampling size만 알아도 되겠다. 

self.N_subgraph
 - add. list(self.num_selection) 




def distance (input : vector, output : scalar) 
- 다양한 Distance 를 나중에 적용할 수 있도록 
- Uclidean을 시작으로 생각하자. 

def modify_distance_matrix(input : uu_distance matrix, sampling index, output : uu_distance matrix , ll_distance matrix, lu_distance matrix) 
 * return 없음. self. ll / uu / ul 수정하기 
 - Sampling Data가 결정되면 Distance_matrix도 그에 맞게 변경하기 
 => unlabel-unlabel :  Row 제거, Column 제거.
 => label-unlabel : Row 추가. 추후 labeling 되는 column 제거.  
 - 추후 psedo labeling은 나중에 생각을 해봅시다. 

def modify_dataset(input : sampling index, output : x 
* return 없음. label / unlabel dataset 변경. 
 - def labeling 활용하기. 

def labeling (input : sampling data vector, output : label) 
=> Original Data에서 vector의 index를 확인하는게 제일 낫겠다.  아니면 p차 coreset seletion 간에 너무 복잡해져. 


def make_subgraph (input : la_distance, sampling index, delta, output : W) 
* subgraph 만들 때, label data는 Sampling index를 기반ori으로 중복되지 않도록 고려해줄 것. 
* 밀도 체크는 그때 그때 합시당. list_Gi의 개수만 세면 되니까. 
- la_distance 에서 distance가 delta 보다 작거나 같으면 1의 값 부여. 아닌 경우 0의 값 부여. 이후에 Wij에서 i = index(sampling index)이며, j = sampling index 일 때 0이 값으로 부여. (label data는 subgraph에 속했다고 표시 x. 중복을 피하기 위해) 
- x에 대해서 column은 어느 subgraph에 속하는지 확인가능 
- sum(row)는 각 subgraph의 밀도를 의미함. 

def check_class(input : label data index, output : True or False) 
=> x가 속한 subgraph들의 index(=label data index)를 넣으면, 각 Class들의 label이 같은지 아닌지 반환하기 

def pseudo_labeling(input : W, label_dataset, sampling index, M,  output : pseudo_lableling_set, pseudo lableing index ) 
 pseudo_labeling_set = [] 
 pseudo_labeling_index = [] 
- for i in range(n) except sampling index : 
	subgraph_index = column of W 중에서 1의 값을 가지는 index
 if check_class(subgraph_index) = 1 :
	max_density_subgraph = 0 
	for j in subgraph_index: 
		max_density_subgraph = max(0, sum(W[j]))  
	if  max_density_subgraph > M : 
		pseudo_labeling_index.append(i) 
		pseudo_labeling_set.append(label_dataset[subgraph_index[0]][0] : label, original dataset[i]) # pseudo labeling 및 각 데이터의 원본을 저장하기 

return pseudo_labeling_set, pseudo_labeling_index 


-------------------------------------------------------------------------------------
<차후에 고민할 점>
def make_adjacency_matrix(input : ll_distance, delta, output : adjacency matrix) 
- 어떤 형태로 접하고 있는지 확인 필요 


def check_prediction (input : pseudo sample, output : prediction) 
- psudo sample에 대한 prediction과 실제 label과 동일한지 유무 체크 





1. Coreset selection을 통해서 sample을 선정한다. 
=> 코드 확인 어느 정도까지 Sampling 하는가? 
=> Coreset이 어떠한 Dataset을 썼는지 확인하고 적용하기 

=> Sampling 할 때 index를 뽑을 건지 유무 확인 필요. 기존 코드 상  label class로 넘겨버렸던 것 같은데. 
==> 아예 Dataset을 구분하는 게 쪽으로 구현하는게 좋겠다. 그래야 p차 Coreset을 할 때 고려할 수 있을 듯. 

* Sampling 한 데이터들은 Label Dataset으로 보내야 한다.
** 남아있는 데이터들에 대해서 어느 subgraph에 속했는지 알야아 한다. 
=> 샘플링 사이즈 
=> 샘플링 사이즈를 기록할 수 있는 Data sampling Size list가 [] 가 필요하겠다. 계속하면서 더하는 형태로. 아니면 Label Dataset에 이중 list 형태로 기록을 해야할려나. 이 정보가 나중에 활용될지를 체크해야겠네. 

=> 새로운 데이터가 추가되었을 때 어떤 subgraph에 속하는지 확인하려면 필요함. 




2. ui 간의 거리 조건을 기반으로 Adjacency graph를 형성한다. 
-> 형성해야 하나? ui에 대한 index만 알면 distance matrix 에서 바로 비교할 수 있겠는걸 
=> adjacency 유무 & 클래스가 모두 동일한지만 체크하면 된다.

3. 각 subgraph 별로 밀도를 측정한다.
- Gi 별 어떻게 소속되어 있는지 유무를 확인할 수 있어야 한다. (ex- index)


4. 조건을 만족한 subgraph에 속하는 xik 들에 대해서 ui와 동일한 prediction을 부여한다.
- 조건 1. 클래스가 동일할 것 
- 조건 2. 밀도가 일정 이상 일 것 

input을 xk로 넣고 그 결과가 Prediction이 나오게 할 수 있을까? 
결과값으로는 판단불가, 혹은 Class 정보의 형태로 나오게. 

흠... 그러면 new input data x'을 넣었을 때, 

for p in range(P) : 
1). x'에 속하는 subgraph Gi 들과 당시의 반지름 값을 확인한다. 
 - p차 번째 Coreset selection 간 어느 subgraph에 속한지 확인한다. (ui) 들과의 거리를 측정할 수 있다. (흠.. distance matrix 사용하기가 조건이 까다로워지네) 
-


2). 속한 subgraph Gi 중에서 조건을 만족한다면 
 - 조건 1 유무를 확인할 수 있어야 겠네. 


Pseudo labeling 한 값들과, 진짜 Labeling 한 것들을 분리해야 한다.
Pseudo labeling 한 값들은 


추후 필요한 것 
- x ik 에 대해서 어떤 클래스, 반지름의 subgraph에 몇 번 속했는지 기록 필요. 
- 초기에 pseudo labeling 한 값들을 제외한 Dataset을 운용하며, 또한 이후의 Dataset에 대해서 두번째 방법을 적용할 수 있어야 한다. 