# Topic 1 차원 감소(Dimensionality Reduction)

출처 : 강필성 교수님의 Business Analytics 강의 

출처 : 단단한 머신러닝 챕터 10 - 차원 축소와 척도 학습

##### 고민 & 해결할 점

1. LLE 에서 각 $W_{ij}$ 를 구하는 방법이 생략되어 있음. 확인할 것 
   
   - $\ W_{ij}$ 계산 과정은 찾음. 하지만 해당 결과로 전개하는 방식은 못찾음. 

## 

##### 차원 축소가 필요한 이유

1. <mark>차원 감소를 통한 계산 효율 향상. </mark>
   
   - 차원이 높을 수록 차원의 저주에 의해 필요로 하는 데이터의 수가 많아지며, 이는 계산량 증가로 연결된다.
     
     > **차원의 저주** : 차원이 증가할 때 동일한 설명을 갖추기 위해서 필요로 하는 사례의 수가 **지수적으로 증가**한다.
   
   - 정보를 유지하는 차원(Intrinsic dimension)은 기존 차원에 비해 작은 경우가 있다. 고려 요소가 많을수록 모델 성능을 향상할 것 같지만, 이는 각 요소들이 독립성을 갖췄을 때에 해당한다. 이는 매우 비현실적인 전제로 변수간 의존성을 제거함으로써 정보 손실을 최소화하면서 차원을 축소할 수 있다. 

2. <mark>노이즈 감소를 통한 예측 성능 향상 </mark>
   
   - 고차원일수록 노이즈가 생길 가능성이 높아, 예측 성능을 낮춘다

### 

##### 차원 축소의 효과

1. 변수간 상관관계를 제거한다

2. 전처리를 단순화시킨다

3. 관련된 정보를 유지하면서 불필요한 변수를 제거한다

4. 시각화가 가능하다 

##### 차원 축소의 방식

![](./picture/1-1.png)

- **Feature Selection** : 기존에 있는 변수 중 <u>"선택" </u>
  
  > ex) 10개의 변수 중 설명력이 높은 3개 선택
  > 
  > Filter(Unsupervised) : 변수 선택과 모델 학습이 독립적 
  > 
  > Wrapper(Supervised) : 모델 학습의 결과에 최적화된 변수 선정  

- **Feature Extraction** : 기존에 있는 <u>변수를 조합하여 새로운 변수를 형성</u>. 
  
  > ex) z1 = x1 + 2x2, z2 = x3 + 3x4

---

---

## Feature Selection - Wrapper(Supervised)

<img src="./picture/1-2.jpg" title="" alt="" width="372">

- 전역검사 : 가능한 모든 조합을 확인하는 것. 경우의 수 $2^n$ -1 (n : 변수의 개수)

- FS(Forward Selection) : 설명력이 높은 변수부터 차례대로 선택하는 방식 
  
  > 설명력 : 모델이 얼마나 데이터를 잘 설명하는 지를 의미. 설명력 지표(Performance Metrics)로 AIC / BIC / Adjustd $R^2$ 이 있음.
  > 
  > - AIC / BIC 는 최소화해야할 지표임. 
  > 
  > - AIC(Akaike Information Criterion)  = -2 Log Likelihood + 2p
  >   
  >   > -2 Log Likelihood : 모형의 적합도 
  >   
  >   > p : 모형의 추정된 파라미터의 개수 
  > 
  > - BIC(Bayes Information Criterion) = -2 Log Likelihood + log(n)p 
  >   
  >   > n : 변수의 개수
  >   
  >   - BIC는 변수가 많을 때 AIC 보다 페널티를 증가시킴. 
  >   
  >   - => **두 지표 모두 모형의 적합도를 올리되, 동시에 사용하는 파라미터의 개수를 최소화할 것** 
  >     
  >     - ($R^2$단점 해소 : 파라미터 개수 늘리기만 해도 $R^2$ 의 값이 증가) 

- BS(Backward Selection) : FS와 반대방향으로, 설명력이 낮은 변수부터 제거하는 방식 

- SS(Stepwise Selection) : 설명력이 높은 변수부터 선택하되, 선택된 변수들의 조합도 같이 고려

- GA(Genetic Algorithm) : 휴리스틱한 접근 방식으로, 유전학습 과정을 모방한 방식 
  
  > 휴리스틱 : 대충 어림짐작하기. 복잡한 문제를 효율적으로 시행착오를 거치며 성능을 확보

##### Genetic Algorithm 과정

1) 염색체(Chromesome) 초기화 및 파라미터 설정 
   
   - 염색체 초기화 : 각 염색체의 변수마다 랜덤하게 사용 유무 결정 
   
   - 파라미터 : 염색체 수 / 성능 평가 방식(Fitness fuction) / 교배방식(Crossover mechanism) / 돌연변이율(Rate of mutation) / 종료 조건

2) 각 염색체 별 선택 변수에 기반하여 모델 학습 

3) 각 염색체 적합도 평가 

4) 우수 염색체 선택(Selection) 
   
   - 결정론적(Deterministic) 선택 : 성능이 좋은 상위 n%의 염색체만 선정
   
   - 확률존적(Probabilistic) 선택  : 성능에 비례하여 각 염색체가 선정될 확률 부여. 

5) 다음 세대 염색체 생성(Crossoveer & Mutation)
   
   - Crossover : 선택한 염색체들 간 변수값을 교체함. 
   
   - Mutation : 일정 확률로 변수의 값을 변경함
   
   > 이전 세대의 최고 성능을 가진 염색체는 그대로 물려주며 원하는 성능이 나오기 까지 2~5번 과정 반복
   > 
   > <img src="./picture/1-3.jpg" title="" alt="" width="328">
6. 최종 변수 집합 선택 

--- 

### Feature Extraction(Unsupervised)

- 다차원 데이터가 가지고 있는 <mark>특정 정보(ex- 분산, 거리 등)을 최대한 보존</mark>하며 <mark>저차원으로 임베딩 간 대표할 수 있는 값</mark>을 찾는 것이 목표임. 
  
  - Linear Embedding
    
    - PCA : 다차원 데이터의 "**분산**"을 최대한 보존하며 축약하자!    
    
    - MDS : 다차원 데이터의 "**거리 정보**"를 최대한 보존하며 축약하자!
  
  - Unlinear Embedding
    
    - ISOMAP : "비선형"/다차원 데이터의 "**거리 정보**"를 최대한 보존하며 축약하자!
    
    - LLE : "비선형"/다차원 데이터의 "**선형 관계**"를 최대한 보존하며 축약하자!
    
    - t-SNE : "비선형"/다차원 데이터의 "**확률분포**"를 최대한 보존하며 축약하자!

##### 1. PCA(Principal Component Analysis)

- Original Data의 **분산**을 가장 잘 보존하는 축을 찾자!

###### 과정

1. Data Centering 
   
   - 추후 수식 정리의 편의성을 확보하기 위해 분산의 평균을 0으로 맞춘다.
   
   - 분산( $Var(X) =\frac{1}{n} * X X^T$ )에서 $X$ 대신, $X-\overline{X}$를 대입한다.  

2. 분산 최적화 문제로 수식화 한다. 
   
   - <mark>$V = \frac{1}{n}(w^TX)(w^TX)^T$ = $\frac{1}{n}w^TXX^Tw = w^TSw$ </mark>
     
     > $w^TX$는 basis $w$에 대해 전사한 매트릭스를 의미
     > 
     > $S$는 벡터 $x$가 정규화 되었을 때의 공분산 매트릭스로, $S=\frac{1}{n}XX^T$임
   
   - 분산을 최대하는 PCA 목적 상 만족시켜야 하는 조건은 아래와 같다. 
     
     - <mark>$max (w^TSw)$   $s.t.$  $w^Tw = 1$ </mark>

3. 최적해의 답을 찾는다. 
   
   - 라그랑주 승수법을 이용 
     
     - $max (w^TSw)$  $s.t.$  $w^Tw = 1$ => <mark>$L = w^TSw - \lambda(w^Tw -1)$</mark>
     
     - 최대값은 미분값이 0인 지점으로 아래를 성립시키는 $\lambda, w$ 찾기 
       
       > $\frac{\partial L}{\partial w} =0 $  <=> $Sw - \lambda w = 0$ <=> $(S-\lambda I)w = 0$  

4. basis $w$중 기본이 되는 축을 찾는다. 
   
   - <mark>$V$</mark> $= w^TSw$ = <mark>$\lambda_1 + \lambda_2 + ... + \lambda_n$</mark>
     
     >  Since $Sw_1$ = $\lambda_1w_1$ & $w^Tw =1$, 
     > 
     > - $w_1^TSw_1 = w_1^T\lambda_1w1 = \lambda_1w_1^Tw_1 = \lambda_1$
     > 
     > - $V = trace(w^TSw) = \lambda_1 + \lambda_2 + ... + \lambda_n$
     >   
     >   > $w = [w_1, w_2, ..., w_n]$
   
   - 고윳값(eigenvalue)을 내림차순에 따라 고유벡터들(eigenvector)을 정렬한다. 
     
     - 위에 식에 의거하여 <mark>basis $w_1$ 에 의해 표현되는 분산의 비율은 $\frac{\lambda_1}{sum_{i=1}^n \lambda_i}$ </mark>이다. 
     
     - $\lambda_1$ 이 가장 크기에, basis $w_1$ 데이터의 분산을 가장 많이 보존함을 알 수 있다. 

5. 기본 데이터를 basis $w_1$ 에 전사함으로써 분산을 최대한 유지한 채 데이터를 축약한다. 

   *#새로운 특징으로부터 원 데이터를 복구시킬 수도 있다.*

    ![](./picture/1-4.png)

###### PCA 이슈

- 몇 개의 Principal component를 선정해야하는 지에 대한 절대적인 측도가 없음 
  
  - 보존하고자 하는 분산 비율과, 각 도메인의 전문가 지식에 달림 
  
  - 분산 비율이 급격하게 감소하는 지점(Elbow point)을 참고하여 선정하기도 함.

###### PCA 한계

- 가우시안 분포를 전제로 작동하여, 그 외 경우(ex- non Gaussian / multimodal Gaussian 분포)에는 잘 작동하지 못함 

- 데이터 분류에는 부적절함 
  
  - ![](./picture/1-5.png)

---

### 2. MDS(Multidimensional Scaling)

- ###### 목적 : 기본 데이터의 "거리" 정보를 보존하는 축을 찾자.
  
  - 다차원에서 대상 간의 거리가 주어져 있을 때, 대상들을 저차원(보통 2,3차원)으로 동일한 상대적 거리를 가진 실수 공간의 점들로 배치시키는 방법 

- **PCA VS MDS**
  
  - ![](./picture/1-6.png)
  
  - >  MDS가 PCA보다 적용할 수 있는 범위가 넓다. PCA의 데이터는 $X$ (in $R^d$) 인데, 항상 모든 속성의 값이 주어진다 할 수 없기 때문. 반면 MDS는 일부 값이 비어있어도 근접도(Proximity) 행렬을 계산할 수 있다. 

###### 과정

1. **Proximity / Distance Matrix 만들기**
   
   - 샘플 간 거리값/ 근접도를  계산한 n x n 행렬을 만들기. 
   
   - 거리/근접도를 어떻게 정의하는냐에 따라 값이 달라질 수 있다.     
     
     - 거리(Distance) : 유클리안, 맨해튼 등의 방식이 있음
       
       - 거리 계산 간에는 삼각 부등식을 만족해야한다.
         
         ![](file://C:\Users\PC\Desktop\TIL\Machine Learning\picture\1-7.png)
     
     - 유사도(Simility) : 상관관계(Correlation), Jaccard 등의 방식이 있다.
   
   - D 차원에서 두 샘플 $z_i$ 와 $z_j$ 의 거리를 $dist_{ij}$ 라고 할 시, 저차원으로 임베딩한 값을 각각 $x_i$, $x_i$가 아래를 만족하면 우리는 거리 값을 유지했다고 할 수 있다.
     
     > $dist_{ij}^2 (=d_{ij}^2) = ||x_i - x_j||^2$

2. **거리 정보를 보존하는 축을 추출하기. (part 1 $D -> B$)**
   
   - 주어진 데이터 X로 부터 구할 수 있는 거리 행렬 D을 조작하여 원하는 값을 찾자!
   
   - 전체 과정 : <mark>거리 행렬 $D$ (n x n)-> 내적 행렬 $B$ (n x n) -> $X$ (d x n)</mark>
     
     - 또는 <mark>$D : (x_r - x_s)^T(x_r-x_s) => B : x_r^Tx_s => X : x$</mark>
     
     - 거리 행렬 $D$ 의 각 성분들은 아래로 표현가능 하다 
       
       > $d_{rs}^2 = (x_r - x_s)^T(x_r-x_s)$
   
   - $b_{rs}$ 를  $d_{rs}$ 에 대한 식으로 변경함으로써 내적 행렬 B를  계산한다. 
     
     > $[B]_{rs} = b_{rs} = x_r^Tx_s$
     
     - 데이터 센터링 과정을 통해 p개의 데이터 샘플의 평균을 0으로 가정한다. 
       
       > $\sum_{r=1}^{n} x_{ri} =0, (i=1,2, ... p)$
       > 
       > $d_{rs}^2 = x_r^Tx_r + x_s^Tx_s - 2x_r^Tx_s$
     
     - 거리 $d_{rs}$ 와 $b_{rs}$ 의 식을 전개하여 관계식으로 찾는다. 
       
       > ![](./picture/1-8.jpg)
       > 
       > ![](./picture/1-9.jpg)
       
       - 위의 (1), (2), (3) 식을 통해 $b_{rs}$ 를 모두 $d_{rs}$ 에 대한 식으로 변경
       
       > ![](./picture/1-10.jpg)
   
   - $b_{rs}$ 를 $a_{rs}, a_{r.}, a_{.s}, a_{..}$ 에 대한 식으로 바꾸는 행렬 계산식 도출한다. 
     
     - $[A]_{rs} = a_{rs}$ , $H = I - \frac{1}{n}11^T$
     
     - then <mark>$B = HAH$. </mark>

3. **거리 정보를 보존하는 축을 추출하기. (part 2 B -> X)**
   
   - 내적 행렬 $B$ 로부터 새로운 축 행렬 $X$(n x p, p < n) 찾아낸다.  
     
     - $B = XX^T, rank(B) = rank(XX^T) = rank(X) = p$
   
   - 내적 행렬 $B$를 고유값 분해(Eigen-decomposition)를 통해 정리한다. 
     
     - $B$ 는 Symmetric, positive semi-definite, rank가 p이기 때문에, non-negative인 p개의 고유값(eigenvalue)과 0값인 n-p 개의 고유값을 가진다.
     
     > positve semi-definite : $x^TBx >= 0$ for all of $x \in R^d /0$
     
     - $B = V \wedge V^T$
       
       > $\wedge = diag(\lambda_1, \lambda_2, ... \lambda_p)$
       > 
       > $V = [v_1, v_2, ... v_p]$
   
   - 비교적 덜 중요한 축들을 제거하여 데이터를 압축한다. 
     
     - 현실 응용에서는 차원 축소의 효율성을 위해, $dist_{ij}$ 와 $||x_i-x_j||$ 꼭 같아야 한다고 요구하진 않는다. 고유값이 0인 경우는 효율성을 위해 제거하기도 한다. 
     
     - (n-p) 개의 0 값인 고유값 때문에 B는 다시 아래와 같이 표현할 수 있다. 
       
       > $B_1 = V_1\wedge_1V_1^T, \wedge_1 = diag(\lambda_1, \lambda_2, ... \lambda_p), V_1 = [v_1, v_2, ... v_p]$
     
     - 위의 식에 의거하여<mark> $X$(n x 1) = $V_1\wedge_1^\frac{1}{2}$</mark> 로 구현

##### 추가 내용

- 기존 차원 D 보다 차원이 줄어들었을 때 정보의 손실이 발생할 수 있다. 따라서 S-Stress 지표를 도입하여 정보의 손실 정도를 측정한다. 
  
  > S - Stress : p차원 공간상에서의 원래거리와 2차원 공간상에서 표현된 거리 사이의 일치성을 측정하는 방식 

- S-Stress 값을 최대한 작게 하도록 반복과정을 수행하며, 최종 표현 결과의 적합도 수준은 스트레스 값에 의하여 결정된다. 
  
  - 통상적으로 S-Stress의 크기가 5% 이내이면 잘 표현되었다고 하며, 20% 이상인 경우에는 잘못 표현되었다고 함.  

-----

--- 

### Non-linear case

##### 매니폴드 학습 - ex) ISOMAP / LLE

> 매니폴드 학습 : 비유클리디안 형태의 데이터를 이용해 모델을 학습하는 것. 
> 
> 비유클리디안 : 유클리드 거리(두 점을 직선으로 이은 것이 최단 거리)를 적용할 수 없는 형태. 

- 위상학 개념을 빌린 차원 축소 방법으로, 주요 특징으로는 국소적으로는 유클리드 공간의 성질을 가진다는 점이다. (<=> 가까운 거리 내에서는 유클리드 거리를 적용할 수 있다)
  
  ![](file://C:\Users\PC\Desktop\TIL\Machine Learning\picture\1-23.png) 
  
  > *비유클리디안 예시*
  
  > 위의 그림에서 점의 이동은 파란 실선에 따라 움직인다.
  > 
  > 따라서 두 점 사이의 거리를 구할 때 두 점 사이의 직선거리가 아닌, 파란 선을 따라 이동한 거리를 재야한다.

- 일반적으로 저차원 매니폴드를 고차원 공간으로 임베딩시킨다. 
  
  - 국소거리간에는 유클리드 공간 성질을 가지나는 점을 이용하여 전체 데이터의 관계를 유클리드 방식으로 표현해낼 수 있다. 

###### 1. ISOMAP(Isometric Feature Mapping) 의의

- <mark>다차원, non-linear 한 데이터 간 거리 정보 보존</mark>을 목표로 한다.  
  
  - 계산 효율성 증대, 전역해(global optimality) 탐색, 점진적인 수렴 보장 가능 
  - 기존 매니폴드 방식(고차원으로 임베딩 후 직선 거리구하기)에 반대하여, 저차원내에서 유사 거리를 구하고자 함.  

- ![](./picture/1-8.png)
  
  > 점 두개를 각각 a점, b점으로 지칭하겠음 
  > 
  > 파란색 선은 a점, b점을 직선으로 이었을 때의 길이 
  > 
  > 빨간색 선은 a점, b점 간에 각 노드를 지나서 온다고 할 때 최소 거리를 의미함. 

- PCA / MDS 를 못 쓰는 이유
  
  - PCA 와 MDS는 다차원 내에서 선형관계가 성립할 때 사용할 수 있는 방법이다. 따라서 비선형 공간의 한 예시인 매니폴드 공간 상에는 적합하지 않은 방법임. 

- 파란색 선이 아니라 빨간 색 선을 구하는 이유 
  
  - 파란색 선은 다차원에서 유클리드 거리를 적용할 수 있을 때의 최소 거리이다. 따라서 유클리드 거리를 적용할 수 없는 비선형 상태에선 해당 거리를 측정할 수 없다.
  
  - 따라서 isomap에선 거리를 계산할 때 국소적인 영역으로 축소함으로써 유클리드 공간의 성질을 획득한 후, 각 거리들의 총합으로써 거리를 측정한다. 

##### ISOMAP 방식

1. 이웃 그래프(Neighborhood graph)를 만듬으로써 유클리드 거리를 사용할 환경을 마련한다.
   
   - $\epsilon$ - Isomap : $\epsilon$ 보다 거리가 작은 두 점을 연결 
   
   - $k$ - Isomap : 두 점간의 거리가 k번째 까지 가까운 경우 연결 

2. 각 노드간의 거리의 조합을 통해 거리 행렬 D를 만든다. 
   
   - $d_G(i,j)$ 의 값을 i와 j가 연결되어 있으면 둘의 거리 값($d_X(i,j)$)으로, 그렇지 않다면 $-\infty$ 를 부여 
   
   - 각 k 값(1,2,... N) 에 대해, $d_G(i,j) = min(d_G(i,j), d_G(i,k) + d_G(k,i))$ 적용 

3. 계산한 거리 행렬 D에 대해서 MDS 방법을 적용하여 d 차원 임베딩 실시한다.   

---

##### 2. LLE(Locally Linear Embedding) 의의 <mark>(이해도 낮음- 고민 더 필요)</mark>

- 고차원 공간에서 데이터들 사이의 <u>선형적 구조</u>를 보존하면서 저차원으로 임베딩하는 것
  
  > 여기서 선형적 구조란, "특정 샘플이 다른 샘플에 의한 선형 조합되는 것''을 의미
  > 
  > ex) - $x_i = w_{ij}x_j + w_{ik}x_k + w_{il}x_l$

- 장점 
  
  1. 사용하기 쉽다. 
  
  2. 최적화가 Local Minimum으로 가지 않는다.  
  
  3. 비선형 임베딩 생성이 가능하다. 
  
  4. 차원 감소 가능 

###### LLE 과정

![](./picture/1-14.png)    *Eigenvectors V가 d+1 차원을 가지는 것은 Rayletz-Ritz Theorem 의한 것*

> Rayletz - Ritz Theorem : 최적의 임베딩은 행렬 M의 bottom d+1 eigenvector들을 계산함으로서 찾아진다. 

    <img src="./picture/1-11.jpg" title="" alt="" width="464">

1. 각 샘플별 이웃 샘플 선정한다. 

2. 각 이웃으로부터 최대한 복원시킴으로써 비용 함수(~복원정도)를 최소화하는 $W_{ij}$ 계산
   
   > $W_{ij} : x_i$ 를 재복원하기 위해 $x_j$에 부여되는 가중치.
   
   ![](./picture/1-9.png)
   
   > 이웃에 의해 완전히 복원되면 $E(W)=0$ 임.  
   
   > $W_{ij} = \frac{\sum_{k\in Q_i} C_{jk}^{-1}}{\sum_{l,s \in Q_i} C_{ls}^{-1}}$
   > 
   > s.t. $C_{jk} = (x_i - x_j)^T(x_i - x_k)$

3. 아래 식을 최소화 시키면서 $W_{ij}$ 으로 가장 잘 복원된 벡터 $y$ 를 계산  
   
   - LLE는 선형관계를 최대한 유지한 채 저차원으로 임베딩시키는 것이다. 
   
   - 따라서 사전에 계산한 비중 행렬 $W$ 에 대해서, X의 값을 저차원으로 임베딩된 y 값또한 $\phi(W)$ 을 최소화시킬 때 선형 관계를 최대한 유지했다고 할 수 있다. 
   
   - 해당 조건을 만족하는 y값을 탐색하기 위해 아래의 식을 전개하여 구한다. 
     
     ![](./picture/1-13.png)
     
     ![](./picture/1-12.png)

--- 

### 3. t-SNE(Stochastic Neighbor Embedding)

- ###### 목표 : 고차원 상의 분포를 저차원 상에 임베딩(전사)했을 때에도 최대한 비슷하게 유지할 것.(<=> $p_{i|j}$ 와 $q_{i|j}$를 최대한 유사하게 만들 것 )

- 의의 : 자기 범주에 있는 것끼리 모아 구분하는 데 용이함
  
  - ![](.\picture/1-22.png)

- 거리에 반비례하여 확률값을 부여함으로써 가까운 이웃 뿐만 아니라, 멀리 있는 이웃들까지 고려한다.(LLE와의 차이)

##### 용어 정리

       <img src=".\picture/1-15.png" title="" alt="" width="463">

> $p_{j|i}$ : 원래 차원($D$)에서 객체 i가 j를 이웃으로 선택할 확률
> 
> $q_{j|i}$ : 축소된 차원($D^{'}$)에서 객체 i가 j를 이웃으로 선택할 확률
> 
> *아래 분모는 이웃 사례를 다 더했을 때 1이 되도록 강제하는 normalizaion임 

##### SNE 과정

1. 원하는 수준의 엔트로피에 맞춰 적절한 이웃 반경 설정하기  
   
   - $x_i$ 는 원래 차원에서 가우시안 분포에 따라 분포한다 했을 때, 표준편차의 크기에 따라 $x_j$ 를 유의미한 이웃으로 고려 유무를 결정한다. 
     
     - <=> 표준편차(=이웃 영역의 반경) 값에 $p_{j|i}$ 값과 엔트로피는 비례한다
   
   - 우리가 원하는 엔트로피 정도에 따라 반경(radius)를 계산한다. 
     
     ![](.\picture/1-16.png)
     
     >  H($P_i$) : 엔트로피
   
   - 이때 SNE는 복잡도(Perplexity)가 5~50 사이에선 대체로 강건(robust)하다.

2. 저차원 표현에 대한 비용 함수(ex-Kullback Leiber divergence)를 설정하여 최소화한다
   
   > Kullback-Leibler divergence : 두 평균 분산에 대한 차이를 비교하는 방법. 
   > 
   >  단, non-symmetric 하기 때문에 <u>distance matrix는 아님</u>
   > 
   > <img src=".\picture/1-17.png" title="" alt="" width="377">
   
   - 비용 함수를 편미분하여 최소값을 찾아낸다. 
     
     *결과는 아래와 같이 단순하게 나오나, 과정은 매우 번거롭다..*
     
     > ![](.\picture/1-18.png)

##### SNE 추가 Develop

1. SNE를 Symmetric 하게 재정의하기 
   
   ![](.\picture/1-19.png)
   
   > i와 j가 함께 뽑힐 확률인 $p_{ij}$ 를 
   > 
   > 1) i에서 j를 이웃으로 뽑을 확률($p_{j|i}$)과 
   > 
   > 2) j에서 i를 이웃으로 뽑을 확률($p_{i|j}$)의 합의 형태로 표현함.  
   
   - 위의 식을 적용할 경우 비용 함수의 gradient는 아래와 같이 표현됨 
     
     <img src=".\picture/1-20.png" title="" alt="" width="389">
   
   - 단, 위의 Symmetric SNE는 과밀 문제(Crowding problem)이 발생함
     
     >  과밀 문제 : 충분히 이격된 고차원 데이터를 저차원에서는 구현 못하는 상태
     
     - SNE가 Gausian 분포에 의존하다보니, j가 일정 이상 i에서 떨어지면 가능성이 급격하게 떨어지게 됨

2. 과밀 문제를 해결하기 위해 자유도가 1인 t 분포를 적용 (t-SNE)
   
   ![](.\picture/1-21.png)
   
   > 고차원에서는 Gausian 분포를, 저차원 상에서는 t 분포를 적용 

3. $y_i$에 대한 gradient 값을 기반으로 gradient descent 방법을 통해 y값 근접시키기
