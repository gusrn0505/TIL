# Topic 4 앙상블 학습(Ensemble Learning)

출처 : 강필성 교수님의 Business Analytics 강의

#### 

#### Ensemble Learning 기본 개념

- 전제 / 상황 
  
  - **모든 환경에 최고의 성능을 내는 상위의 알고리즘이란 없다(No Free Lunch Thm).** 
    
    즉, Data set의 특징에 따라 최고 성능을 내는 알고리즘이 달라진다. 
  
  - 따라서, 수많은 기술들을 적용해보는 것이 임의의 새로운 분류 문제를 푸는데 최고의 보험이다.
    
    <img title="" src="./picture/4-1.png" alt="" width="271"><img title="" src="./picture/4-2.png" alt="" width="295">
    
    > y축이 Error rate을 의미. 낮을 수록 좋다.
  
  - 하지만, <mark>여러 기술들을 적절히 결합한다면, 개별 기술보다 성능이 **"대체로"** 높게 나온다.</mark>
    
    <img src="./picture/4-4.jpg" title="" alt="" width="561">

- <u>Do we Need Hundreds of Classifiers to Solve Real World Classification Problem?</u>
  
  - 2014년 논문 발표 당시 모든 분류 Dataset에 대해 boosting을 제외한 모든 알고리즘을 적용함.
  
  ![](./picture/4-3.png)(상위 5위에 속하는 알고리즘)
  
  - 결론 1 : Rank가 1인 경우는 없다. (No Free Lunch)
    
    > Rank는 각 데이터셋별 알고리즘 성능 순위를 평균한 것
  
  - 결론 2 : 그래도 Random forests와 SVM 계열이 유의미하게 상대적으로 분류 성능이 높게 나온다. 

---

#### Bias - Variance Decomposition

- ###### 이론적 배경
  
  - 데이터는 모델에 Noise(~Error)가 추가된 형태로 나온다 가정
    
     $y = F^*(x) + \epsilon, \epsilon$ ~ $N(0, \sigma^2)$ 
    
    > - $F^*(x)$ : Target Fuction. 위 모델을 찾으려 하지만 정확히 알 순 없다. 
    > 
    > - $\epsilon$ : 에러. 독립적이고 동일하게 분포되어있다고 가정 
  
  - 각 Dataset에 대해 $F^*(x)$ 모델을 적용하여 나온 결과를 토대로 $\hat F_i(x)$ 모델 예측 
    
     ![](./picture/4-5.png)
    
    > $\overline F(x)$ = $E[\hat F_D(x)]$ 
  
  - 특정 포인트 $x_0$에 대해서 $Err(x_0)$ 의 식을 정리하여 <mark>편차와, 분산을 분리함. </mark>
    
    ![](./picture/4-6.png)
    
    > 왜 2번째 줄에 $F^*(x_0)$가 아닌 $\hat F^*(x_0)$가 들어간거지? 동일하다고 보는건가?
    > 
    > since $E[\overline F(x_0) - \hat F(x_0)] =0,$
    
    <img src="./picture/4-7.png" title="" alt="" width="404">

- ###### 편차와 분산의 특성
  
  ![](./picture/4-8.png)
  
  - **편차(Bias) : 다양한 데이터셋으로부터 반복적으로 모델링 할 때 평균적 결과들이 유사한가?** 
    
    - Low Bias :  데이터 셋으로부터 평균적으로 정확히 측정하고 있다.
    
    - High Bias : 방향이 맞지 않다. (Poor match) 
  
  - **분산(Variance) : 개별적인 모델링이 평균과 얼마나 큰 차이를 보이는가** 
    
    - Low Variance : 다른 데이터 셋이라 하더라도 측정값이 거의 변하지 않는다. 
    
    - High Variance : 값이 퍼져 있다 (Weak match)
  
  - `Error은 원 데이터에서 부여되는 것으로 제거할 수 없다
  
  - 편차와 분산은 각각으로부터 독립적이지 않다.(Why?)

- **모델 복잡도에 따른 분류** 
  
  ![](./picture/4-10.png)
  
  - Lower model complexity : High bias & low variance 
    
    - 개별 모델링의 값의 범위는 좁으나, 평균치가 정답과 거리가 있다. 
    
    > ex) Logistic regression, LDA, k-NN with large k, etc
    
    - Bagging 방법과 잘 맞음. Bagging을 통해 분산을 줄여줄 수 있음 
  
  - Higher model complexity : low bias & high variance 
    
    - 평균치로는 정답과 가까우나, 개별 모델링의 값의 범위가 넓다. 
    
    > DT, ANN, SVM, k-NN with small k
    
    - Boosting과 잘 맞음. Boosting을 통해 편차를 줄여줄 수 있음

###### 앙상블 학습의 특징

- **목적 : 다수의 학습을 적용을 통해 다양성을 확보하여 Error을 줄인다.** 
  
  1. 분산을 줄인다 & 데이터 다양성 확보 : Bagging 기술을 적용한다. 
  
  2. 편차를 줄인다 & 모델 다양성 확보  : Boosting 기술을 적용한다.  

- **앙상블 학습 특징** 
  
  - 성능이 있으면서 일정 이상 다른 모델을 합쳤을 때(다양성을 확보할 때) 효과가 좋다. 
  
  - 따라서 앙상블 구조를 짤 때 주요 질문은 아래와 같다. 
    
    1. **충분한 다양성을 확보하기 위해 앙상블 구조의 개별 요소들을 어떻게 생성할 것인가?** (매우 중요)
    
    2. 어떻게 개별 분류기의 결과를 결합할 것인가?

- **방식** 
  
  ![](./picture/4-11.png)
  
  > 좌측 : Implicit Diversity 확보. 각 학습기에 서로 다른 Random subset을 제공한 다음, 결과값을 합침 
  > 
  > 우측 : Explicit Diversity 확보. 이전 학습 과정이 이후 과정에 영향을 미치는 등 기존 Data가 이전과 다름을 보장함. 

- 앙상블 학습이 효과가 있는 이유 (수리적 풀이)
  
  $y_m(x)=f(x)+\epsilon_m(x)<=> E_x[(y_m(x)-f(x))^2]=E_x(\epsilon_m(x)^2)$  일 때,
  
  ![](./picture/4-13.png)
  
  - 위의 수식은 Cauchy's 부등식에 의해 항상 성립함. 
    
    > $(ax+by)^2 <= (a^2 + b^2)(x^2+y^2)$
    > 
    > - $(\epsilon_1 + ... + \epsilon_m)^2 <= (1^2 + ... + 1^2)(\epsilon_1^2 + ... + \epsilon_m^2)$
  
  - 더불어 좌측은 앙상블 Error를, 우측은 평균 Error을 의미함. 
    
    ![](./picture/4-12.png)
    
    > $f(x) = \frac{1}{M} * M *f(x)$
    > 
    > $y_m(x) - f(x) = \epsilon_m(x)$

- Error의 평균이 0이고, 각각 상관관계가 없다고 가정할 시 
  
  $E_{Ensemble} = \frac{1}{M} E_{Avg}$ 이 성립한다.  (best case)
  
      

---

#### Bagging(<mark>B</mark>ootstrap <mark>Agg</mark>regat<mark>ing</mark>)

- **방향성 / 의의** 
  
  ![](./picture/4-19.png)
  
  - 모든 앙상블의 결과값들은 서로 다른 학습 데이터로부터 만들어진다. 
  
  - 각각의 데이터 셋은 전체 N개 데이터에서 <mark>N번 랜덤 복원 추출</mark>로 만든다 **(Bootstrap)**. 
    
    - $y=f(x) + \epsilon$ 에서 원본 데이터는 $\epsilon$ 에  dependent 할 위험성이 있음. 
    
    - 따라서, Bootstrap을 통해 의도적으로 데이터의 분포를 왜곡함으로써 $\epsilon$ 이 모델과 독립적이도록 만든다. 
  
  - 각각의 데이터 셋으로 도출한 결과값을 수합하여 최종 예측 모델을 형성한다. 
  
  - 대체로 학습 모델의 성능을 향상시킨다. 
    
    ![](./picture/4-21.png)
    
    > y축이 Error rate로 낮을수록 좋음 

- **특징** 
  
  - N번 복원 추출 간 1번도 선택되지 않는 경우 
    
    ![](./picture/4-14.png)
    
    - 즉, N이 일정 이상 큰 수라면, $\frac{2}{3}$ 정도는 Bootsrap에 1회 이상 속하며, $\frac{1}{3}$ 정도는 한번도 안 뽑힘. 
    
    - 한번도 안 뽑히는 경우는 <mark>OOB(Out of Bag)</mark>이며, 추후 평가 데이터(Validate Data)로 사용한다. 즉, **매번 다른 데이터 셋을 통해 학습할 때 다른 평가 데이터를 사용하여 신뢰도 및 일반화의 정도를 향상시킨다.**
  
  - 모든 모델에 적용할 수 있으나, Low Bias & high variance인 모델과 잘 어울린다.
    
    ![](./picture/4-15.png)
    
    >  Ex)- ANN, SVM, DT
  
  - 각 데이터 셋별로 Parallel 하게 학습할 수 있어 병렬 학습이 가능한다.
  
  - 각 데이터 셋별 결과를 합치는 과정을 거쳐야 한다.
  
  - 
  
  



- **Result Aggregation**
  
  <img src="./picture/4-20.png" title="" alt="" width="483">
  - <u>수많은 결합 방식이 있으며, 상황마다 성능 차가 달라짐</u>
  
  - Ex -1) Majority voting
    
    ![](file://C:\Users\PC\Desktop\TIL\Machine Learning\picture\4-16.png)
  
  - Ex -2) Weighted voting (weight = 개별 모델의 학습 정확도)
    
    ![](file://C:\Users\PC\Desktop\TIL\Machine Learning\picture\4-17.png)
  
  - Ex -3) Weighted voting(weight = 각 클래스별 예상 확률)
    
    ![](file://C:\Users\PC\Desktop\TIL\Machine Learning\picture\4-18.png)
  
  - Ex -4) Stacking : 각 결과를 수합하여 예측하는 새로운 예측 모델 구축
    
    ![](./picture/4-18.jpg)


