# Topic 2 커널(Kernel) 기반 학습

출처 : 강필성 교수님의 Business Analytics 강의

#### 기본 개념

- ###### Shatter
  
  > 함수 F에 의해 n개의 point가 임의의 +1/-1의 값을 가질 때의 모든 경우가 직선 분류기에 의해 구현될 경우, 함수 F는 n개의 points를 shatter 할 수 있다. 
  
      <img src="./picture/2-1.png" title="" alt="" width="342">
  
  - 2차원 상에서 3개의 점은 직선방정식에 의해 Shatter 가능하다 
    
    <img title="" src="./picture/2-2.png" alt="" width="229">
  
  - 2차원 상에서 4개의 점은 직선방정식에 의해 Shatter 불가능하다. 
    
    ![](./picture/2-3.png)
  
  - 2차원 상에서 2개의 점은 원에 의해 Shatter 불가능하다. 

- ###### VC Dimension
  
  > 특정 함수 H에 의해 Shatter 될 수 있는 point들의 최대 개수
  > 
  >     Q. 여기서 H는 함수의 형태만을 지칭하나? (ex- 선, 원, 다항식) 
  
  - 가설 공간의 수용량을 측정하는데 사용됨. 
  
  - 수용량은 함수의 복잡도, 분류 경계면의 융튱성(flexibility)를 측정하는 수단이 됨. 

- ###### Structural Risk Minimization(SRM)
  
      ![](./picture/2-4.png)
  
  > h : capacity(모델의 복잡도). VC dim으로 수치화됨
  
  - 모델의 최종적인 error(-bound on test error)은 Training Error(empirical error)과 Capacity term의 합으로 Bound 된다.
    
    ![](./picture/2-5.png)
    
    ![](./picture/2-6.png)
    
    > L : Loss 함수. 예측이 맞으면 0, 틀리면 1의 값을 가짐. 
    > 
    > h : VC dimention 
    > 
    > n :  학습 데이터의 수
    > 
    > $\delta $ :  0~1 사이에 들어가는 확률 parameter 
    
    - 학습 데이터의 수가 증가하면, Capacity 감소 및 Training error 감소함. 
    
    - <mark>모델 복잡도가 증가하면, Capacity는 증가 및 Training error 감소함 (Trading off 관계)</mark>



##### 짥막 정리

- 과거 머신러닝 분야에서는 Kernel 방식이 Deep-learning 방식보다 각광받음. 
  
  - 머신러닝에서 SRM에 의거하여, 모델 복잡도가 높은 Deep-learning은 Kernel에 비해 성능이 좋지 않다고 여김. 
  
  - 또한 Kernel 방식을 통해서는 바로 최적해(global optimum)을 찾을 수 있는 데 비해, Deep-learning은 Local optimum을 찾음.  

- 하지만 위의 두 이유는 Deep-learning의 발전을 통해 해결될 수 있었음 
  
  - 1. 모델 복잡도가 매우 높아도, 정규화 등 몇가지 조치를 해주면 유의미한 $R[f_*]$ 를 만들 수 있음 
    
    2. 다차원 공간에서 모든 방향에 대해 local optimum인 경우는 거의 없음을 확인. Deep-learning으로도 대단히 높은 확률로 Global optimum을 찾을 수 있음을 증명
       
       <img src=".\picture/2-9.png" title="" alt="" width="416">



----- 

### SVR(Support Vectoc Machine)

- SVR은 분류 결정 함수이다. 
  
      <img src=".\picture/2-7.png" title="" alt="" width="378">
  
  - $g(x)$는 Linear SVR 이며, 추후 Kernel 도입시 Non-linear functions 처럼 적용할 수 있다. 



##### 의의

- 이진 분류 문제에서 SRM을 최소화하는 모델을 찾을 수 있다. 



##### 분류 - 선형/비선형 유무, 예외 허용 유무

    <img title="" src=".\picture/2-12.png" alt="" width="495">





##### 1. SVR - Linear & Hard margin

- ##### Margin
  
  > Margin : 분류 경계면으로 부터 가장 가까운 양쪽 범주 객체와의 거리
  
  >      <img title="" src=".\picture/2-10.png" alt="" width="267"> 
  
  - margin 길이를 최대로 가지는 직선 분류기는 <mark>강건성이 높으며, Capacity term을 줄여 전체 Error을 줄일 수 있다.</mark> 
    
    - Margin과 VC-dimention의 관계식은 아래와 같다. 
      
      ![](.\picture/2-11.png)
      
      위의 식에서 Margin 외에는 전부 고정값으로, Margin을 최소함으로써 Vc-dim을 최소화시킬 수 있다.  
      
      > h : Capacity term(VC - dim)
      > 
      > R : 전체 data를 감싸는 호구의 반지름 (고정값)
      > 
      > $\triangle$ : Margin (변동값)
      > 
      > D : 차원 수 (고정값)  
  
  - 따라서 Margin을 최대로 가지는 직선 분류기를 제일 좋은 것으로 판단, 해당 직선 분류기를 찾는 것이 목적이다.
  
  

###### 과정

1. 최적화 문제 정의 
   
   - Margin의 크기를 최대화하면서, 이진 분류를 설립하는 식 설정 
     
     > $min \frac{1}{2}||w||^2$ s.t. $y_i(w^Tx_i + b) >= 1 $ 
     > 
     > - margin의 크기 : $\frac{2}{||w||^2}$ . 이것을 최대화 하는 것은 $\frac{1}{2}||w||^2$ 을 최소화하는 것과 동일 
     > 
     > - $y_i$는 1) $w^Tx + b >= 1$ 일때 +1 값이, 2) $w^Tx+b <= -1$ 일때 -1 값을 가짐. 두 경우 합쳐서 $y_i(w^Tx_i + b) >= 1$ 를 의미함. 



2. 라그랑주 승수법을 통해서 라그랑주 목적 함수(Primal fuction)로 변환 
   
   ![](.\picture/2-13.png)(식 1)
   
   > $\alpha_i$ : 라그랑주 승수 
   
   - KKT Condition에 의해서, 아래 조건이 성립할 때 목적함수는 최솟값을 가짐 
     
     <img src=".\picture/2-14.png" title="" alt="" width="519"><mark> (조건 1)</mark>
   
   - 또한 KKT에 의해서 목적함수 속 제약식 부분들은 아래의 조건을 만족해야 함. 
     
     ![](.\picture/2-16.png) (식 2)
     
     s.t. <img title="" src=".\picture/2-17.jpg" alt="" width="211"> 
     
     - 즉, $w^Tx_i +b$ = $y_i$ (식 3) 경우 외엔 $\alpha_i$ 가 0으로, 고려하지 않아도 된다. 
     
     - <=> Support Vector 위에 있는 점들만 고려해주면 된다.  (조건 2)
       
       > Support Vector :  $w^Tx_i + b$ = $y_i$을 만족시키는 벡터 
   
   
   
   - <mark>조건 1, 조건 2에 의해 $w$는 Support Vector 위의 점에게만 영향을 받는다.</mark>
     
     <img src=".\picture/2-18.jpg" title="" alt="" width="490"><mark>(조건 3)</mark>
   
   
   
   

3. 목적 함수(Primal fuction)을 듀얼 함수(Dual fuction)로 변경하여 답 찾기. 
   
   > 듀얼 문제 : 목적함수의 x 변수를 $\lambda$에 대한 식으로 대입하여 나오는 문제. 
   > 
   > 종종 $L_P 를 L_D$ 로 바꾸면 수월하게 풀릴 때가 있음. 
   > 
   > 라그랑주 목적 함수의 최소값 = 라그랑주 듀얼 문제의 최대값).
   
   - 목적함수에 KKT Condition으로 나온 조건 1을 대입하면 아래의 수식이 나옴
     
     <img src=".\picture/2-15.png" title="" alt="" width="361">
     
     - 위의 수식에서 $y_i, y_j, x_i, x_j$ 는 주어지는 값으로 상수 취급할 때, 
       
       듀얼 문제는 $\alpha$ 에 대한 <mark>Convex 함수 형태를 띔</mark>. 
       
       즉, <mark>항상 최적값(global optimum)을 찾을 수 있음 </mark>
   
   - 위의 수식을 충족시키는 $\alpha_i$ 값을 찾음으로써 새로운 값($x_{new}$)를 분류하는 함수를 만들 수 있다. 
     
     <img src=".\picture/2-22.png" title="" alt="" width="283">

 

+ 추가 -  Maring 계산하는 법 
  
  + Support Vector (식 3)을 정리 및 조건 1을 대입하면 아래의 식이 됨
    
    ![](.\picture/2-19.png)
    
    위의 식 양 측에 $\alpha_iy_i$ 를 곱한다음 Sum을 취하면 아래 식이 됨
    
    ![](.\picture/2-20.png)
    
    $y_i^2$ 이 1인 점, 조건 1을 반영하여 margin의 길이($\rho$)를 구할 수 있음.  
    
    <img src=".\picture/2-23.png" title="" alt="" width="489">






















