# Topic 2 커널(Kernel) 기반 학습

출처 : 강필성 교수님의 Business Analytics 강의



#### 기본 개념

- ###### Shatter
  
  > 함수 F에 의해 n개의 point가 임의의 +1/-1의 값을 가질 때의 모든 경우가 직선 분류기에 의해 구현될 경우, 함수 F는 n개의 points를 shatter 할 수 있다. 
  
      <img src="./picture/2-1.png" title="" alt="" width="342">
  
  - 2차원 상에서 3개의 점은 직선방정식에 의해 Shatter 가능하다 
    
    <img title="" src="./picture/2-2.png" alt="" width="229">
  
  - 2차원 상에서 4개의 점은 직선방정식에 의해 Shatter 불가능하다. 
    
    ![](./picture/2-3.png)
  
  - 2차원 상에서 2개의 점은 원에 의해 Shatter 불가능하다. 



- ###### VC Dimension
  
  > 특정 함수 H에 의해 Shatter 될 수 있는 point들의 최대 개수
  
  - 가설 공간의 수용량을 측정하는데 사용됨. 
  
  - 수용량은 함수의 복잡도, 분류 경계면의 융튱성(flexibility)를 측정하는 수단이 됨. 



- ###### Structural Risk Minimization(SRM)
  
      ![](./picture/2-4.png)
  
  > h : capacity(모델의 복잡도). VC dim으로 수치화되기도 함. 
  
  - 모델의 최종적인 error(-bound on test error)은 Training Error(empirical error)과 Capacity term의 합으로 Bound 된다.
    
    ![](./picture/2-5.png)
    
    ![](./picture/2-6.png)
    
    > L : Loss 함수. 예측이 맞으면 0, 틀리면 1의 값을 가짐. 
    > 
    > h : VC dimention 
    > 
    > n :  학습 데이터의 수
    > 
    > $\delta $ :  0~1 사이에 들어가는 확률 parameter 
    
    - 학습 데이터의 수가 증가하면, Capacity 감소 및 Training error 감소함. 
    
    - <mark>모델 복잡도가 증가하면, Capacity는 증가 및 Training error 감소함 (Trading off 관계)</mark>



짥막 정리 

- 과거 머신러닝 분야에서는 Kernel 방식이 Deep-learning 방식보다 각광받음. 
  
  - 머신러닝에서 SRM에 의거하여, 모델 복잡도가 높은 Deep-learning은 Kernel에 비해 성능이 좋지 않다고 여김. 
  
  - 또한 Kernel 방식을 통해서는 바로 최적해(global optimum)을 찾을 수 있는 데 비해, Deep-learning은 Local optimum을 찾음.  

- 하지만 위의 두 이유는 Deep-learning의 발전을 통해 해결될 수 있었음 
  
  - 1. 모델 복잡도가 매우 높아도, 정규화 등 몇가지 조치를 해주면 유의미한 $R[f_*]$ 를 만들 수 있음 
    
    2. 다차원 공간에서 모든 방향에 대해 local optimum인 경우는 거의 없음을 확인. Deep-learning으로도 대단히 높은 확률로 Global optimum을 찾을 수 있음을 증명






