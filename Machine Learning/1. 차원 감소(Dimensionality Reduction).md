# Topic 1 차원 감소(Dimensionality Reduction)

출처 : 강필성 교수님의 Business Analytics 강의 

## 머신러닝 과정

1. 전처리(Pre-Processing)
   
   - 정규화 
   
   - **차원 축소**
   
   - 이미지 처리 

2. 학습 
   
   - Supervised
   
   - Unsupervised
   
   - Minimization 

3. 오류 분석(Error Analysis)
   
   - 정확도 / Recall
   
   - 과대 적합
   
   - 평가 / 교차검증 등 

### 차원 축소가 필요한 이유

1. 차원 감소를 통한 계산 효율 향상. 
   
   - 차원이 높을 수록 차원의 저주에 의해 필요로 하는 사례가 많아지며, 이는 계산량 증가로 연결된다.
     
     > **차원의 저주** : 차원이 증가할 때 동일한 설명을 갖추기 위해서 필요로 하는 사례의 수가 **지수적으로 증가**한다.
   
   - 정보를 유지하는 차원(Intrinsic dimension)은 기존 차원에 비해 작은 경우가 있다. 고려 요소가 많을수록 모델 성능을 향상할 것 같지만, 이는 각 요소들이 독립성을 갖췄을 때에 해당한다. 이는 매우 비현실적인 전제로 변수간 의존성을 제거함으로써 정보 손실을 최소화하면서 차원을 축소할 수 있다. 

2. 노이즈 감소를 통한 예측 성능 향상 
   
   - 고차원일수록 노이즈가 생길 가능성이 높아, 예측 성능을 낮춘다

### 차원 축소의 효과

1. 변수간 상관관계를 제거한다

2. 전처리를 단순화시킨다

3. 관련된 정보를 유지하면서 불필요한 변수를 제거한다

4. 시각화가 가능하다 

### 차원 축소의 방식

![](./picture/1-1.png)

- Feature Selection : 기존에 있는 변수 중 "선택" 
  
  > ex) 10개의 변수 중 설명력이 높은 3개 선택
  > 
  > Filter(Unsupervised) : 변수 선택과 모델 학습이 독립적 
  > 
  > Wrapper(Supervised) : 모델 학습의 결과에 최적화된 변수 선정  

- Feature Extraction : 기존에 있는 변수를 조합하여 최대한 많은 정보(변수, 거리 등)를 보존하는 새로운 변수를 추출. 
  
  > ex) z1 = x1 + 2x2, z2 = x3 + 3x4

---

## Feature Selection - Wrapper(Supervised)

<img src="./picture/1-2.jpg" title="" alt="" width="372">

- 전역검사 : 모든 가능한 조합을 확인하는 것. 경우의 수 $2^n$ -1 (n : 변수의 개수)

- FS(Forward Selection) : 설명력이 높은 변수부터 차례대로 선택하는 방식 
  
  > 설명력 : 모델이 얼마나 데이터를 잘 설명하는 지를 의미. 설명력 지표(Performance Metrics)로 AIC / BIC / Adjustd $R^2$ 이 있음.

- BS(Backward Selection) : FS와 반대방향으로, 설명력이 낮은 변수부터 제거하는 방식 

- SS(Stepwise Selection) : 설명력이 높은 변수부터 선택하되, 선택된 변수들의 조합도 같이 고려

- GA(Genetic Algorithm) : 휴리스틱한 접근 방식으로, 유전학습 과정을 모방한 방식 
  
  > 휴리스틱 : 대충 어림짐작하기. 복잡한 문제를 효율적으로 시행착오를 거치며 성능을 확보

#### Genetic Algorithm 과정

1) 염색체(Chromesome) 초기화 및 파라미터 설정 
   
   - 염색체 초기화 : 각 염색체의 변수마다 랜덤하게 사용 유무 결정 
   
   - 파라미터 : 염색체 수 / 성능 평가 방식(Fitness fuction) / 교배방식(Crossover mechanism) / 돌연변이율(Rate of mutation) / 종료 조건

2) 각 염색체 선택 변수별 모델 학습 

3) 각 염색체 적합도 평가 

4) 우수 염색체 선택(Selection) 
   
   - 결정론적(Deterministic) 선택 : 성능이 좋은 상위 n%의 염색체만 선정
   
   - 확률존적(Probabilistic) 선택  : 성능에 비례하여 각 염색체가 선정될 확률 부여. 

5) 다음 세대 염색체 생성(Crossoveer & Mutation)
   
   - Crossover : 선택한 염색체들 간 변수값을 교체함. 
   
   - Mutation : 일정 확률로 변수의 값을 변경함
   
   > 이전 세대의 최고 성능을 가진 염색체는 그대로 물려주며 원하는 성능이 나오기 까지 2~5번 과정 반복
   > 
   > <img src="./picture/1-3.jpg" title="" alt="" width="328">
6. 최종 변수 집합 선택 

--- 

## Feature Extraction

### 1. PCA(Principal Component Analysis)

- ##### 목적 : Original Data의 **분산**을 가장 잘 보존하는 축을 찾자!
  
  ##### 과정
1. Data Centering 
   
   - 추후 수식 정리의 편의성을 확보하기 위해 분산의 평균을 0으로 맞춘다.
   
   - 분산( $Var(X) =\frac{1}{n} * X X^T$ )에서 $X$ 대신, $X-\overline{X}$를 대입한다.  

2. 최적화 문제로 수식화 한다. 
   
   - <mark>$V = \frac{1}{n}(w^TX)(w^TX)^T$ = $\frac{1}{n}w^TXX^Tw = w^TSw$ </mark>
     
     > $w^TX$는 basis $w$에 대해 전사한 매트릭스를 의미
     > 
     > $S$는 벡터 $x$가 정규화 되었을 때의 공분산 매트릭스로, $S=\frac{1}{n}XX^T$임
   
   - 분산을 최대하는 PCA 목적 상 만족시켜야 하는 조건은 아래와 같음 
     
     - <mark>$max (w^TSw)$   $s.t.$  $w^Tw = 1$ </mark>
     - Q. $w^Tw$ 의 결과가 $I$ 가 나와야 하는 거 아닌가? $w$가 basis 의 집합인지, basis 1개 인지에 따라 결과가 달라질 듯

3. 최적해의 답을 찾는다. 
   
   - 라그랑주 승수법을 이용 
     
     - $max (w^TSw)$  $s.t.$  $w^Tw = 1$ <=> <mark>$L = w^TSw - \lambda(w^Tw -1)$</mark>
     
     - 최대값은 미분값이 0인 지점으로,  
       
       - $\frac{\partial L}{\partial w} =0 $  => <u>$Sw - \lambda w = 0$ (1번식) </u>=> $(S-\lambda I)w = 0$ 
       
       - 을 성립시키는 $\lambda $와 $w$ 찾기. 

4. basis $w$중 기본이 되는 축 찾는다. 
   
   - <mark>$V$</mark> $= \frac{1}{n}(w^TX)(w^TX)^T = \frac{1}{n}w^TXX^Tw = w^TSw$ = <mark>$\lambda_1 + \lambda_2 + ... + \lambda_n$</mark>
     
     - Since $Sw_1$ = $\lambda_1w_1$ & $w^Tw =1$, 
       
       - $w_1^TSw_1 = w_1^T\lambda_1w1 = \lambda_1w_1^Tw_1 = \lambda_1$
     
     - $V = trace(w^TSw) = \lambda_1 + \lambda_2 + ... + \lambda_n$
   
   - eigenvalue를 내림차순에 따라 eigenvector 들 정렬한다. 
     
     - 위에 식에 의거하여 <mark>basis $w_1$ 에 의해 표현되는 분산의 비율은 $\frac{\lambda_1}{sum_{i=1}^n \lambda_i}$ </mark>이다. 
     
     - $\lambda_1$ 이 가장 크기에, basis $w_1$ 이 기본 데이터의 분산을 가장 많이 보존한다. 

5. 기본 데이터를 basis $w_1$ 에 전사하여 새로운 특징(값)을 얻는다. 

6. 새로운 특징으로부터 원 데이터를 복구시킨다. 

    ![](./picture/1-4.png)

- ##### PCA 이슈
  
  - 몇 개의 Principal component를 선정해야 하는가? 
    
    - 정확히 정해진 개수는 없음. 
    
    - 보존하고자 하는 분산 비율과, 각 도메인의 전문가 지식에 달림 
    
    - 또는 분산 비율이 급격하게 감소하는 지점(Elbow point)까지 선정 

- ##### PCA 한계
  
  - 가우시안 분포를 전제로 작동하여, 그 외 경우(ex- non Gaussian / multimodla Gaussian 분포)에는 잘 작동하지 못함 
  
  - 데이터 분류에는 부적절함 
    
    - ![](./picture/1-5.png)

---

### 2. MDS(Multidimensional Scaling)

- ###### 목적 : 기본 데이터의 "거리" 정보를 보존하는 축을 찾자.

- **PCA VS MDS**
  
  - ![](./picture/1-6.png)
  
  - >  MDS가 PCA보다 적용할 수 있는 범위가 넓음. PCA의 데이터는 $X$ (in $R^d$) 인데, 항상 주어진다 할 수 없기 때문. 반면 MDS는 샘플 n개가 있다면 Proximity matrix가 주어진다 할 수 있음  . 



Q. MDS 가 거리 정보를 보존하는 축을 찾는 것은 이해가 됨(축을 기반으로 distance matrix를 복원시킬 수 있으니까). 그런데 MDS의 결과를 통해 각 항목들이 얼마나 가깝고 먼지를 판단할 수 있는지가 이론 측면에서 이해가 안됨. 



###### 과정

1. Proximity / Distance Matrix 만들기 
   
   - 조건 : Distance Matrix라면 삼각 부등식을 만족해야함
     
     > ![](./picture/1-7.png)
   
   - Distance : 유클리안, 맨해튼 등의 방식이 있음 
   
   - Similarity : 상관관계(Correlation), Jaccard 등의 방식이 있음 

2. 거리 정보를 보존하는 축을 추출하기. (part 1 $D -> B$)
   
   - 전체 과정 : <mark>Distance matrix $D$ (n x n)-> 내적 matrix $B$ (n x n) -> $X$ (d x n)</mark>
     
     - 또는 <mark>$D : (x_r - x_s)^T(x_r-x_s) => B : x_r^Tx_s => X : x$</mark>
   
   - Distance matrix $D$ 의 각 성분들은 아래로 표현가능 하다 
     
     > $d_{rs}^2 = (x_r - x_s)^T(x_r-x_s)$
   
   - 내적 Matrix $B$ 는 Distance matrix $D$ 로 부터 얻어질 수 있다. 
     
     > $[B]_{rs} = b_{rs} = x_r^Tx_s$
   
   - $b_{rs}$ 를 우리가 알 수 있는  $d_{rs}$ 에 대한 식으로 변경함으로써 계산한다. (*우리는 Distance matrix에서 $d_{rs}$ 만 계산해낼 수 있음  )
     
     - 모든 p 변수에 대한 평균이 0으로 가정 
       
       > $\sum_{r=1}^{n} x_{ri} =0, (i=1,2, ... p)$
       > 
       > $d_{rs}^2 = x_r^Tx_r + x_s^Tx_s - 2x_r^Tx_s$
       
       > ![](./picture/1-8.jpg)
       > 
       > ![](./picture/1-9.jpg)
       
       - 위의 (1), (2), (3) 식을 통해 $b_{rs}$ 를 모두 $d_{rs}$ 에 대한 식으로 변경
       
       > ![](./picture/1-10.jpg)
   
   - $b_{rs}$ 를 $a_{rs}, a_{r.}, a_{.s}, a_{..}$ 에 대한 식으로 바꾸는 행렬 계산식 도출 
     
     - $[A]_{rs} = a_{rs}$ , $H = I - \frac{1}{n}11^T$
     
     - then <mark>$B = HAH$. </mark>

3. 거리 정보를 보존하는 축을 추출하기. (part 2 B -> X)
   
   - Inner Matrix $B$ 로부터 새로운 coordinate matrix $X$(n x p, p < n) 찾아내기 
     
     - $B = XX^T, rank(B) = rank(XX^T) = rank(X) = p$
   
   - Matrix $B$를 고유값 분해(Eigen-decomposition)를 통해 정리 
     
     - $B$ 는 Symmetric, positive semi-definite, rank가 p이기 때문에, non-negative인 p개의 고유값(eigenvalue)과 0값인 n-p 개의 고유값을 가진다. 
       
       > positve semi-definite : $x^TBx >= 0$ for all of $x \in R^d /0$
     
     - $B = V \wedge V^T$
       
       > $\wedge = diag(\lambda_1, \lambda_2, ... \lambda_p)$
       > 
       > $V = [v_1, v_2, ... v_p]$
     
     - (n-p) 개의 0 값인 고유값 때문에 B는 다시 아래와 같이 표현됨 
       
       > $B_1 = V_1\wedge_1V_1^T, \wedge_1 = diag(\lambda_1, \lambda_2, ... \lambda_p), V_1 = [v_1, v_2, ... v_p]$
     
     - 위의 식에 의거하여<mark> $X$(n x 1) = $V_1\wedge_1^\frac{1}{2}$</mark> 로 구현

-----

## Non-linear case

##### 3. ISOMAP(Isometric Feature Mapping) 의의

- 다차원, non-linear 한 기본 데이터의 최대한 많은 정보를 보전하는 차원을 찾는 것 
  
  - 계산 효율성 증대, 전역해(global optimality) 탐색, 점진적인 수렴 보장 가능  

- ![](./picture/1-8.png)
  
  > 점 두개를 각각 a점, b점으로 지칭하겠음 
  > 
  > 파란색 선은 a점, b점을 직선으로 이었을 때의 길이 
  > 
  > 빨간색 선은 a점, b점 간에 각 노드를 지나서 온다고 할 때 최소 거리를 의미함. 
  
  - 1) PCA / MDS 를 못 쓰는 이유 
       
       - PCA 와 MDS는 다차원 내에 최소거리를 측정함. 이를 그림 A의 파란색 실선과 동일함. 하지만 이는 주어진 데이터 공간에 적합하지 않음. 
    
    2) 파란색 선이 아니라 빨간 색 선을 구하는 이유 
       
       - 파란색 선은 기하학적으로 봤을 때 최적 거리라 할 수 있으나, 해당 거리를 측정할 수 없음. 이에 isomap에선 점 a, b에 대한 거리 d(a,b)를 가까온 노드와 연결하여 나온 거리로 재정의하여 계산함. 

##### ISOMAP 방식

1. 이웃 그래프(Neighborhood graph)를 만든다. 
   
   - $\epsilon$ - Isomap : $\epsilon$ 보다 거리가 작은 두 점을 연결 
   
   - $k$ - Isomap : 두 점간의 거리가 k번째 까지 가까운 경우 연결 

2. 가장 가까운 길을 계산하기 
   
   - $d_G(i,j)$ 의 값을 i와 j가 연결되어 있으면 둘의 거리 값($d_X(i,j)$)으로, 그렇지 않다면 $-\infty$ 를 부여 
   
   - 각 k 값(1,2,... N) 에 대해, $d_G(i,j) = min(d_G(i,j), d_G(i,k) + d_G(k,i)$ 적용 

3. MDS 방법을 통해 d 차원 임베딩 실시  



##### 4. LLE(Locally Linear Embedding) 의의 <mark>(이해도 낮음)</mark>

1. 적용하기 쉬움 

2. 최적해 탐색 간 local minima 미포함 

3. highly nonlinear emdedding 가능. (정확히 무슨 의미일까)

4. 차원 감소 가능 

5. 비슷한 항목끼리 뭉치게 만듬, 주요 성분 도출?  

##### LLE 과정

![](./picture/1-14.png)    * Eigenvectors V가 d+1 차원을 가지는 것은 Rayletz-Ritz Theorem 의한 것 *

> Rayletz - Ritz Theorem : 최적의 임베딩은 매트릭스 M의 bottom d+1 eigenvector들을 계산함으로서 찾아진다. 

    <img src="./picture/1-11.jpg" title="" alt="" width="464">

1. 각 점별 이웃들 계산 

2. 비용 함수(~복원정도)를 최소화하는 각 이웃으로부터 최대한 복원시키는 비중 $W_{ij}$ 계산
   
   - 변수 $x$를 기반으로 $W$ 을 계산하는 것 
   
   ![](./picture/1-9.png)
   
   > $W_{ij}$ : $x_i$ 를 재복원하기 위해 $x_j$에 부여되는 가중치. 
   > 
   > 이웃에 의해 완전히 복원되면(<=> 한 점이 이웃에 의해 둘러싸이면) $E(W)=0$ 임.  

3. 아래 식을 최소화 시키면서 $W_{ij}$ 으로 가장 잘 복원된 벡터 $y$ 를 계산  <mark>(아래 식 이해도 낮음)</mark>
   
   - 변수 $W_{ij}$ 를 기반으로 벡터 $y$ 를 계산하는 것 
     
     ![](./picture/1-12.png)
     
     <img src="./picture/1-13.png" title="" alt="" width="278">

### 5. t-SNE(Stochastic Neihbor Embedding) 의의

- 자기 범주에 있는 것끼리 모아 구분하기 - 가장 많이 사용하는 방법  
  
  - ![](.\picture/1-22.png)

- 가까운 이웃 뿐만 아니라, 멀리 있는 이웃들까지 고려하자! (LLE와 차이)
  
  - 단, 거리에 반비례하여 확률값을 부여하자

##### t-SNE 과정

       <img src=".\picture/1-15.png" title="" alt="" width="463">

> $p_{j|i}$ : 원래 차원($D$)에서 객체 i가 j를 이웃으로 선택할 확률
> 
> $q_{j|i}$ : 축소된 차원($D^{'}$)에서 객체 i가 j를 이웃으로 선택할 확률
> 
> *아래 분모는 이웃 사례를 다 더했을 때 1이 되도록 강제하는 normalizaion임 

###### 목표 : 고차원 상의 분포를 저차원 상에 임베딩(전사)했을 때에도 최대한 비슷하게 유지할 것.(<=> $p_{i|j}$ 와 $q_{i|j}$를 최대한 유사하게 만들 것 )

##### SNE 과정

1. 적절한 이웃 반경 설정하기 - 원하는 수준의 엔트로피에 맞춰서 
   
   - $x_i$ 는 원래 차원에서 가우시안 분포에 따라 분포한다 했을 때, 표준편차의 크기에 따라 $x_j$ 를 유의미한 이웃으로 고려할 수도 아닐 수도 있음  
     
     - <=> radius(=표준편차)에 $p_{j|i}$ 값과 엔트로피는 비례한다
   
   - 따라서 우리가 원하는 엔트로피 정도에 따라 반경(radius)를 계산한다. 
     
     ![](.\picture/1-16.png)
     
     >  H($P_i$) : 엔트로피
   
   - 이때 SNE는 복잡도(Perplexity)가 5~50 사이에선 대체로 강건(robust)하다.

2. 저차원 표현에 대한 비용 함수 - Kullback Leiber divergence - 설정하여, 최소화할 것
   
   > Kullback-Leibler divergence : 두 평균 분산에 대한 차이를 비교하는 방법. 
   > 
   >  단, non-symmetric 하기 때문에 distance matrix는 아님
   > 
   > <img src=".\picture/1-17.png" title="" alt="" width="377">
   
   - 비용 함수를 편미분하여 최소값을 찾아낼 것. 결과는 아래와 같이 단순하게 나오나, 과정은 매우 귀찮음. 
     
     - ![](.\picture/1-18.png)

##### SNE 추가 Develop

1. SNE를 Symmetric 하게 재정의하기 
   
   ![](.\picture/1-19.png)
   
   > i와 j가 함께 뽑힐 확률인 $p_{ij}$ 를 1) i에서 j를 이웃으로 뽑을 확률($p_{j|i}$)과 2) j에서 i를 이웃으로 뽑을 확률($p_{i|j}$)의 합의 형태로 표현함.  
   
   - 위의 식을 적용할 경우 비용 함수의 gradient는 아래와 같이 표현됨 
     
     <img src=".\picture/1-20.png" title="" alt="" width="389">
   
   - 단, 위의 Symmetric SNE는 Crowding problem이 발생함 
     
     - SNE가 Gausian 분포에 의존하다보니, j가 일정 이상 i에서 떨어지면 가능성이 급격하게 떨어지게 됨

2. Crowding Problem을 해결하기 위해 자유도가 1인 t 분포를 적용 (t-SNE)
   
   ![](.\picture/1-21.png)
   
   > 고차원에서는 Gausian 분포를, 저차원 상에서는 t 분포를 적용 
