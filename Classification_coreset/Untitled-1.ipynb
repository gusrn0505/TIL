{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, utils, datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    "    )\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "\n",
    "torch.manual_seed(23)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    # use_cuda가 true라면 kwargs를 다음과 같이 지정하기. \n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MNIST(nn.Module) : #(1, 28, 28) -> (16, 12, 12) -> (32, 5, 5)\n",
    "    def __init__(self) : \n",
    "        super(MNIST, self).__init__() \n",
    "\n",
    "        self.conv1 = nn.Conv2d(1,16, (5,5))\n",
    "        self.conv1_normalize = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d((2,2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16,32, (3,3))\n",
    "        self.conv2_normalize = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d((2,2))\n",
    "\n",
    "        self.drop = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(800, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self,x) : \n",
    "        x = self.conv1(x) \n",
    "        x = self.conv1_normalize(x) \n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x) \n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2_normalize(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.drop(x)\n",
    "        x = x.view(-1, 800)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "CNN = MNIST().to(device)\n",
    "\n",
    "optimizer = optim.Adam(CNN.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size = 1, gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import  DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def train(batch_size, total_epoch, model, device, train_dataset, optimizer, epoch) : \n",
    "    model.train()\n",
    "    data_loader = DataLoader(train_dataset, batch_size)\n",
    "\n",
    "    for i, (data, label) in enumerate(data_loader) : \n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, label)\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        avg_loss = loss / len(data_loader)\n",
    "        if i % 100 == 0 : \n",
    "            print('Train epoch : {} [{}/{} ({:.0f}%)]\\t Loss: {:.4f}'. format(total_epoch, i, len(data_loader), 100. * i / len(data_loader), avg_loss))\n",
    "    return model \n",
    "\n",
    "\n",
    "def test(batch_size, epoch, model, device, test_dataset, optimizer) : \n",
    "    model.eval() \n",
    "    test_loss = 0 \n",
    "    correct = 0 \n",
    "\n",
    "    data_loader = DataLoader(test_dataset, batch_size)\n",
    "\n",
    "    with torch.no_grad() : \n",
    "        for (data, label) in data_loader : \n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, label, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim = True)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_dataset)\n",
    "\n",
    "    print('\\n Test set : Average loss : {:.4f}, Accuracy : {}/{} ({:.0f}%) \\n'.format(test_loss, correct, len(test_dataset), 100. *correct / len(test_dataset)))\n",
    "\n",
    "    return correct/len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch : 50 [0/1200 (0%)]\t Loss: 0.0020\n",
      "Train epoch : 50 [100/1200 (8%)]\t Loss: 0.0003\n",
      "Train epoch : 50 [200/1200 (17%)]\t Loss: 0.0002\n",
      "Train epoch : 50 [300/1200 (25%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [400/1200 (33%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [500/1200 (42%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [600/1200 (50%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [700/1200 (58%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [800/1200 (67%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [900/1200 (75%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [1000/1200 (83%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [1100/1200 (92%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [0/1200 (0%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [100/1200 (8%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [200/1200 (17%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [300/1200 (25%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [400/1200 (33%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [500/1200 (42%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [600/1200 (50%)]\t Loss: 0.0002\n",
      "Train epoch : 50 [700/1200 (58%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [800/1200 (67%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [900/1200 (75%)]\t Loss: 0.0002\n",
      "Train epoch : 50 [1000/1200 (83%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [1100/1200 (92%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [0/1200 (0%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [100/1200 (8%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [200/1200 (17%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [300/1200 (25%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [400/1200 (33%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [500/1200 (42%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [600/1200 (50%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [700/1200 (58%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [800/1200 (67%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [900/1200 (75%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [1000/1200 (83%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [1100/1200 (92%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [0/1200 (0%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [100/1200 (8%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [200/1200 (17%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [300/1200 (25%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [400/1200 (33%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [500/1200 (42%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [600/1200 (50%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [700/1200 (58%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [800/1200 (67%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [900/1200 (75%)]\t Loss: 0.0001\n",
      "Train epoch : 50 [1000/1200 (83%)]\t Loss: 0.0000\n",
      "Train epoch : 50 [1100/1200 (92%)]\t Loss: 0.0000\n",
      "\n",
      " Test set : Average loss : 0.0445, Accuracy : 9852/10000 (99%) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from MNIST_train_test import train,\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "        # train_test.py 의 train 함수를 통해 손 쉽게 모델 학습 진행 \n",
    "        # train 수정 필요. \n",
    "    CNN = MNIST_train(50, 50, CNN, device, train_data, optimizer, epoch)\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "        # train_test.py 의 test 함수를 통해 손 쉽게 모델 학습 진행 \n",
    "    # 추후 pseudo labeling dataset에 대해서 바꿔줄 필요가 있음. \n",
    "accuracy = MNIST_test(50, 50, CNN, device, test_data, optimizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9852"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ce5735ac0795ce80d9ca82db09fbd3aedd56acddf82388eb9cd49352a280e35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
