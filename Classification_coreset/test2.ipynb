{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# DataLoader은 Dataset을 샘플에 쉽게 접근할 수 있도록 순회가능한 객체(iterable)로 감쌉니다\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.models as models \n",
    "\n",
    "import pdb\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "# pretty print. 들여쓰기 등을 지원해준다. \n",
    "\n",
    "import pprint\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# local stuff\n",
    "# 폴더에 있는 경우 A.B 형태로 기술 \n",
    "from dsets.mnist import MNIST\n",
    "from mymodels.mnist_net import Net\n",
    "from auto_encoder import AutoEncoder, ConvAutoEncoder, ae_train\n",
    "from train_test import train\n",
    "from init_pool_tools import obtain_init_pool\n",
    "from coreset import Coreset_Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba76f79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded AE & CAE\n"
     ]
    }
   ],
   "source": [
    "original_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    "    )\n",
    "\n",
    "\n",
    "    # MNIST Dataset을 가공할 수 있는 list로 변경. feature와 label 각각 저장 \n",
    "original_all = []\n",
    "original_dataset = []\n",
    "original_label = [] \n",
    "\n",
    "for i, sample in enumerate(original_data) : \n",
    "    original_all.append(sample)\n",
    "    feature = np.array(sample[0])\n",
    "    original_dataset.append(feature)\n",
    "    original_label.append([sample[1], i]) # original index 포함시키기. \n",
    "\n",
    "unlabeled_dataset = original_dataset[:]\n",
    "unlabeled_dataset_label = original_label[:]\n",
    "labeled_dataset = [] \n",
    "labeled_dataset_label = []\n",
    "\n",
    "PATH = './weights/MNIST/'\n",
    "AE = torch.load(PATH + 'AE.pt')  \n",
    "AE.load_state_dict(torch.load(PATH + 'AE_state_dict.pt'))  \n",
    "\n",
    "CAE = torch.load(PATH + 'CAE.pt')  \n",
    "CAE.load_state_dict(torch.load(PATH + 'CAE_state_dict.pt'))  \n",
    "\n",
    "print(\"Successfully loaded AE & CAE\")\n",
    "\n",
    "use_cuda = True\n",
    "\n",
    "torch.manual_seed(23)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    # use_cuda가 true라면 kwargs를 다음과 같이 지정하기. \n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84da4789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max distance from cluster : 11.56\n"
     ]
    }
   ],
   "source": [
    "from active_learn import active_sample\n",
    "\n",
    "sample_size = 50\n",
    "sampling_method = 'ae_coreset'\n",
    "sample_dataset, sample_index,radius  = active_sample(unlabeled_dataset, labeled_dataset, sample_size, method=sampling_method, model=CAE, device=device)\n",
    "\n",
    "sample_data = [unlabeled_dataset[i] for i in sample_index]\n",
    "sample_label = [unlabeled_dataset_label[i] for i in sample_index]\n",
    "\n",
    "    # Sampling에 따른 Datset 수정\n",
    "if len(labeled_dataset_label) == 0 :  \n",
    "    labeled_dataset = sample_data[:]\n",
    "    labeled_dataset_label = sample_label[:]\n",
    "else : \n",
    "    labeled_dataset = np.concatenate((labeled_dataset,sample_data),axis=0)\n",
    "    labeled_dataset_label = np.concatenate((labeled_dataset_label, sample_label), axis =0)\n",
    "\n",
    "\n",
    "for i in sample_index[::-1] : \n",
    "    del unlabeled_dataset[i]\n",
    "    del unlabeled_dataset_label[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d601c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from active_learn import get_features\n",
    "\n",
    "def make_subgraph(sampling_label, original_dataset, radii, model):\n",
    "    x = [original_dataset[i[1]] for i in sampling_label] \n",
    "    dataset = original_dataset\n",
    "\n",
    "    if model == AE or model == CAE : \n",
    "        x = get_features(model, x, device)\n",
    "        dataset = get_features(model, dataset, device)\n",
    "\n",
    "    dist = pairwise_distances(x,dataset, metric='euclidean')\n",
    "\n",
    "    subgraph= dist.copy()\n",
    "    density_subgraph = []\n",
    "    for i, row in enumerate(dist) : \n",
    "        for j, distance in enumerate(row) : \n",
    "            if distance > radii or j == sampling_label[i][1] : subgraph[i,j] =int(0) \n",
    "            else : subgraph[i,j] = int(1) \n",
    "        \n",
    "        density_subgraph.append(sum(subgraph[i]))\n",
    "    \n",
    "\n",
    "    return np.array(subgraph), density_subgraph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0bcaf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph, density_subgraph = make_subgraph(sample_label, original_dataset, radius, CAE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d8908cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[1621.0, 4.0, 72.0, 106.0, 308.0, 5906.0, 21.0, 13.0, 3.0, 13.0, 10.0, 69.0, 213.0, 137.0, 61.0, 51.0, 3.0, 355.0, 6.0, 22.0, 31.0, 23714.0, 17.0, 1601.0, 11.0, 29.0, 12.0, 124.0, 40.0, 17.0, 48.0, 65.0, 1134.0, 11.0, 5803.0, 5.0, 2.0, 15.0, 10.0, 67.0, 1045.0, 296.0, 16.0, 89.0, 298.0, 37.0, 62.0, 68.0, 29671.0, 451.0]\n",
      "2.0 29671.0\n"
     ]
    }
   ],
   "source": [
    "print(subgraph)\n",
    "print(density_subgraph)\n",
    "print(min(density_subgraph) ,max(density_subgraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85cc0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacency_subgraph(sample_data, sample_label, radii, model, M) : \n",
    "    # labeled_dataset을 Sample_dataset으로 변경 필요. \n",
    "    dataset = sample_data\n",
    "    num_subgraph = len(sample_label)\n",
    "    if model == AE or model==CAE : \n",
    "        dataset = get_features(model,dataset, device)\n",
    "    \n",
    "    dist = pairwise_distances(dataset, dataset, metric='euclidean')\n",
    "    adj_dist = dist.copy()\n",
    "    \n",
    "    for i, row in enumerate(dist) : \n",
    "        adj_d = np.where(row < 2*radii[0])[0] # 겹치는 것들의 index만 도출하기 \n",
    "        for j, distance in enumerate(row) : \n",
    "            if distance >= 2*radii[0] : adj_dist[i,j] = int(0)   \n",
    "            elif 2*radii[0] > distance and distance >= radii[0]  : \n",
    "                adj_dist[i,j] = int(1)\n",
    "            elif i==j : adj_dist[i,j] = int(0) # 자기자신은 제거\n",
    "            else : \n",
    "                print('Break')\n",
    "\n",
    "    classified_subgraph_index = []\n",
    "\n",
    "\n",
    "    for i in range(num_subgraph) : \n",
    "        i_sub_class = \"x\"\n",
    "        adj_index = np.where(adj_dist[ :,i] ==1)[0] \n",
    "        if len(adj_index)==0 : continue \n",
    "        i_sub_class = sample_label[i][0]\n",
    "\n",
    "        for j in adj_index :  \n",
    "            if i_sub_class != sample_label[j][0] : \n",
    "                i_sub_class = \"x\"\n",
    "                continue\n",
    "        if i_sub_class != \"x\" and len(adj_index) >= M : \n",
    "            classified_subgraph_index.append(i)\n",
    "    \n",
    "    classified_label = [sample_label[i] for i in classified_subgraph_index]\n",
    "\n",
    "    return dist, adj_dist, classified_subgraph_index, classified_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dc3aa299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확인해야 하는 것. M을 조건으로 추가했을 때 어떤가 \n",
    "# 왜 접하고 있는 subgraph의 개수가 6개를 넘어가는 게 있지? 점이 중복된 건가? \n",
    "\n",
    "dist_class, adj_dist, classified_subgraph_index, pseudo_class_label = adjacency_subgraph(sample_dataset, sample_label, radius, CAE, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c34be55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.       175.12886   73.471    ...  46.456486  43.843826  75.7779  ]\n",
      " [175.12886    0.       177.05714  ... 132.01562  152.70993  151.18013 ]\n",
      " [ 73.471    177.05714    0.       ...  92.4026    39.476887  26.207045]\n",
      " ...\n",
      " [ 46.456486 132.01562   92.4026   ...   0.        52.92574   80.31994 ]\n",
      " [ 43.843826 152.70993   39.476887 ...  52.92574    0.        32.581097]\n",
      " [ 75.7779   151.18013   26.207045 ...  80.31994   32.581097   0.      ]]\n",
      "[0, 1, 3, 7, 9, 10, 14, 19, 22, 23, 24, 29, 36, 39, 42, 43, 44, 46, 49]\n",
      "19\n",
      "[[1, 941], [4, 1448], [0, 4218], [4, 9880], [6, 14129], [6, 16796], [0, 18896], [6, 22962], [0, 27477], [1, 27489], [4, 28128], [0, 37511], [4, 44998], [0, 47470], [0, 50344], [6, 52206], [0, 56469], [0, 57634], [0, 59510]]\n"
     ]
    }
   ],
   "source": [
    "print(dist_class)\n",
    "print(classified_subgraph_index)\n",
    "print(len(classified_subgraph_index))\n",
    "print(pseudo_class_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a2165923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subgraph[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c326ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def first_classification(classified_subgraph_index, pseudo_class_label, subgraph, density_subgraph, ratio) : \n",
    "    dense_classified_subgraph = [density_subgraph[i] for i in classified_subgraph_index]    \n",
    "    sort_by_density = sorted(dense_classified_subgraph, reverse=True)\n",
    "    rank = int(ratio*len(sort_by_density))\n",
    "    M = sort_by_density[max(rank-1, 0)] # 밀도 상위 M % 의 subgraph만을 사용. \n",
    "\n",
    "    classification = defaultdict(list)\n",
    "    for i, index in enumerate(classified_subgraph_index) : \n",
    "        if density_subgraph[index] < M : continue\n",
    "        x_index = list(np.where(subgraph[index] == 1)[0])\n",
    "        label = pseudo_class_label[i][0]\n",
    "    \n",
    "        classification[label] += x_index\n",
    "    return classification\n",
    "\n",
    "def check_performance(classification, original_label) : \n",
    "    score = defaultdict(list) \n",
    "    all_score = 0 \n",
    "    all_count = 0 \n",
    "    for i in sorted(list(classification.keys())) : \n",
    "        x_index = classification[i] \n",
    "        num_x = len(x_index)\n",
    "        count = 0 \n",
    "        for index in x_index :\n",
    "            if original_label[index][0] == i : count += 1 \n",
    "        \n",
    "        i_score = count/num_x\n",
    "        all_score += count\n",
    "        all_count += num_x\n",
    "        score[i] = [num_x, i_score]\n",
    "    \n",
    "    all_score = all_score/all_count\n",
    "    \n",
    "    return all_count, all_score, score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "52e00873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4479\n",
      "0.9995534717570886\n",
      "defaultdict(<class 'list'>, {0: [1095, 1.0], 1: [3222, 0.999689633767846], 4: [28, 1.0], 6: [134, 0.9925373134328358]})\n"
     ]
    }
   ],
   "source": [
    "check = first_classification(classified_subgraph_index, pseudo_class_label, subgraph, density_subgraph, 0.99)\n",
    "num_classification, score, dic_score = check_performance(check, original_label)\n",
    "\n",
    "print(num_classification)\n",
    "print(score)\n",
    "print(dic_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "71ce2b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 201]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_label[201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ead03a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class(subgraph, density_subgraph, M, labeled_dataset_label) : \n",
    "    num_sample = np.shape(subgraph)[1]\n",
    "\n",
    "    classification =[-1]*num_sample \n",
    "    classified_index = []\n",
    "\n",
    "    filtered_subgraph_index = []\n",
    "\n",
    "\n",
    "    # xk 가 속한 subgraph의 label이 모두 같을 확률 \n",
    "    for i in range(num_sample) : \n",
    "        in_subgraph_index = np.where(subgraph[ :,i] ==1)\n",
    "        if len(in_subgraph_index[0])==0 : continue \n",
    "        i_class = labeled_dataset_label[in_subgraph_index[0][0]]\n",
    "\n",
    "        for j in in_subgraph_index[1:] :  \n",
    "            if i_class != labeled_dataset_label[j] : \n",
    "                i_class = -1\n",
    "                continue\n",
    "        if i_class != -1 : \n",
    "            classification[i] = i_class\n",
    "            classified_index.append(i)\n",
    "    \n",
    "    score = len(classified_index)\n",
    "    pseudo_label = [classification[i] for i in classified_index]\n",
    "\n",
    "    return score, classified_index, pseudo_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "41cf267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dest_dir 위치에 결과 기록하기. 입력할 결과들을 입력값으로 넣음 \n",
    "def log(dest_dir, episode_id, sample_method,  label_dataset_label, num_classification, ratio, accuracy):\n",
    "    # log file은 dest_dir 위치에 log.csv를 두기 위한 주소이다. \n",
    "    log_file = os.path.join(dest_dir, 'log.csv')\n",
    "\n",
    "    # 주소가 정확하지 않을  해당 위치에 파일이 존재하지 않을 때, log_rows를 다음과 같이 정한다. \n",
    "    if not os.path.exists(log_file):\n",
    "        log_rows = [['Episode Id','Sample Method','Labeled Pool', 'Num of classification', 'Ratio', 'Accuracy']]\n",
    "    # 파일이 존재할 때에는 데이터를 처리해서 불러온다. \n",
    "    else:\n",
    "        log_rows = np.genfromtxt(log_file, delimiter=',', dtype=str, encoding='utf-8').tolist()\n",
    "\n",
    "    # episod_id, sample_mthod, sample_time 등의 값을 추가한다. \n",
    "    log_rows.append([episode_id,sample_method, len(label_dataset_label), num_classification, ratio, accuracy])\n",
    "    \n",
    "    # 데이터를 저장한다. 파일이 없다면 새로 만든다. \n",
    "    np.savetxt(log_file,log_rows,'%s,%s,%s,%s,%s,%s',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a45a88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"output/\"\n",
    "dataset_name = \"MNIST\"\n",
    "\n",
    "\n",
    "dest_dir = os.path.join(output_dir, dataset_name)\n",
    "episode_id = 0\n",
    "sample_method = \"ae_coreset\"\n",
    "num_classification = num_classification\n",
    "ratio = 0.01\n",
    "accuracy = score\n",
    "\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.mkdir(dest_dir)\n",
    "\n",
    "now = datetime.now()\n",
    "dest_dir_name = str(now.year) + str(now.month) + str(now.day) + str(now.hour) + str(now.minute) + str(now.second)\n",
    "dest_dir_name = os.path.join(dest_dir, dest_dir_name)\n",
    "\n",
    "if not os.path.exists(dest_dir_name):\n",
    "    os.mkdir(dest_dir_name)\n",
    "save_path = os.path.join(dest_dir_name,'init.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d3e59860",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(dest_dir_name, episode_id, sample_method, labeled_dataset_label, num_classification, ratio, accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "7ce5735ac0795ce80d9ca82db09fbd3aedd56acddf82388eb9cd49352a280e35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
