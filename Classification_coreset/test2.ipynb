{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# DataLoader은 Dataset을 샘플에 쉽게 접근할 수 있도록 순회가능한 객체(iterable)로 감쌉니다\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.models as models \n",
    "\n",
    "import pdb\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "# pretty print. 들여쓰기 등을 지원해준다. \n",
    "\n",
    "import pprint\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# local stuff\n",
    "# 폴더에 있는 경우 A.B 형태로 기술 \n",
    "from dsets.mnist import MNIST\n",
    "from mymodels.mnist_net import Net\n",
    "from auto_encoder import AutoEncoder, ConvAutoEncoder, ae_train\n",
    "from train_test import train\n",
    "from init_pool_tools import obtain_init_pool\n",
    "from coreset import Coreset_Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba76f79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded AE & CAE\n"
     ]
    }
   ],
   "source": [
    "original_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    "    )\n",
    "\n",
    "\n",
    "    # MNIST Dataset을 가공할 수 있는 list로 변경. feature와 label 각각 저장 \n",
    "original_all = []\n",
    "original_dataset = []\n",
    "original_label = [] \n",
    "\n",
    "for sample in original_data : \n",
    "    original_all.append(sample)\n",
    "    feature = np.array(sample[0])\n",
    "    original_dataset.append(list(feature.reshape(len(feature[0])*len(feature[0][0]))))\n",
    "    original_label.append(sample[1])\n",
    "\n",
    "unlabeled_dataset = original_dataset[:]\n",
    "unlabeled_dataset_label = original_label[:]\n",
    "labeled_dataset = [] \n",
    "labeled_dataset_label = []\n",
    "\n",
    "PATH = './weights/'\n",
    "AE = torch.load(PATH + 'AE.pt')  \n",
    "AE.load_state_dict(torch.load(PATH + 'AE_state_dict.pt'))  \n",
    "\n",
    "CAE = torch.load(PATH + 'CAE.pt')  \n",
    "CAE.load_state_dict(torch.load(PATH + 'CAE_state_dict.pt'))  \n",
    "\n",
    "print(\"Successfully loaded AE & CAE\")\n",
    "\n",
    "\n",
    "\n",
    "use_cuda = True\n",
    "\n",
    "\n",
    "torch.manual_seed(23)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    # use_cuda가 true라면 kwargs를 다음과 같이 지정하기. \n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84da4789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max distance from cluster : 0.18\n"
     ]
    }
   ],
   "source": [
    "from active_learn import active_sample, remove_rows\n",
    "\n",
    "sample_size = 1000\n",
    "sampling_method = 'ae_coreset'\n",
    "sample_dataset, sample_index,radius  = active_sample(unlabeled_dataset, labeled_dataset, sample_size, method=sampling_method, model=AE, device=device)\n",
    "\n",
    "sample_data = [unlabeled_dataset[i] for i in sample_index]\n",
    "sample_label = [unlabeled_dataset_label[i] for i in sample_index]\n",
    "\n",
    "    # Sampling에 따른 Datset 수정 \n",
    "labeled_dataset = sample_data[:]\n",
    "labeled_dataset_label = sample_label[:]\n",
    "\n",
    "unlabeled_dataset = remove_rows(unlabeled_dataset, sample_data)\n",
    "for i in sample_index[::-1] : \n",
    "    del unlabeled_dataset_label[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d601c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from active_learn import get_features\n",
    "\n",
    "def make_subgraph(sampling_index, original_dataset, radii, model):\n",
    "\n",
    "    x = [original_dataset[i] for i in sampling_index]\n",
    "    dataset = original_dataset\n",
    "\n",
    "    if model == AE or model == CAE : \n",
    "        x = get_features(model, x, device)\n",
    "        dataset = get_features(model, dataset, device)\n",
    "\n",
    "    dist = pairwise_distances(x,dataset, metric='euclidean')\n",
    "\n",
    "    subgraph= dist.copy()\n",
    "    density_subgraph = []\n",
    "    for i, row in enumerate(dist) : \n",
    "        for j, distance in enumerate(row) : \n",
    "            if distance > radii or j == sampling_index[i] : subgraph[i,j] =int(0) # 자기자신은 제거  \n",
    "            else : subgraph[i,j] = int(1) \n",
    "        \n",
    "        density_subgraph.append(sum(subgraph[i]))\n",
    "    \n",
    "\n",
    "    return np.array(subgraph), density_subgraph\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0bcaf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph, density_subgraph = make_subgraph(sample_index, original_dataset, radius, AE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d8908cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[24.0, 36.0, 162.0, 21.0, 17.0, 122.0, 29.0, 34.0, 10.0, 7.0, 47.0, 44.0, 11.0, 1.0, 147.0, 66.0, 24.0, 120.0, 19.0, 4.0, 16.0, 96.0, 15.0, 93.0, 112.0, 116.0, 42.0, 7.0, 72.0, 24.0, 325.0, 86.0, 11.0, 43.0, 232.0, 19.0, 131.0, 74.0, 18.0, 76.0, 11.0, 38.0, 237.0, 33.0, 14.0, 0.0, 18.0, 58.0, 8.0, 20.0, 6.0, 172.0, 18.0, 283.0, 303.0, 116.0, 33.0, 115.0, 1.0, 2.0, 15.0, 24.0, 151.0, 44.0, 1033.0, 31.0, 40.0, 31.0, 0.0, 11.0, 209.0, 166.0, 34.0, 49.0, 46.0, 20.0, 14.0, 0.0, 1.0, 1393.0, 233.0, 163.0, 4.0, 2.0, 2.0, 23.0, 305.0, 0.0, 6.0, 603.0, 9.0, 132.0, 2.0, 5.0, 1697.0, 47.0, 42.0, 25.0, 6.0, 27.0, 4.0, 40.0, 6.0, 154.0, 3.0, 84.0, 3.0, 32.0, 129.0, 122.0, 4.0, 49.0, 98.0, 10.0, 1.0, 19.0, 324.0, 11.0, 0.0, 0.0, 49.0, 8.0, 44.0, 18.0, 927.0, 420.0, 181.0, 15.0, 158.0, 6.0, 250.0, 21.0, 161.0, 85.0, 486.0, 19.0, 130.0, 985.0, 73.0, 187.0, 28.0, 247.0, 21.0, 36.0, 18.0, 13.0, 3.0, 28.0, 58.0, 16.0, 8.0, 10.0, 166.0, 5.0, 16.0, 9.0, 5.0, 12.0, 24.0, 59.0, 56.0, 21.0, 192.0, 84.0, 7.0, 173.0, 12.0, 11.0, 32.0, 74.0, 19.0, 10.0, 106.0, 21.0, 72.0, 45.0, 688.0, 38.0, 134.0, 651.0, 9.0, 87.0, 0.0, 10.0, 32.0, 15.0, 12.0, 1.0, 0.0, 1.0, 0.0, 42.0, 20.0, 20.0, 16.0, 4.0, 28.0, 17.0, 36.0, 149.0, 123.0, 14.0, 18.0, 260.0, 18.0, 13.0, 96.0, 11.0, 95.0, 0.0, 28.0, 36.0, 67.0, 208.0, 83.0, 100.0, 298.0, 86.0, 0.0, 31.0, 65.0, 25.0, 40.0, 23.0, 21.0, 5.0, 12.0, 24.0, 3.0, 3.0, 3.0, 52.0, 38.0, 6.0, 14.0, 17.0, 16.0, 4.0, 22.0, 4.0, 20.0, 72.0, 46.0, 89.0, 4.0, 352.0, 23.0, 303.0, 14.0, 23.0, 196.0, 39.0, 40.0, 0.0, 1.0, 5.0, 127.0, 77.0, 0.0, 89.0, 164.0, 25.0, 14.0, 196.0, 4.0, 758.0, 209.0, 80.0, 69.0, 29.0, 28.0, 43.0, 18.0, 6.0, 179.0, 41.0, 22.0, 6.0, 3.0, 13.0, 176.0, 284.0, 16.0, 36.0, 31.0, 11.0, 57.0, 361.0, 10.0, 36.0, 67.0, 57.0, 74.0, 74.0, 2.0, 217.0, 3.0, 0.0, 217.0, 69.0, 163.0, 1.0, 29.0, 1037.0, 44.0, 33.0, 86.0, 54.0, 4.0, 176.0, 106.0, 24.0, 11.0, 49.0, 35.0, 78.0, 1420.0, 54.0, 3.0, 23.0, 7.0, 5.0, 10.0, 53.0, 27.0, 177.0, 46.0, 13.0, 0.0, 107.0, 251.0, 0.0, 180.0, 0.0, 913.0, 90.0, 0.0, 102.0, 180.0, 332.0, 134.0, 122.0, 20.0, 127.0, 223.0, 17.0, 146.0, 10.0, 7.0, 6.0, 176.0, 16.0, 27.0, 32.0, 42.0, 190.0, 71.0, 145.0, 4.0, 9.0, 376.0, 20.0, 5.0, 8.0, 1.0, 36.0, 47.0, 22.0, 30.0, 11.0, 1.0, 0.0, 2.0, 154.0, 175.0, 466.0, 325.0, 6.0, 1.0, 159.0, 23.0, 20.0, 26.0, 18.0, 28.0, 26.0, 29.0, 284.0, 56.0, 10.0, 13.0, 159.0, 236.0, 25.0, 46.0, 456.0, 169.0, 199.0, 13.0, 101.0, 11.0, 14.0, 142.0, 160.0, 265.0, 26.0, 5.0, 10.0, 133.0, 84.0, 18.0, 21.0, 74.0, 56.0, 1.0, 104.0, 23.0, 186.0, 21.0, 109.0, 47.0, 146.0, 137.0, 525.0, 1582.0, 4.0, 0.0, 52.0, 2.0, 232.0, 1137.0, 10.0, 18.0, 176.0, 527.0, 355.0, 30.0, 67.0, 74.0, 127.0, 431.0, 29.0, 17.0, 3.0, 15.0, 11.0, 109.0, 7.0, 6.0, 116.0, 4.0, 5.0, 299.0, 4.0, 83.0, 19.0, 28.0, 105.0, 95.0, 4630.0, 74.0, 82.0, 111.0, 12.0, 282.0, 18.0, 56.0, 31.0, 1430.0, 73.0, 199.0, 103.0, 1225.0, 2.0, 137.0, 25.0, 6.0, 0.0, 17.0, 12.0, 343.0, 2.0, 4.0, 53.0, 28.0, 18.0, 0.0, 108.0, 80.0, 33.0, 8.0, 9.0, 84.0, 202.0, 78.0, 84.0, 105.0, 31.0, 9.0, 260.0, 49.0, 1568.0, 22.0, 21.0, 159.0, 54.0, 9.0, 184.0, 15.0, 37.0, 89.0, 31.0, 48.0, 471.0, 15.0, 151.0, 181.0, 5.0, 50.0, 0.0, 8.0, 693.0, 34.0, 8.0, 2.0, 4.0, 0.0, 9.0, 13.0, 72.0, 66.0, 1222.0, 20.0, 43.0, 3.0, 74.0, 3.0, 153.0, 144.0, 53.0, 71.0, 185.0, 644.0, 5.0, 371.0, 8.0, 30.0, 154.0, 9.0, 46.0, 362.0, 41.0, 14.0, 61.0, 20.0, 132.0, 16.0, 55.0, 61.0, 35.0, 248.0, 12.0, 20.0, 31.0, 4.0, 3.0, 24.0, 91.0, 143.0, 33.0, 49.0, 29.0, 13.0, 22.0, 104.0, 193.0, 81.0, 486.0, 2.0, 8.0, 20.0, 285.0, 89.0, 251.0, 213.0, 317.0, 71.0, 4.0, 73.0, 2.0, 10.0, 62.0, 82.0, 36.0, 158.0, 57.0, 6.0, 6.0, 24.0, 8.0, 44.0, 32.0, 172.0, 11.0, 10.0, 32.0, 23.0, 3.0, 40.0, 14.0, 2.0, 109.0, 9.0, 24.0, 15.0, 23.0, 136.0, 7.0, 28.0, 5.0, 29.0, 70.0, 2.0, 445.0, 573.0, 117.0, 188.0, 30.0, 122.0, 27.0, 26.0, 18.0, 9.0, 4.0, 15.0, 15.0, 0.0, 9.0, 103.0, 55.0, 90.0, 41.0, 452.0, 33.0, 32.0, 2.0, 38.0, 12.0, 58.0, 64.0, 8.0, 25.0, 15.0, 1688.0, 31.0, 82.0, 51.0, 22.0, 49.0, 3.0, 15.0, 13.0, 183.0, 2.0, 1.0, 0.0, 88.0, 8.0, 1.0, 52.0, 5.0, 150.0, 37.0, 12.0, 343.0, 75.0, 26.0, 13.0, 89.0, 1.0, 23.0, 21.0, 51.0, 21.0, 3.0, 5.0, 29.0, 8.0, 67.0, 69.0, 189.0, 3.0, 0.0, 108.0, 167.0, 36.0, 4.0, 106.0, 15.0, 39.0, 10.0, 1157.0, 395.0, 26.0, 9.0, 5.0, 221.0, 4.0, 153.0, 107.0, 15.0, 57.0, 244.0, 4.0, 472.0, 22.0, 21.0, 53.0, 267.0, 2.0, 20.0, 26.0, 0.0, 65.0, 357.0, 3.0, 112.0, 147.0, 25.0, 54.0, 25.0, 15.0, 5.0, 24.0, 2.0, 27.0, 20.0, 94.0, 244.0, 68.0, 347.0, 52.0, 231.0, 27.0, 103.0, 193.0, 157.0, 37.0, 13.0, 19.0, 20.0, 158.0, 24.0, 1.0, 8.0, 14.0, 5.0, 26.0, 29.0, 1.0, 1123.0, 0.0, 157.0, 8.0, 21.0, 27.0, 27.0, 1.0, 16.0, 100.0, 3.0, 351.0, 12.0, 109.0, 52.0, 9.0, 207.0, 6.0, 148.0, 7.0, 203.0, 71.0, 39.0, 11.0, 1152.0, 40.0, 101.0, 49.0, 6.0, 29.0, 81.0, 174.0, 155.0, 6.0, 12.0, 2.0, 8.0, 16.0, 6.0, 12.0, 17.0, 3.0, 52.0, 16.0, 9.0, 2.0, 28.0, 82.0, 173.0, 50.0, 1.0, 2.0, 1.0, 34.0, 56.0, 27.0, 17.0, 39.0, 75.0, 16.0, 4.0, 81.0, 388.0, 0.0, 85.0, 74.0, 639.0, 41.0, 2.0, 10.0, 1.0, 56.0, 40.0, 0.0, 26.0, 43.0, 4.0, 8.0, 8.0, 13.0, 82.0, 0.0, 2.0, 7.0, 0.0, 10.0, 0.0, 6.0, 714.0, 12.0, 17.0, 29.0, 44.0, 309.0, 3096.0, 15.0, 1.0, 8.0, 26.0, 32.0, 168.0, 26.0, 42.0, 0.0, 6.0, 11.0, 6.0, 4.0, 26.0, 95.0, 75.0, 0.0, 3.0, 7.0, 71.0, 186.0, 61.0, 126.0, 2.0, 3348.0, 1.0, 34.0, 128.0, 82.0, 3.0, 4.0, 140.0, 20.0, 26.0, 31.0, 67.0, 3308.0, 20.0, 89.0, 69.0, 198.0, 1.0, 3668.0, 0.0, 134.0, 34.0, 21.0, 48.0, 5.0, 117.0, 41.0, 7.0, 459.0, 23.0, 77.0, 31.0, 31.0, 43.0, 110.0, 103.0, 1.0, 1027.0, 442.0, 125.0, 192.0, 4.0, 13.0, 49.0, 11.0, 87.0, 581.0, 27.0, 18.0, 10.0, 11.0, 64.0, 18.0, 0.0, 16.0, 289.0, 8.0, 3.0, 16.0, 6.0, 214.0, 42.0, 19.0, 57.0, 4.0, 15.0, 3.0, 12.0, 14.0, 8.0, 2.0, 116.0, 19.0, 11.0, 256.0, 164.0, 21.0, 9.0, 51.0, 256.0, 109.0, 20.0, 42.0, 14.0, 18.0, 10.0, 19.0, 234.0, 86.0, 44.0, 2.0, 170.0, 40.0, 90.0, 33.0, 53.0, 909.0, 98.0, 17.0, 0.0, 1.0, 381.0, 60.0, 154.0, 44.0, 68.0, 29.0, 121.0, 8.0, 59.0, 14.0, 63.0, 67.0, 605.0, 16.0, 11.0, 463.0, 5.0, 16.0, 18.0, 4.0]\n",
      "0.0 4630.0\n"
     ]
    }
   ],
   "source": [
    "print(subgraph)\n",
    "print(density_subgraph)\n",
    "print(min(density_subgraph) ,max(density_subgraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "85cc0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가 수정 필요! 뭔가 이상함!\n",
    "def adjacency_subgraph(labeled_dataset, labeled_dataset_label, radii, model) : \n",
    "    dataset = labeled_dataset\n",
    "    num_subgraph = len(labeled_dataset_label)\n",
    "    if model == AE or model==CAE : \n",
    "        dataset = get_features(model,dataset, device)\n",
    "    \n",
    "    dist = pairwise_distances(dataset, dataset, metric='euclidean')\n",
    "    adj_dist = dist.copy()\n",
    "    \n",
    "    for i, row in enumerate(dist) : \n",
    "        adj_d = np.where(row < 2*radii[0])[0] \n",
    "        for j, distance in enumerate(row) : \n",
    "            if distance >= 2*radii[0] : adj_dist[i,j] = int(0)   \n",
    "            # 뭔가 이상.. 여기선 문제없지만, 밖에서 다시 거리를 재면 성립안하는 경우가 있음. \n",
    "            elif 2*radii[0] > distance and distance >= radii[0]  : \n",
    "                adj_dist[i,j] = int(1)\n",
    "            elif i==j : adj_dist[i,j] = int(0) # 자기자신은 제거\n",
    "            else : \n",
    "                print('Break')\n",
    "\n",
    "    classified_subgraph_index = []\n",
    "\n",
    "    for i in range(num_subgraph) : \n",
    "        i_sub_class = \"x\"\n",
    "        adj_index = np.where(adj_dist[ :,i] ==1)\n",
    "        if len(adj_index[0])==0 : continue \n",
    "        i_sub_class = labeled_dataset_label[i]\n",
    "\n",
    "        for j in adj_index[0] :  \n",
    "            if i_sub_class != labeled_dataset_label[j] : \n",
    "                i_sub_class = \"x\"\n",
    "                continue\n",
    "        if i_sub_class != \"x\" : \n",
    "            classified_subgraph_index.append(i)\n",
    "    \n",
    "    classified_label = [labeled_dataset_label[i] for i in classified_subgraph_index]\n",
    "\n",
    "    return dist, adj_dist, classified_subgraph_index, classified_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dc3aa299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확인해야 하는 것. M을 조건으로 추가했을 때 어떤가 \n",
    "# 왜 접하고 있는 subgraph의 개수가 6개를 넘어가는 게 있지? 점이 중복된 건가? \n",
    "\n",
    "dist_class, adj_dist, classified_subgraph_index, pseudo_class_label = adjacency_subgraph(labeled_dataset, labeled_dataset_label, radius, AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0c34be55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        5.0313087 5.5453887 ... 4.70069   6.020795  2.71513  ]\n",
      " [5.0313087 0.        4.9629364 ... 6.3302293 2.0796616 2.7819836]\n",
      " [5.5453887 4.9629364 0.        ... 9.53124   3.6579835 5.7253575]\n",
      " ...\n",
      " [4.70069   6.3302293 9.53124   ... 0.        8.268933  3.945808 ]\n",
      " [6.020795  2.0796616 3.6579835 ... 8.268933  0.        4.448273 ]\n",
      " [2.71513   2.7819836 5.7253575 ... 3.945808  4.448273  0.       ]]\n",
      "[0, 1, 4, 5, 7, 9, 10, 12, 14, 15, 17, 19, 20, 22, 23, 25, 26, 27, 29, 31, 33, 34, 35, 38, 40, 44, 45, 47, 49, 50, 52, 56, 58, 59, 60, 61, 66, 67, 68, 69, 72, 73, 76, 77, 82, 83, 85, 87, 88, 90, 92, 93, 96, 97, 98, 99, 100, 101, 102, 105, 106, 107, 108, 110, 113, 115, 117, 119, 121, 127, 130, 131, 132, 138, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 158, 159, 162, 163, 164, 166, 167, 168, 169, 170, 171, 173, 174, 178, 179, 180, 182, 183, 184, 186, 188, 190, 191, 192, 193, 194, 198, 203, 205, 207, 209, 210, 211, 214, 218, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 237, 238, 239, 240, 242, 245, 246, 249, 250, 253, 254, 255, 257, 261, 264, 267, 269, 270, 271, 272, 273, 275, 276, 277, 278, 279, 282, 285, 288, 294, 297, 298, 299, 301, 302, 306, 308, 310, 312, 314, 315, 317, 319, 321, 322, 323, 324, 327, 329, 331, 333, 342, 344, 345, 346, 347, 353, 356, 357, 359, 361, 362, 363, 367, 368, 369, 370, 371, 372, 375, 377, 378, 380, 381, 382, 383, 384, 385, 388, 389, 390, 393, 394, 397, 398, 400, 401, 405, 406, 407, 412, 414, 418, 420, 423, 425, 426, 428, 429, 431, 432, 441, 442, 445, 447, 448, 451, 455, 456, 458, 463, 464, 465, 466, 467, 470, 475, 482, 484, 485, 486, 491, 497, 498, 500, 503, 506, 508, 509, 511, 512, 514, 515, 516, 519, 522, 523, 524, 525, 526, 527, 528, 530, 533, 537, 539, 543, 545, 546, 549, 551, 555, 562, 564, 565, 566, 568, 570, 571, 572, 573, 576, 578, 579, 580, 582, 583, 585, 586, 587, 588, 590, 591, 592, 594, 595, 597, 598, 600, 602, 603, 604, 605, 606, 607, 608, 609, 610, 612, 614, 615, 616, 619, 620, 621, 625, 626, 627, 629, 630, 632, 633, 634, 635, 636, 637, 639, 640, 641, 644, 645, 649, 650, 651, 652, 656, 658, 661, 662, 663, 664, 665, 666, 667, 670, 672, 673, 675, 676, 677, 680, 683, 684, 685, 687, 688, 691, 695, 696, 697, 698, 700, 701, 702, 706, 711, 715, 719, 720, 723, 724, 725, 728, 732, 733, 734, 735, 736, 737, 738, 741, 742, 743, 744, 748, 750, 754, 755, 756, 757, 758, 759, 761, 764, 765, 766, 767, 768, 770, 772, 775, 781, 782, 783, 789, 793, 794, 795, 796, 798, 799, 802, 804, 805, 806, 807, 810, 813, 814, 817, 818, 820, 821, 823, 824, 825, 827, 829, 833, 834, 835, 837, 842, 845, 847, 849, 850, 859, 861, 862, 863, 864, 865, 866, 871, 872, 873, 874, 875, 876, 878, 880, 881, 885, 887, 888, 889, 892, 893, 894, 900, 901, 905, 906, 909, 910, 911, 912, 920, 921, 926, 927, 929, 932, 933, 937, 940, 941, 943, 945, 946, 948, 949, 952, 953, 954, 955, 956, 957, 958, 960, 962, 963, 964, 965, 968, 969, 970, 971, 973, 976, 977, 978, 980, 981, 982, 983, 989, 991, 992, 993, 997, 998]\n",
      "528\n",
      "[0, 3, 0, 1, 3, 5, 3, 2, 3, 0, 0, 3, 2, 0, 0, 2, 0, 0, 0, 1, 2, 3, 2, 1, 3, 3, 0, 1, 3, 0, 1, 1, 3, 3, 0, 1, 0, 0, 2, 3, 3, 0, 0, 5, 3, 0, 0, 3, 0, 3, 5, 3, 0, 3, 2, 0, 1, 3, 3, 0, 3, 3, 1, 2, 1, 0, 0, 1, 5, 0, 1, 0, 7, 2, 3, 0, 2, 3, 0, 0, 0, 2, 0, 1, 0, 1, 3, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 3, 0, 1, 0, 7, 3, 1, 1, 1, 3, 2, 3, 2, 0, 0, 3, 1, 1, 0, 2, 3, 0, 0, 0, 0, 0, 1, 0, 3, 3, 0, 2, 0, 1, 2, 1, 1, 0, 5, 2, 0, 0, 1, 1, 0, 1, 3, 5, 0, 0, 0, 0, 3, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0, 1, 3, 3, 2, 0, 0, 2, 2, 0, 0, 3, 2, 3, 0, 3, 3, 1, 0, 3, 1, 5, 0, 0, 3, 0, 1, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 3, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 2, 3, 1, 5, 0, 3, 0, 2, 5, 0, 0, 2, 0, 0, 5, 2, 3, 3, 0, 3, 0, 0, 0, 0, 0, 0, 5, 0, 2, 0, 0, 2, 1, 0, 3, 0, 0, 5, 2, 1, 0, 0, 0, 2, 0, 3, 0, 3, 1, 0, 3, 1, 0, 3, 1, 1, 1, 0, 0, 3, 3, 2, 0, 0, 0, 3, 0, 2, 1, 5, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 5, 0, 1, 2, 1, 2, 0, 3, 1, 3, 2, 3, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 3, 1, 0, 0, 2, 0, 3, 0, 1, 2, 0, 0, 0, 0, 3, 1, 0, 5, 0, 1, 0, 0, 0, 3, 0, 3, 3, 0, 0, 1, 0, 2, 2, 2, 3, 1, 2, 0, 2, 0, 2, 3, 0, 2, 1, 3, 2, 0, 0, 3, 0, 1, 1, 3, 3, 6, 1, 1, 0, 0, 3, 0, 2, 7, 0, 0, 0, 0, 3, 0, 2, 0, 3, 0, 3, 1, 0, 1, 1, 3, 0, 3, 0, 1, 0, 3, 0, 3, 0, 5, 2, 2, 0, 2, 3, 2, 0, 0, 0, 0, 3, 3, 2, 1, 3, 3, 0, 0, 0, 2, 2, 2, 0, 1, 2, 0, 0, 0, 3, 3, 3, 0, 2, 3, 1, 3, 3, 0, 0, 5, 3, 0, 2, 3, 0, 1, 0, 0, 0, 0, 1, 2, 7, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 2, 0, 0, 2, 0, 3, 0, 3, 3, 2, 0, 0, 0, 3, 1, 0, 2, 2, 5, 1, 0, 5, 0, 1, 3, 3, 0, 5, 2, 3, 2, 0, 2, 0, 0, 3, 3, 1, 0, 3, 2, 5, 0, 1, 1, 1, 2, 0, 3, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(dist_class)\n",
    "print(classified_subgraph_index)\n",
    "print(len(classified_subgraph_index))\n",
    "print(pseudo_class_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c326ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def first_classification(classified_subgraph_index, pseudo_class_label, subgraph, density_subgraph, ratio) : \n",
    "    dense_classified_subgraph_index = [density_subgraph[i] for i in classified_subgraph_index]\n",
    "    sorted_density = sorted(dense_classified_subgraph_index, reverse=True)\n",
    "    rank = int(ratio*len(density_subgraph))\n",
    "\n",
    "    M = sorted_density[rank-1]\n",
    "\n",
    "    classification = defaultdict(list)\n",
    "    for i, index in enumerate(classified_subgraph_index) : \n",
    "        if density_subgraph[index] < M : continue\n",
    "        x_index = list(np.where(subgraph[index] == 1)[0])\n",
    "        label = pseudo_class_label[i]\n",
    "    \n",
    "        classification[label] = classification[label] + x_index\n",
    "    return classification\n",
    "\n",
    "def check_performance(classification, original_label) : \n",
    "    score = defaultdict(list) \n",
    "    all_score = 0 \n",
    "    all_count = 0 \n",
    "    for i in sorted(list(classification.keys())) : \n",
    "        x_index = classification[i] \n",
    "        num_x = len(x_index)\n",
    "        count = 0 \n",
    "        for index in x_index :\n",
    "            if original_label[index] == i : count += 1 \n",
    "        \n",
    "        i_score = count/num_x\n",
    "        all_score += count\n",
    "        all_count += num_x\n",
    "        score[i] = [num_x, i_score]\n",
    "    \n",
    "    all_score = all_score/all_count\n",
    "    \n",
    "    return all_count, all_score, score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "52e00873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7716\n",
      "0.9612493519958528\n",
      "defaultdict(<class 'list'>, {1: [3026, 0.9619960343688037], 2: [1290, 0.9604651162790697], 3: [1235, 0.928744939271255], 6: [1157, 0.9723422644770959], 7: [1008, 0.9871031746031746]})\n"
     ]
    }
   ],
   "source": [
    "check = first_classification(classified_subgraph_index, pseudo_class_label, subgraph, density_subgraph, 0.02)\n",
    "num_classification, score, dic_score = check_performance(check, original_label)\n",
    "\n",
    "print(num_classification)\n",
    "print(score)\n",
    "print(dic_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "41cf267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dest_dir 위치에 결과 기록하기. 입력할 결과들을 입력값으로 넣음 \n",
    "def log(dest_dir, episode_id, sample_method,  label_dataset_label, num_classification, ratio, accuracy):\n",
    "    # log file은 dest_dir 위치에 log.csv를 두기 위한 주소이다. \n",
    "    log_file = os.path.join(dest_dir, 'log.csv')\n",
    "\n",
    "    # 주소가 정확하지 않을  해당 위치에 파일이 존재하지 않을 때, log_rows를 다음과 같이 정한다. \n",
    "    if not os.path.exists(log_file):\n",
    "        log_rows = [['Episode Id','Sample Method','Labeled Pool', 'Num of classification', 'Ratio', 'Accuracy']]\n",
    "    # 파일이 존재할 때에는 데이터를 처리해서 불러온다. \n",
    "    else:\n",
    "        log_rows = np.genfromtxt(log_file, delimiter=',', dtype=str, encoding='utf-8').tolist()\n",
    "\n",
    "    # episod_id, sample_mthod, sample_time 등의 값을 추가한다. \n",
    "    log_rows.append([episode_id,sample_method, len(label_dataset_label), num_classification, ratio, accuracy])\n",
    "    \n",
    "    # 데이터를 저장한다. 파일이 없다면 새로 만든다. \n",
    "    np.savetxt(log_file,log_rows,'%s,%s,%s,%s,%s,%s',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a45a88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"output/\"\n",
    "dataset_name = \"MNIST\"\n",
    "\n",
    "\n",
    "dest_dir = os.path.join(output_dir, dataset_name)\n",
    "episode_id = 0\n",
    "sample_method = \"ae_coreset\"\n",
    "num_classification = num_classification\n",
    "ratio = 0.01\n",
    "accuracy = score\n",
    "\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.mkdir(dest_dir)\n",
    "\n",
    "now = datetime.now()\n",
    "dest_dir_name = str(now.year) + str(now.month) + str(now.day) + str(now.hour) + str(now.minute) + str(now.second)\n",
    "dest_dir_name = os.path.join(dest_dir, dest_dir_name)\n",
    "\n",
    "if not os.path.exists(dest_dir_name):\n",
    "    os.mkdir(dest_dir_name)\n",
    "save_path = os.path.join(dest_dir_name,'init.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d3e59860",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(dest_dir_name, episode_id, sample_method, labeled_dataset_label, num_classification, ratio, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a0a7cefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009000000000000001, 0.010000000000000002, 0.011, 0.012, 0.013000000000000001, 0.014000000000000002, 0.015, 0.016, 0.017, 0.018000000000000002, 0.019000000000000003, 0.02, 0.021, 0.022000000000000002, 0.023, 0.024, 0.025, 0.026000000000000002, 0.027000000000000003, 0.028, 0.029, 0.030000000000000002, 0.031, 0.032, 0.033, 0.034, 0.035, 0.036000000000000004, 0.037000000000000005, 0.038, 0.039, 0.04, 0.041, 0.042, 0.043000000000000003, 0.044000000000000004, 0.045, 0.046, 0.047, 0.048, 0.049, 0.05, 0.051000000000000004, 0.052000000000000005, 0.053000000000000005, 0.054, 0.055, 0.056, 0.057, 0.058, 0.059000000000000004, 0.060000000000000005, 0.061, 0.062, 0.063, 0.064, 0.065, 0.066, 0.067, 0.068, 0.069, 0.07, 0.07100000000000001, 0.07200000000000001, 0.07300000000000001, 0.074, 0.075, 0.076, 0.077, 0.078, 0.079, 0.08, 0.081, 0.082, 0.083, 0.084, 0.085, 0.08600000000000001, 0.08700000000000001, 0.08800000000000001, 0.089, 0.09, 0.091, 0.092, 0.093, 0.094, 0.095, 0.096, 0.097, 0.098, 0.099, 0.1, 0.101, 0.10200000000000001, 0.10300000000000001, 0.10400000000000001, 0.10500000000000001, 0.106, 0.107, 0.108, 0.109, 0.11, 0.111, 0.112, 0.113, 0.114, 0.115, 0.116, 0.117, 0.11800000000000001, 0.11900000000000001, 0.12000000000000001, 0.121, 0.122, 0.123, 0.124, 0.125, 0.126, 0.127, 0.128, 0.129, 0.13, 0.131, 0.132, 0.133, 0.134, 0.135, 0.136, 0.137, 0.138, 0.139, 0.14, 0.14100000000000001, 0.14200000000000002, 0.14300000000000002, 0.14400000000000002, 0.14500000000000002, 0.146, 0.147, 0.148, 0.149, 0.15, 0.151, 0.152, 0.153, 0.154, 0.155, 0.156, 0.157, 0.158, 0.159, 0.16, 0.161, 0.162, 0.163, 0.164, 0.165, 0.166, 0.167, 0.168, 0.169, 0.17, 0.171, 0.17200000000000001, 0.17300000000000001, 0.17400000000000002, 0.17500000000000002, 0.17600000000000002, 0.177, 0.178, 0.179, 0.18, 0.181, 0.182, 0.183, 0.184, 0.185, 0.186, 0.187, 0.188, 0.189, 0.19, 0.191, 0.192, 0.193, 0.194, 0.195, 0.196, 0.197, 0.198, 0.199, 0.2, 0.201, 0.202, 0.203, 0.20400000000000001, 0.20500000000000002, 0.20600000000000002, 0.20700000000000002, 0.20800000000000002, 0.20900000000000002, 0.21, 0.211, 0.212, 0.213, 0.214, 0.215, 0.216, 0.217, 0.218, 0.219, 0.22, 0.221, 0.222, 0.223, 0.224, 0.225, 0.226, 0.227, 0.228, 0.229, 0.23, 0.231, 0.232, 0.233, 0.234, 0.23500000000000001, 0.23600000000000002, 0.23700000000000002, 0.23800000000000002, 0.23900000000000002, 0.24000000000000002, 0.241, 0.242, 0.243, 0.244, 0.245, 0.246, 0.247, 0.248, 0.249, 0.25, 0.251, 0.252, 0.253, 0.254, 0.255, 0.256, 0.257, 0.258, 0.259, 0.26, 0.261, 0.262, 0.263, 0.264, 0.265, 0.266, 0.267, 0.268, 0.269, 0.27, 0.271, 0.272, 0.273, 0.274, 0.275, 0.276, 0.277, 0.278, 0.279, 0.28, 0.281, 0.28200000000000003, 0.28300000000000003, 0.28400000000000003, 0.28500000000000003, 0.28600000000000003, 0.28700000000000003, 0.28800000000000003, 0.28900000000000003, 0.29, 0.291, 0.292, 0.293, 0.294, 0.295, 0.296, 0.297, 0.298, 0.299, 0.3, 0.301, 0.302, 0.303, 0.304, 0.305, 0.306, 0.307, 0.308, 0.309, 0.31, 0.311, 0.312, 0.313, 0.314, 0.315, 0.316, 0.317, 0.318, 0.319, 0.32, 0.321, 0.322, 0.323, 0.324, 0.325, 0.326, 0.327, 0.328, 0.329, 0.33, 0.331, 0.332, 0.333, 0.334, 0.335, 0.336, 0.337, 0.338, 0.339, 0.34, 0.341, 0.342, 0.343, 0.34400000000000003, 0.34500000000000003, 0.34600000000000003, 0.34700000000000003, 0.34800000000000003, 0.34900000000000003, 0.35000000000000003, 0.35100000000000003, 0.35200000000000004, 0.353, 0.354, 0.355, 0.356, 0.357, 0.358, 0.359, 0.36, 0.361, 0.362, 0.363, 0.364, 0.365, 0.366, 0.367, 0.368, 0.369, 0.37, 0.371, 0.372, 0.373, 0.374, 0.375, 0.376, 0.377, 0.378, 0.379, 0.38, 0.381, 0.382, 0.383, 0.384, 0.385, 0.386, 0.387, 0.388, 0.389, 0.39, 0.391, 0.392, 0.393, 0.394, 0.395, 0.396, 0.397, 0.398, 0.399, 0.4, 0.401, 0.402, 0.403, 0.404, 0.405, 0.406, 0.40700000000000003, 0.40800000000000003, 0.40900000000000003, 0.41000000000000003, 0.41100000000000003, 0.41200000000000003, 0.41300000000000003, 0.41400000000000003, 0.41500000000000004, 0.41600000000000004, 0.41700000000000004, 0.418, 0.419, 0.42, 0.421, 0.422, 0.423, 0.424, 0.425, 0.426, 0.427, 0.428, 0.429, 0.43, 0.431, 0.432, 0.433, 0.434, 0.435, 0.436, 0.437, 0.438, 0.439, 0.44, 0.441, 0.442, 0.443, 0.444, 0.445, 0.446, 0.447, 0.448, 0.449, 0.45, 0.451, 0.452, 0.453, 0.454, 0.455, 0.456, 0.457, 0.458, 0.459, 0.46, 0.461, 0.462, 0.463, 0.464, 0.465, 0.466, 0.467, 0.468, 0.46900000000000003, 0.47000000000000003, 0.47100000000000003, 0.47200000000000003, 0.47300000000000003, 0.47400000000000003, 0.47500000000000003, 0.47600000000000003, 0.47700000000000004, 0.47800000000000004, 0.47900000000000004, 0.48000000000000004, 0.481, 0.482, 0.483, 0.484, 0.485, 0.486, 0.487, 0.488, 0.489, 0.49, 0.491, 0.492, 0.493, 0.494, 0.495, 0.496, 0.497, 0.498, 0.499, 0.5, 0.501, 0.502, 0.503, 0.504, 0.505, 0.506, 0.507, 0.508, 0.509, 0.51, 0.511, 0.512, 0.513, 0.514, 0.515, 0.516, 0.517, 0.518, 0.519, 0.52, 0.521, 0.522, 0.523, 0.524, 0.525, 0.526, 0.527, 0.528, 0.529, 0.53, 0.531, 0.532, 0.533, 0.534, 0.535, 0.536, 0.537, 0.538, 0.539, 0.54, 0.541, 0.542, 0.543, 0.544, 0.545, 0.546, 0.547, 0.548, 0.549, 0.55, 0.551, 0.552, 0.553, 0.554, 0.555, 0.556, 0.557, 0.558, 0.559, 0.56, 0.561, 0.562, 0.5630000000000001, 0.5640000000000001, 0.5650000000000001, 0.5660000000000001, 0.5670000000000001, 0.5680000000000001, 0.5690000000000001, 0.5700000000000001, 0.5710000000000001, 0.5720000000000001, 0.5730000000000001, 0.5740000000000001, 0.5750000000000001, 0.5760000000000001, 0.5770000000000001, 0.578, 0.579, 0.58, 0.581, 0.582, 0.583, 0.584, 0.585, 0.586, 0.587, 0.588, 0.589, 0.59, 0.591, 0.592, 0.593, 0.594, 0.595, 0.596, 0.597, 0.598, 0.599, 0.6, 0.601, 0.602, 0.603, 0.604, 0.605, 0.606, 0.607, 0.608, 0.609, 0.61, 0.611, 0.612, 0.613, 0.614, 0.615, 0.616, 0.617, 0.618, 0.619, 0.62, 0.621, 0.622, 0.623, 0.624, 0.625, 0.626, 0.627, 0.628, 0.629, 0.63, 0.631, 0.632, 0.633, 0.634, 0.635, 0.636, 0.637, 0.638, 0.639, 0.64, 0.641, 0.642, 0.643, 0.644, 0.645, 0.646, 0.647, 0.648, 0.649, 0.65, 0.651, 0.652, 0.653, 0.654, 0.655, 0.656, 0.657, 0.658, 0.659, 0.66, 0.661, 0.662, 0.663, 0.664, 0.665, 0.666, 0.667, 0.668, 0.669, 0.67, 0.671, 0.672, 0.673, 0.674, 0.675, 0.676, 0.677, 0.678, 0.679, 0.68, 0.681, 0.682, 0.683, 0.684, 0.685, 0.686, 0.687, 0.6880000000000001, 0.6890000000000001, 0.6900000000000001, 0.6910000000000001, 0.6920000000000001, 0.6930000000000001, 0.6940000000000001, 0.6950000000000001, 0.6960000000000001, 0.6970000000000001, 0.6980000000000001, 0.6990000000000001, 0.7000000000000001, 0.7010000000000001, 0.7020000000000001, 0.7030000000000001, 0.7040000000000001, 0.705, 0.706, 0.707, 0.708, 0.709, 0.71, 0.711, 0.712, 0.713, 0.714, 0.715, 0.716, 0.717, 0.718, 0.719, 0.72, 0.721, 0.722, 0.723, 0.724, 0.725, 0.726, 0.727, 0.728, 0.729, 0.73, 0.731, 0.732, 0.733, 0.734, 0.735, 0.736, 0.737, 0.738, 0.739, 0.74, 0.741, 0.742, 0.743, 0.744, 0.745, 0.746, 0.747, 0.748, 0.749, 0.75, 0.751, 0.752, 0.753, 0.754, 0.755, 0.756, 0.757, 0.758, 0.759, 0.76, 0.761, 0.762, 0.763, 0.764, 0.765, 0.766, 0.767, 0.768, 0.769, 0.77, 0.771, 0.772, 0.773, 0.774, 0.775, 0.776, 0.777, 0.778, 0.779, 0.78, 0.781, 0.782, 0.783, 0.784, 0.785, 0.786, 0.787, 0.788, 0.789, 0.79, 0.791, 0.792, 0.793, 0.794, 0.795, 0.796, 0.797, 0.798, 0.799, 0.8, 0.801, 0.802, 0.803, 0.804, 0.805, 0.806, 0.807, 0.808, 0.809, 0.81, 0.811, 0.812, 0.8130000000000001, 0.8140000000000001, 0.8150000000000001, 0.8160000000000001, 0.8170000000000001, 0.8180000000000001, 0.8190000000000001, 0.8200000000000001, 0.8210000000000001, 0.8220000000000001, 0.8230000000000001, 0.8240000000000001, 0.8250000000000001, 0.8260000000000001, 0.8270000000000001, 0.8280000000000001, 0.8290000000000001, 0.8300000000000001, 0.8310000000000001, 0.8320000000000001, 0.8330000000000001, 0.834, 0.835, 0.836, 0.837, 0.838, 0.839, 0.84, 0.841, 0.842, 0.843, 0.844, 0.845, 0.846, 0.847, 0.848, 0.849, 0.85, 0.851, 0.852, 0.853, 0.854, 0.855, 0.856, 0.857, 0.858, 0.859, 0.86, 0.861, 0.862, 0.863, 0.864, 0.865, 0.866, 0.867, 0.868, 0.869, 0.87, 0.871, 0.872, 0.873, 0.874, 0.875, 0.876, 0.877, 0.878, 0.879, 0.88, 0.881, 0.882, 0.883, 0.884, 0.885, 0.886, 0.887, 0.888, 0.889, 0.89, 0.891, 0.892, 0.893, 0.894, 0.895, 0.896, 0.897, 0.898, 0.899, 0.9, 0.901, 0.902, 0.903, 0.904, 0.905, 0.906, 0.907, 0.908, 0.909, 0.91, 0.911, 0.912, 0.913, 0.914, 0.915, 0.916, 0.917, 0.918, 0.919, 0.92, 0.921, 0.922, 0.923, 0.924, 0.925, 0.926, 0.927, 0.928, 0.929, 0.93, 0.931, 0.932, 0.933, 0.934, 0.935, 0.936, 0.937, 0.9380000000000001, 0.9390000000000001, 0.9400000000000001, 0.9410000000000001, 0.9420000000000001, 0.9430000000000001, 0.9440000000000001, 0.9450000000000001, 0.9460000000000001, 0.9470000000000001, 0.9480000000000001, 0.9490000000000001, 0.9500000000000001, 0.9510000000000001, 0.9520000000000001, 0.9530000000000001, 0.9540000000000001, 0.9550000000000001, 0.9560000000000001, 0.9570000000000001, 0.9580000000000001, 0.9590000000000001, 0.9600000000000001, 0.961, 0.962, 0.963, 0.964, 0.965, 0.966, 0.967, 0.968, 0.969, 0.97, 0.971, 0.972, 0.973, 0.974, 0.975, 0.976, 0.977, 0.978, 0.979, 0.98, 0.981, 0.982, 0.983, 0.984, 0.985, 0.986, 0.987, 0.988, 0.989, 0.99, 0.991, 0.992, 0.993, 0.994, 0.995, 0.996, 0.997, 0.998, 0.999]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "a = list(np.arange(0.001,1, 0.001))\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
