{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35615e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# DataLoader은 Dataset을 샘플에 쉽게 접근할 수 있도록 순회가능한 객체(iterable)로 감쌉니다\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.models as models \n",
    "\n",
    "import pdb\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "# pretty print. 들여쓰기 등을 지원해준다. \n",
    "\n",
    "import pprint\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# local stuff\n",
    "# 폴더에 있는 경우 A.B 형태로 기술 \n",
    "from dsets.mnist import MNIST\n",
    "from mymodels.mnist_net import Net\n",
    "from auto_encoder import AutoEncoder, ConvAutoEncoder, ae_train\n",
    "from train_test import MNIST_train, MNIST_test\n",
    "from init_pool_tools import obtain_init_pool\n",
    "from coreset import Coreset_Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59fb7e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded AE & CAE\n"
     ]
    }
   ],
   "source": [
    "original_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    "    )\n",
    "\n",
    "\n",
    "    # MNIST Dataset을 가공할 수 있는 list로 변경. feature와 label 각각 저장 \n",
    "original_all = []\n",
    "original_dataset = []\n",
    "original_label = [] \n",
    "\n",
    "for i, sample in enumerate(original_data) : \n",
    "    original_all.append(sample)\n",
    "    feature = np.array(sample[0])\n",
    "    original_dataset.append(feature)\n",
    "    original_label.append([sample[1], i]) # original index 포함시키기. \n",
    "\n",
    "unlabeled_dataset = original_dataset[:]\n",
    "unlabeled_dataset_label = original_label[:]\n",
    "labeled_dataset = [] \n",
    "labeled_dataset_label = []\n",
    "\n",
    "PATH = './weights/MNIST/'\n",
    "AE = torch.load(PATH + 'AE.pt')  \n",
    "AE.load_state_dict(torch.load(PATH + 'AE_state_dict.pt'))  \n",
    "\n",
    "CAE = torch.load(PATH + 'CAE.pt')  \n",
    "CAE.load_state_dict(torch.load(PATH + 'CAE_state_dict.pt'))  \n",
    "\n",
    "print(\"Successfully loaded AE & CAE\")\n",
    "\n",
    "use_cuda = True\n",
    "\n",
    "torch.manual_seed(23)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    # use_cuda가 true라면 kwargs를 다음과 같이 지정하기. \n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ae95ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6ecefb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def make_subgraph(sampling_label, original_dataset, radii, model):\n",
    "    x = [original_dataset[i[1]] for i in sampling_label] \n",
    "    dataset = original_dataset\n",
    "\n",
    "    if model is not None : \n",
    "        x = get_features(model, x, device)\n",
    "        dataset = get_features(model, dataset, device)\n",
    "\n",
    "    dist = pairwise_distances(x,dataset, metric='euclidean')\n",
    "\n",
    "    subgraph= dist.copy()\n",
    "    density_subgraph = []\n",
    "    for i, row in enumerate(dist) : \n",
    "        for j, distance in enumerate(row) : \n",
    "            if distance > radii or j == sampling_label[i][1] : subgraph[i,j] =int(0) \n",
    "            else : subgraph[i,j] = int(1) \n",
    "        \n",
    "        density_subgraph.append(sum(subgraph[i]))\n",
    "    \n",
    "\n",
    "    return np.array(subgraph), density_subgraph\n",
    "\n",
    "\n",
    "def get_features(model, dataset, device):\n",
    "    features = []\n",
    "\n",
    "    # 모델 evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 off 시키는 함수 \n",
    "    # model을 evaluate를 할 수 있도록 세팅시키는 것 \n",
    "    model.eval()\n",
    "    # torch.no_grad : autograd engine을 비활성화시켜 필요한 메모리를 줄여주고 연산속도를 증가시킴 \n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size = 64) # 여기서 Dataloader로 보내는 구나. \n",
    "\n",
    "    with torch.no_grad() : \n",
    "        for sample in dataloader:\n",
    "            sample = sample.clone().detach().to(device)\n",
    "        \n",
    "            output = model.get_codes(sample)\n",
    "            features = features + list(output.cpu().numpy()) \n",
    "    \n",
    "    return features\n",
    "\n",
    "def adjacency_subgraph(sample_dataset, sample_label, radii, model, M) :  \n",
    "    if len(sample_label) <= 1 : return \n",
    "    dataset = sample_dataset\n",
    "    num_subgraph = len(sample_label)\n",
    "    if model is not None : \n",
    "        dataset = get_features(model,dataset, device)\n",
    "    \n",
    "    dist = pairwise_distances(dataset, dataset, metric='euclidean')\n",
    "    adj_dist = dist.copy()\n",
    "    \n",
    "    for i, row in enumerate(dist) : \n",
    "        for j, distance in enumerate(row) : \n",
    "            if distance >= 2*radii[0] : adj_dist[i,j] = int(0)   \n",
    "            elif 2*radii[0] > distance and distance >= radii[0]  : \n",
    "                adj_dist[i,j] = int(1)\n",
    "            elif i==j : adj_dist[i,j] = int(0) # 자기자신은 제거\n",
    "            else : \n",
    "                print('Break')\n",
    "\n",
    "    classified_subgraph_index = []\n",
    "\n",
    "\n",
    "    for i in range(num_subgraph) : \n",
    "        i_sub_class = \"x\"\n",
    "        adj_index = np.where(adj_dist[ :,i] ==1)[0] \n",
    "        if len(adj_index)==0 : continue \n",
    "        i_sub_class = sample_label[i][0]\n",
    "\n",
    "        for j in adj_index :  \n",
    "            if i_sub_class != sample_label[j][0] : \n",
    "                i_sub_class = \"x\"\n",
    "                continue\n",
    "        if i_sub_class != \"x\" and len(adj_index) >= M : \n",
    "            classified_subgraph_index.append(i)\n",
    "    \n",
    "    if len(classified_subgraph_index) == 0 : classified_label = []\n",
    "    else : classified_label = [sample_label[i] for i in classified_subgraph_index]\n",
    "\n",
    "    return dist, adj_dist, classified_subgraph_index, classified_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "aa3eb2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_dataset = original_dataset[:]\n",
    "unlabeled_dataset_label = original_label[:]\n",
    "labeled_dataset = [] \n",
    "labeled_dataset_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "25c510ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max distance from cluster : 170.53\n",
      "Unlabeled pool size:  59999\n",
      "Labeled pool size:  1\n",
      "Max distance from cluster : 110.01\n",
      "Unlabeled pool size:  59998\n",
      "Labeled pool size:  2\n",
      "Max distance from cluster : 105.06\n",
      "Unlabeled pool size:  59997\n",
      "Labeled pool size:  3\n",
      "Max distance from cluster : 101.23\n",
      "Unlabeled pool size:  59996\n",
      "Labeled pool size:  4\n",
      "Max distance from cluster : 65.38\n",
      "Unlabeled pool size:  59995\n",
      "Labeled pool size:  5\n",
      "Max distance from cluster : 62.99\n",
      "Unlabeled pool size:  59994\n",
      "Labeled pool size:  6\n",
      "Max distance from cluster : 60.86\n",
      "Unlabeled pool size:  59993\n",
      "Labeled pool size:  7\n",
      "Max distance from cluster : 57.20\n",
      "Unlabeled pool size:  59992\n",
      "Labeled pool size:  8\n",
      "Max distance from cluster : 52.99\n",
      "Unlabeled pool size:  59991\n",
      "Labeled pool size:  9\n",
      "Max distance from cluster : 52.65\n",
      "Unlabeled pool size:  59990\n",
      "Labeled pool size:  10\n",
      "Max distance from cluster : 44.19\n",
      "Unlabeled pool size:  59989\n",
      "Labeled pool size:  11\n",
      "set()\n",
      "set()\n",
      "Max distance from cluster : 43.58\n",
      "Unlabeled pool size:  59988\n",
      "Labeled pool size:  12\n",
      "set()\n",
      "set()\n",
      "Max distance from cluster : 41.03\n",
      "Unlabeled pool size:  59987\n",
      "Labeled pool size:  13\n",
      "set()\n",
      "set()\n",
      "Max distance from cluster : 38.52\n",
      "Unlabeled pool size:  59986\n",
      "Labeled pool size:  14\n",
      "set()\n",
      "set()\n",
      "Max distance from cluster : 35.20\n",
      "Unlabeled pool size:  59985\n",
      "Labeled pool size:  15\n",
      "set()\n",
      "Max distance from cluster : 34.86\n",
      "Unlabeled pool size:  59984\n",
      "Labeled pool size:  16\n",
      "set()\n",
      "Max distance from cluster : 33.95\n",
      "Unlabeled pool size:  59983\n",
      "Labeled pool size:  17\n",
      "set()\n",
      "Max distance from cluster : 33.29\n",
      "Unlabeled pool size:  59982\n",
      "Labeled pool size:  18\n",
      "set()\n",
      "set()\n",
      "Max distance from cluster : 32.86\n",
      "Unlabeled pool size:  59981\n",
      "Labeled pool size:  19\n",
      "set()\n",
      "set()\n",
      "Max distance from cluster : 30.66\n",
      "Unlabeled pool size:  59980\n",
      "Labeled pool size:  20\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "from active_learn import active_sample\n",
    "from active_learn import first_classification, check_performance\n",
    "ratio = 0.99\n",
    "\n",
    "log_score = [] \n",
    "classified = [] \n",
    "\n",
    "if len(unlabeled_dataset) < sample_size:\n",
    "    sample_size = len(unlabeled_dataset)\n",
    "    \n",
    "\n",
    "for i in range(sample_size) : \n",
    "    sample_data, sample_index,radius  = active_sample(unlabeled_dataset, labeled_dataset, 1, model=CAE, device=device)\n",
    "\n",
    "    if i == 0 : \n",
    "        sample_dataset = [unlabeled_dataset[i] for i in sample_index]\n",
    "        sample_label = [unlabeled_dataset_label[i] for i in sample_index]\n",
    "    \n",
    "    else : \n",
    "        new_sample_dataset = [unlabeled_dataset[i] for i in sample_index]\n",
    "        new_sample_label = [unlabeled_dataset_label[i] for i in sample_index]\n",
    "        sample_dataset = np.concatenate((sample_dataset, new_sample_dataset), axis=0)\n",
    "        sample_label = np.concatenate((sample_label, new_sample_label), axis =0)\n",
    "\n",
    "\n",
    "    # Sampling에 따른 Dataset 수정 \n",
    "    if len(labeled_dataset_label) == 0 :  \n",
    "        labeled_dataset = sample_data[:]\n",
    "        labeled_dataset_label = sample_label[:]\n",
    "    else : \n",
    "        labeled_dataset = np.concatenate((labeled_dataset,new_sample_dataset),axis=0)\n",
    "        labeled_dataset_label = np.concatenate((labeled_dataset_label, new_sample_label), axis =0)\n",
    "\n",
    "    for j in sample_index[::-1] : \n",
    "        del unlabeled_dataset[j]\n",
    "        del unlabeled_dataset_label[j]\n",
    "\n",
    "    print(\"Unlabeled pool size: \",len(unlabeled_dataset))\n",
    "    print(\"Labeled pool size: \",len(labeled_dataset))\n",
    "\n",
    "    if i == 0 : continue\n",
    "    subgraph, density_subgraph = make_subgraph(sample_label, original_dataset, radius, CAE)\n",
    "    # 마지막 숫자를 통해서 접하는 subgraph의 수 정할 수 있음. \n",
    "    dist_class, adj_dist, classified_subgraph_index, pseudo_class_label = adjacency_subgraph(sample_dataset, sample_label, radius, CAE, 0)\n",
    "    if len(classified_subgraph_index) == 0 : continue \n",
    "    f_classification = first_classification(classified_subgraph_index, pseudo_class_label, subgraph, density_subgraph, ratio)\n",
    "    num_classification, score, dic_score = check_performance(f_classification,original_label)\n",
    "    log_score += (i, num_classification, score)\n",
    "    test_set = set([]) \n",
    "    for j in classified_subgraph_index : \n",
    "        test_set.union(set(np.where(subgraph[ j,:] ==1)[0]))\n",
    "        print(test_set)\n",
    "    classified += list(test_set)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4272aa6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e00abfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set(classified[0]).union(set(classified[2]))\n",
    "# 이전에 중복인가? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7a818dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classified[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
