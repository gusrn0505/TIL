{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7112a0a3",
   "metadata": {},
   "source": [
    "### <$Q_\\pi$를 추정하기 위한 비활성 MC 예측(정책평가) >\n",
    "출처 : 단단한 강화학습 챕터 5 수도코드 \n",
    "\n",
    "입력 : 임의의 목표 정책 $\\pi$ \n",
    "\n",
    "모든 $s \\in S, a \\in A(s)$를 초기화 : \n",
    "- $Q(s,a) \\in R$ (임의로 설정) \n",
    "- C(s,a) <- 0 \n",
    "\n",
    "(각 에피소드에 대해) 무한 루프 : \n",
    "- b <- 정책 $\\pi$가 보증된 임의의 정책 \n",
    "- 정책 b를 따르는 에피소드를 생성 : $S_0, A_0, R_1, ... S_{T-1}, A_{T-1}, R_T$\n",
    "- G <- 0 \n",
    "- W <- 1 \n",
    "- 에피소드의 각 단계에 대한 루프 ,$t= T-1, T-2, ... 0$ \n",
    "\n",
    "> G <- $\\gamma G + R_{t-1}$\n",
    "\n",
    "> $C(S_t, A_t) <- C(S_t, A_t) + W$ \n",
    "\n",
    "> $Q(S_t, A_t) <- Q(S_t, A_t) + \\frac {W}{C(S_t, A_t)} [G- Q(S_t, A_T)]$\n",
    "\n",
    "> W <- $W \\frac{\\pi(A_t|S_t)} {b(A_t|S_t)}$\n",
    "\n",
    "> W = 0 이면 루프를 종료 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c539030",
   "metadata": {},
   "source": [
    "**구현해야 하는 것** \n",
    "- Q(s,a) : 기존 함수 유지 \n",
    "- C(s,a) : dict 또는 이중 리스트로 구현 \n",
    "\n",
    "- b <- 정책 $\\pi$ 가 보증된 임의의 정책 : How? \n",
    "\n",
    "- W : int \n",
    "\n",
    "- $\\pi(a,s)$ : 외부 함수로 구현해야함. \n",
    "\n",
    "\n",
    "**필요한 것** \n",
    "- 외부함수 pi , reward func \n",
    "\n",
    "**함수/데이터의 형태** \n",
    "- class epsilon_soft_MC : \n",
    "\n",
    "> def __init__(self, S, A, reward_func, pi, epsilon=0.001, gamma = 0.9, num_episode = 10, len_episode = 20) \n",
    "\n",
    "> def choice_sample(self, list, prob) : 기존 함수 유지 \n",
    "\n",
    "> def make_episode(self, start_s, T, pi) : 기존 함수 유지  \n",
    "\n",
    "> def update_returns(self) : 비활성 MC에 따라서 구현 \n",
    "\n",
    "**전제**\n",
    "- 상태 s에서 행동 a를 했을 때 결정론적으로 s' 상태로 변화한다. \n",
    "\n",
    "<외부 함수> \n",
    "- def reward_func(s',a,s) : \n",
    "\n",
    "- def pi(s,a) : \n",
    "\n",
    "\n",
    "<고민점>\n",
    "- 정책 $\\pi$가 보증된 임의의 정책을 어떻게 표현할 것인가? \n",
    "\n",
    "> 보증한다는 것은 $\\pi$ 가 가지는 모든 경우의 수를 포함하고 있어야 한다는 것. 일반적인 경우로 epsilon-soft로 정의하자. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23afc03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 용 임시 데이터 \n",
    "S = list(range(100)) \n",
    "A = list(range(-5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f83cf2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "056ca72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외부용 함수 reward_func 간략 구현 (이전 예시 활용)\n",
    "def reward_func(next_s, a, s) : \n",
    "    # next_s 와 s의 차이가 짝수이면 +1, 홀수면 -1 \n",
    "    # 단, a의 크기에 반비례함. \n",
    "    if abs(next_s - s) %2 == 0 : reward = 1 \n",
    "    else : reward = -1\n",
    "    \n",
    "    if a == 0 : \n",
    "        return 0 \n",
    "    else : \n",
    "        return reward / a # 즉, a가 양수이며 짝수이며, 가능한 작을 때 (=2) 일 때 최대의 보상이 주어지도록 설정 \n",
    "    \n",
    "#최대값이 2개 이상인 경우, 임의로 1개의 최대값을 만들어낸 행동 a를 산출 \n",
    "def choose_random_max(lst) :\n",
    "    max_arg = np.where(np.array(lst) >= max(lst))\n",
    "    return random.choice(list(max_arg)[0]) #max_arg가 array 형태로 안에 있는 list를 꺼내기 위해 [0] 사용 \n",
    "    \n",
    "def pi(s,a) : #s 상황에서 a를 선택할 확률. 전체 합은 1이여야 한다. \n",
    "    # 확률은 모두 동일하게 설정 \n",
    "    \n",
    "    return 1/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8fbde6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입실론 소프트 활성 MC 제어 참고 \n",
    "\n",
    "class off_policy_MC :  \n",
    "    def __init__(self, S, A, pi, reward_func, epsilon = 0.001, gamma = 0.9, num_episode = 10, len_episode = 20) : \n",
    "        self.S = S \n",
    "        self.A = A \n",
    "        self.epsilon = epsilon \n",
    "        self.reward_func = reward_func\n",
    "        self.pi = pi\n",
    "        self.b, self.C = self.initiate_b()\n",
    "        self.gamma = 0.9 \n",
    "        self.num_episode = num_episode\n",
    "        self.T = len_episode\n",
    "        \n",
    "        self.set_episode = [] # 추후에 어떤 episode들이 있었나 확인용 \n",
    "        \n",
    "        self.Q, self.return_lst, self.s_prob = self.initiate() \n",
    "    \n",
    "    \n",
    "    def initiate_b(self) : \n",
    "        # pi를 보증하는 b 정책 초기화하기. 예측이기 때문에 b 정책이 바뀔 일은 없음.\n",
    "        # C(s,a) 도 같이 초기화 \n",
    "        pi_dic = defaultdict(float)\n",
    "        c_dic = defaultdict(float)\n",
    "        for s in self.S :\n",
    "            for a in self.A :  \n",
    "                pi_dic[(s,a)] = 1/len(self.A)\n",
    "                c_dic[(s,a)] = 0\n",
    "        return pi_dic, c_dic\n",
    "    \n",
    "    def initiate(self) :  \n",
    "        Q = defaultdict(float)\n",
    "        s_prob = defaultdict(list)\n",
    "        for s in self.S : \n",
    "            s_prob[s] = [0]*len(self.A)\n",
    "            for index, a in enumerate(self.A) : \n",
    "                Q[(s,a)] = 0 \n",
    "                s_prob[s][index] = self.pi(s,a)\n",
    "\n",
    "        return_lst = [[0]*len(self.A)]*len(self.S) # return_lst의 데이터 형식 변경\n",
    "        return Q, return_lst, s_prob\n",
    "    \n",
    "    def choice_sample(self, s) : \n",
    "        b_a_list = [] \n",
    "        for _ in self.A :\n",
    "            b_a_list.append(self.b[(s,_)])\n",
    "            \n",
    "        a= random.choices(self.A, weights = b_a_lsit[s]) # s는 index와 동일해야 한다. \n",
    "        \n",
    "        # 수정. a[0] +s 가 s의 범위 안에 들어오도록 수정 \n",
    "        return a[0], min(max(s+a[0],0), 99)  \n",
    "    \n",
    "    def make_episode(self, start_s, T) :\n",
    "        s = start_s\n",
    "        episode = {\"S\" : [], \"A\" : [], \"R\" : []}\n",
    "        episode[\"R\"].append(0) # R_0 값 부여 \n",
    "        for _ in range(T) : \n",
    "            episode[\"S\"].append(s)\n",
    "            a, next_s = self.choice_sample(s)\n",
    "            r = self.reward_func(next_s, a, s)\n",
    "            episode[\"A\"].append(a)\n",
    "            episode[\"R\"].append(r)\n",
    "            s = next_s\n",
    "        return episode \n",
    " \n",
    "    def update_returns(self) : # returns 가 사라짐에 따라 대거 수정  \n",
    "        for _ in range(self.num_episode) :\n",
    "            start_s = random.choice(self.S)\n",
    "\n",
    "            episode = self.make_episode(start_s, self.T)\n",
    "            G = 0 \n",
    "            W = 1 # W값 추가 \n",
    "            \n",
    "            for t in reversed(range(self.T)) : \n",
    "                G = self.gamma *G + episode[\"R\"][t]\n",
    "                s, a  = episode[\"S\"][t], episode[\"A\"][t]\n",
    "                self.C[(s,a)] = self.C[(s,a)] + W\n",
    "                self.Q[(s,a)] = self.Q[(s,a)] + W/self.C[(s,a)] * (G - self.Q[(s,a)])\n",
    "                W = W*self.pi(s,a)/self.b[(s,a)] \n",
    "                if W == 0 : break        \n",
    "        \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a5d30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "reward_func() missing 1 required positional argument: 's'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# update_returns 구현 테스트 \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(pi(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43moff_policy_MC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreward_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36moff_policy_MC.__init__\u001b[1;34m(self, S, A, pi, reward_func, epsilon, gamma, num_episode, len_episode)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m=\u001b[39m len_episode\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_episode \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# 추후에 어떤 episode들이 있었나 확인용 \u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_lst, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36moff_policy_MC.initiate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA) : \n\u001b[0;32m     37\u001b[0m         Q[(s,a)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[1;32m---> 38\u001b[0m         s_prob[s][index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m return_lst \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA)]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS) \u001b[38;5;66;03m# return_lst의 데이터 형식 변경\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Q, return_lst, s_prob\n",
      "\u001b[1;31mTypeError\u001b[0m: reward_func() missing 1 required positional argument: 's'"
     ]
    }
   ],
   "source": [
    "# update_returns 구현 테스트 \n",
    "print(pi(2,3))\n",
    "test = off_policy_MC(S,A,reward_func, pi) \n",
    "\n",
    "\n",
    "# 오류가 왜 생기는 거지? reward_func 는 아래 식에서 관련이 없는데? \n",
    "\n",
    "#print(test.C)\n",
    "#print(test.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50c13b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ec079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c25b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45650c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ee474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a0e906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cbb2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb5112a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6305b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711fc19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f364b869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4125387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514817c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11efad2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
