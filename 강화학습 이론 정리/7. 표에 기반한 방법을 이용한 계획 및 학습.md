### Chapter 7 : 표에 기반한 방법을 이용한 계획 및 학습

출처 : 단단한 강화학습 - Part 1 표 형태의 해법 속 챕터 7



##### 서론

- **모델에 기반한 강화학습**과 **모델 없는 강화학습**에 대한 종합적인 시각을 개발할 것이다. 
  
  - **공통점** 
    
    - 중요 핵심 목표로 가치 함수를 계산한다. 
    
    - 미래 사건을 내다보고, 보강된(Backed-up) 가치를 계산하고, 그것을 근사적 가치 함수를 위한 갱신 목표로 사용한다. 
  
  - **차이점** 
    
    - 모델에 기반한 방법에서는 계획(Planning)이 중요한 역할을 한다.
    
    - 모델 없는 방법에서는 학습(Learning)이 중요한 역할ㅇ르 한다.  

---

##### 모델과 계획

- 용어 
  
  > 모델(Model) : 환경이 행동에 어떻게 반응할 것인지를 예측하기 위해 학습자가 사용할 수 있는 모든 것. 경험을 모사하거나 시뮬레이션하기 위해 사용될 수 있다.
  > 
  > 분포 모델(Distribution model) : 모든 가능성을 제공하고 각 가능성에 해당하는 확률을 제공하는 모델 
  > 
  > 표본 모델(sample model) : 모든 가능성 중 확률에 따라 추출된 하나의 가능성만을 제공하는 모델 
  > 
  > 계획(Planning) : 모델링된 환경과의 상호작용을 위해 모델을 입력으로 하여 정책을 만들어 내거나 향상시키는 모든 계산 과정 



- **분포 모델 / 표본 모델의 차이점**
  
  - 분포 모델 
    
    - 분포 모델은 항상 표본을 추출할 수 있다는 점에서 표본 모델보다 강력하다. 
    
    - 시작 상태와 정책이 있다면 모든 가능한 에피소드와 확률을 생성가능하다.
    
    - 시뮬레이션된 경험(simulated experience)를 만들기 위해 모델을 사용한다.
  
  - 표본 모델
    
    - 많은 실제 적용 사례에서 표본 모델이 분포 모델보다 구하기가 훨씬 용이하다. 
    
    - 시작 상태와 정책이 있다면 하나의 완전한 에피소드를 도출할 수 있다. 
    
    - 환경을 시뮬레이션 하기 위해 모델을 사용한다. 



- **계획의 두가지 접근법** - 저자의 계획 정의에 기반하여 
  
  - **상태 공간 계획(State-space planning)** : 주로 최적 정책이나 목표를 향한 최적 경로를 찾기 위해 상태 공간을 탐색 
  
  - **계획 공간 계획(Plan-space planning)** : 계획 공간에 대한 탐색을 통해 계획이 수행된다. (이 책에선 다루지 않을 예정)



- 모든 상태 공간 계획 방법은 하나의 공통된 구조를 갖는다는 관점을 고수할 것
  
  - (1) 모든 상태 공간 계획은 가치 함수 계산을 정책을 향상시키기 위한 핵심적인 중간 단계를 포함한다.  
  
  - (2) 시뮬레이션된 경험에 적용된 갱신 또는 보강 과정에 의해 가치 함수를 계산한다. 
    
    > ![](picture/7-1.png)



- 학습과 계획 방법의 핵심은 **보강 갱신 과정에 의한 가치 함수의 추정**이다. 
  
  - 학습과 계획 사이에 분명 차이가 있다. 
    
    - 계획은 <u>모델로부터 생성된 시뮬레이션된 경험을 사용</u>한다. 
    
    - 학습 방법은 <u>환경으로부터 생성된 진짜 경험을 이용</u>한다 
  
  - 그럼에도 공통의 구조를 갖는다는 것은 <mark>많은 개념과 알고리즘이 학습과 계획 사이에서 전환될 수 있음을 의미</mark>한다.
    
    - 특히 많은 경우, 학습 알고리즘은 계획 방법의 핵심 갱신 단계를 대체할 수 있다.
    
    - **학습 방법**은 진짜 경험 뿐이 아니라 <u>시뮬레이션된 경험에도 적용</u>될 수 있다. 
      
      - ex)- 무작위 표본 단일 단계 표 기반 Q 계획 
        
        ![](picture/7-2.png)

--- 

##### 다이나 : 계획, 행동, 학습의 통합

- 계획이 환경과 상호작용하며 온라인으로 수행될 때는 수많은 이슈가 발생한다. 
  
  - 상호작용으로부터 얻은 새로운 정보는 모델을 변경하는 등 계획과 상호작용한다. 
  
  - 결정을 내리는 것과 모델을 학습하는 것 모두 계산이 많이 필요하다면, 이용 가능한 계산 능력을 두 작업으로 나누어야 할 수 있다. 



- 위와 같은 이슈를 논의하기 위해 <mark>다이나 Q(Dyna-Q)</mark>를 제시한다. 
  
  ![](picture/7-3.png)
  
  - 실제 경험은 **2가지 역할**을 한다. 
    
    - **(1) 모델학습**(model-learning) : 모델이 실제 환경을 더 정확하게 모사시킨다.
      
      - 간접적 강화학습이라 불리며, 이 과정은 계획에 포함된다. 
    
    - **(2) 직접적 강화학습** : 가치 함수와 정책을 직접 향상시킨다.



- **간접적 방법과 직접적 방법 모두 장단점이 있다.**
  
  - 간접적 방법은 효율적으로 제한된 경험을 이용하여 더 좋은 정책을 획득한다. 
  
  - 직접적 방법은 훨씬 간단하고 모델의 설계에 포함된 편차에 영향을 받지 않는다. 



- 다이나 학습자의 전체 구조는 다음과 같다. 
  
  ![](picture/7-4.png)
  
  > 중간 열은 학습자와 환경 사이의 기본적인 상호작용과 실제 경험의 궤적을 의미
  > 
  > 오른쪽 열은 모델 기반 과정을 의미한다.
  > 
  > > 탐색 제어(Search control) : 모델로부터 시뮬레이션된 경험 도출 
  
  - 일반적으로 직접적/간접적 방법 모두 동일한 강화학습 방법이 사용된다.
    
    - 즉, 강화학습은 학습과 계획 둘 다를 위한 '마지막 공통 경로'다. 
    
    - 학습과 계획은 각자가 사용하는 경험만 다를 뿐 거의 동일한 구조를 공유한다는 점에서 깊게 결합되어 있다. 
  
  - 다이나 Q에서 행동, 모델학습, 직접적인 강화학습 과정은 거의 계산을 필요로 하지 않기 때문에, <u>남는 시간은 많은 계산을 필요로 하는 계획에 사용</u>될 수 있다. 
    
    - 계획은 변경된 모델에 맞춰 가치 함수의 값을 갱신하는 것으로 계산이 많다.
    
    - 또한 실제 행동이 다수의 상태 가치를 변경한다면 다수의 반복 계산이 필요하다.



- 다이나 Q를 수행할 때마다 계획 단계의 반복 횟수(n)의 크기에 따라 성능의 차이가 있다.
  
  ![](picture/7-5.png)
  
  > 문제는 S로부터 G까지 가능한 빨리 가는 것이다. 
  > 
  > n=0 인 학습자는 오직 직접적인 강화학습만을 이용한다.
  
  - 최초의 에피소드 이후에 모든 n 값에 대해 성능이 향상되었지만, <u>n 값이 클수록 훨씬 더 빨리 향상되었다</u>. 
    
    ![](picture/7-6.png)
    
    > → 는 각 에피소드에서 탐욕적 행동을 나타낸다. 
    > 
    > → 가 없다면 해당 상태의 모든 행동 가치가 동일하다. 
    
    - 즉, <u>계획을 많이 실행할 수록, 많은 상태 속에서 탐욕적인 정책을 산출</u>할 수 있다.
      
      - 왼쪽의 경우, 경우의 수가 매우 많이 나온다. 
      
      - 오른쪽의 경우, 초반의 상/하 방향만 정해지면 그 이후에는 확정적이다. 
    
    

--- 

##### 모델이 틀렸을 때

- 모델이 부정확할 때 계획 과정은 <u>준최적(suboptimal) 정책을 계산</u>할 가능성이 높다. 
  
  - 특히, 환경이 확률론적이고 오직 제한된 개수의 표본만이 측정되었거나, 
  
  - 모델이 불완전하게 일반화된 함수 근사를 사용하여 학습되었거나 
  
  - 환경이 변했고 새로운 환경의 행동이 아직 감지 안되는 등 
  
  - => 여러가지 이유로 모델은 부정확할 수 있다. 



- **변화하는 환경에 대해서 최적의 방법을 찾느냐는 탐험과 활용 사이의 갈등의 문제이다.**
  
  ![](picture/7-7.png)
  
  > Dyna-Q+ : 각 상태-행동 쌍을 추적하여 환경과의 실제 상호작용 속에서 마지막 상태-행동 쌍에서 얼마나 많은 시간 단계가 경과했는지를 확인한다. 
  > 
  > - 경과한 시간이 길수록 모델이 부정확할 가능성이 더 커진다 가정할 때, 오랫동안 시도되지 않은 행동을 테스트하기 위해 보상($ r + k \sqrt \tau$)을 제공한다. 
  >   
  >   > r : 보상 
  >   > 
  >   > k : 임의의 작은 수
  >   > 
  >   > $\tau$ : 시도되지 않은 기간  
  
  
  
  - 환경이 보다 유리하게 변한다고 해도, 모델이 변화를 감지하지 못한다면 준최적 정책을 반복한다. 
    
    ![](picture/7-8.png)

--- 

##### 우선순위가 있는 일괄처리

- 경험했던 상태-행동 쌍에서 균등하게 최초의 한 쌍을 선택하는 것은 보통 최선이 아니다. 
  
  - 목표와 연결되지 않은 곳(가치가 0)에서 갱신을 해도 변화가 없다. 
  
  - 계획이 진행됨에 따라 유용한 갱신의 영역은 커지지만, 계획 자체는 계획이 가장 잘 수행되는 곳에 초점을 맞추었을 때보다 여전히 더 비효율적이다. 



- 따라서 <u>목표 상태/가치 함수에 변화가 생긴 상태로부터 역방향으로 진행</u>한다(**역행초점**).
  
  - 한 상태의 가치 추정값이 변화했다고 했을 때, 그 상태를 일으킨 행동과 그 전의 상태와 행동 등 역순으로  갱신해나갈 수 있다.
    
    

- 이때 시급성에 따라 갱신의 우선순위를 정하고, 그 우선순위에 따라 갱신을 수행한다
  
  > 이는 **우선순위가 있는 일괄처리**의 기본 개념이다.
  
  - 확률론적 환경에서 <u>전이 확률 추정값의 다양성</u>이 <u>변화의 크기</u> 및 상태-행동 쌍이 갱신되어야 하는 <u>시급성에 있어서 다양성을 야기</u>한다.
  
  - 상태-행동 쌍의 가치 추정값이 갱신할 떄, 값이 변화하는 크기에 따라 상태-행동 쌍의 나열 순서를 정한다. 
    
    - 가장 우선순위가 높은 상태-행동 쌍이 갱신된 다음, 그 쌍 이전에 존재했던 쌍들 각각에 미치는 효과가 계산된다. 
    
    - 그 효과가 일정 기준값보다 크면, 새로운 우선순위를 가지고 나열에 삽입된다. 
    
    - 이때 나열에 삽입된 것보다 낮은 우선순위를 갖는 쌍들은 제거한다. 
  
  - <mark>=> 문제에 따라 정책 수렴 속도를 5~10배까지도 향상시킨다.</mark>



- 우선순위가 있는 일괄처리는 확률론적인 환경으로 확장하는 것은 어렵지 않다. 
  
  - 모든 가능한 다음 상태와 그들의 발생 확률을 고려하여 <u>기댓값 갱신</u>을 하면된다.



- <mark>**모든 종류의 상태 공간 계획은 갱신의 유형만 다를 뿐 가치 갱신의 나열로 인식된다.**</mark>
  
  - 갱신의 유형은 **기댓값 갱신, 표본 갱신, 갱신의 크기, 수행되는 순서** 등 이 있다.
  
  - 갱신 방법에 따라 장단점이 갈린다. 
    
    > ex)- 기댓값 갱신 : 발생 확률이 낮은 전이에 많은 계산량을 낭비할 수 있다.  
  
  - 이 절에서는 역행 초점을 강조했으나, 하나의 전략일 뿐이다. 
    
    > ex)- 순행 초점 : 현재 정책 하에서 자주 마주치는 상태로부터 얼마나 쉽게 그 상태에 도달할 수 있는지에 따라 상태에 초점을 둠 



--- 

##### 기댓값 갱신 대 표본 갱신

- 단일 단계 갱신에 초점을 맞췄을 때, 갱신은 세 개의 2차원적 구분자에 따라 구분된다. 
  
  ![](picture/7-9.png)
  
  > 지금까지 나온 방법들은 총 8가지 구분 중 7가지에 해당된다. 
  > 
  > 나머지 1개는 유의미하지 않아 다루지 않는다.
  
  - (1) 상태 가치 / 행동 가치 중 어떤 것을 갱신하는가? 
  
  - (2) 최적 정책 / 임의 정책 중 무엇의 가치를 추정하는가? 
  
  - (3) 기댓값 갱신 / 표본 갱신 중 무엇을 사용할 것인가? 



- **기댓값 갱신 vs 표본 갱신** 
  
  - 가용 여부 : 기댓값 갱신은 분포 모델이 있어야만 가능하다. 표본 갱신은 환경과 표본 모델로 부터 나오는 표본 전이만 있어도 된다. 
  
  - 추정값 편차 : 기댓값 갱신은 없으나, 표본 갱신은 표폰 편차가 있다.
  
  - 계산량 : 기댓값 갱신은 대략 표본 갱신에 필요한 계산량의 <u>분기계수(b)</u>배에 해당하는 계산량이 필요하다. 
    
    > 분기계수(b) : $\hat p(s'|s,a) >0 $ 을 만족하는 가능한 다음 상태 s'의 개수 
    
    > 기댓값 갱신 : $Q(s,a) ← \sum_{s', r} \hat p(s',r|s,a)[r + \gamma max_{a'}Q(s',a')]$
    > 
    > > $\hat p(s',r|s,a)$ : 추정된 동역학 형태의 모델 
    > 
    > 표본 갱신 : $Q(s,a) ← Q(s,a) + \alpha [R+ \gamma max_{a'}Q(S',a') - Q(s,a)]$
  
  - => <mark>기댓값 갱신을 계산할 시간이 충분하다면 기댓값 갱신이 좋다.</mark> <mark>하지만 그렇지 않다면 표본 갱신을 더 선호한다. </mark>
    
    - 규모가 큰 문제에서는 항상 후자의 상황에 놓이게 된다. 



- **큰 확률론적 분기를 갖는 문제와 정확한 해를 도출해야 하는 상태가 너무 문제에서는 표본 갱신의 성능이 좋다.**
  
  ![](picture/7-10.png)
  
  > b개의 모든 이후 상태의 발생 확률이 동일 / 초기 추정값의 오차가 1으로 가정
  
  - 기댓값 갱신이 되고 나면 추정 오차는 0으로 감소한다. 
  
  - 표본 갱신의 초정 오차는 $\sqrt \frac{b-1}{bt}$ 에 따라 감소한다. 
    
    > t : 수행된 표본 갱신의 수 
    
    - 이 때 <u>적당히 큰 b에 대해 전체 b번의 갱신</u> 중 **초기의 아주 적은 수의 갱신만으로 오차가 극적으로 감소**한다.
    
    - <mark>소요되는 시간이 하나의 상태-행동 쌍이 기댓값 갱신을 수행하는 시간으로도</mark>, <mark>기댓값 갱신 효과의 몇 퍼센트 이내로 가치를 극적으로 향상</mark>시킬 수 있다. 
    
    - +표본 갱신 간 모델이 보완 된다는 점을 고려할 때 더욱 성과가 좋을 것이다. 



--- 

##### 궤적 표본 추출

- 갱신을 분산시키는 두 가지 방법 
  
  - (1) 철저한(exhausitive) 일괄처리 : 모든 상태를 고려 및 갱신 
  
  - **(2) 궤적 표본 추출** : 활성 정책 분포에 따라 관측된 분포에 따라 갱신을 분산



- 활성 정책 vs 균등 분포 상 궤적 표본 추출의 특징 
  
  ![](C:\Users\user\AppData\Roaming\marktext\images\2022-08-23-11-01-53-image.png)
  
  > 각 전이에 대해 기대되는 보상은 평균이 0이고 분산이 1인 정규 분포로 선택됨
  > 
  > 모든 전이에 대해 각 전이가 종단 상태로 갈 확률은 0.1이다. 
  
  - 단기적으로 활성 정책 분포는 최적의 행동에 집중하여 성과가 좋으나, 장기적으로는 보다 다양한 경우를 고려한 균등 분포가 더 결과가 좋아질 수 있다.
  
  - 활성 정책은 <u>분기계수(b)의 크기에 반비례</u>, <u>전체 상태의 수의 크기에 비례</u>하여 **더 좋은 성과를 내는 기간이 길어진다**. 
    
    - 즉, <mark>규모가 큰 문제에서는 활성 정책을 기반으로 갱신하는 것이 선호된다.</mark> 


