## Chapter 2 : 다중 선택

출처 : 단단한 강화학습 - Part 1 표 형태의 해법 속 챕터 2  

#### 

#### 서론

- 강화학습은 **지침**이 아닌, 행동을 **평가**하는 훈련 정보를 사용한다. 
  
  > 지침(Instruct) : 지금의 행동과는 상관없이 취해야 할 올바른 행동을 알려준다. 
  > 
  > 평가(Evalutate) : 지금의 행동이 얼마나 좋은지만 이야기한다. 

- 평가적인 피드백은 <u>취해진 행동에 전적으로 의존</u>하는 반면, 지침적인 피드백은 취해진 행동과는 무관하게 이루어진다. 

- 전제 
  
  - 이번 장에서는 문제의 복잡성을 피하기 위해, 단순한 환경(**비연합** 구조) 에서 강화학습의 평가적인 측면을 다룬다. 
    
    > 비연합(nonassociative) : 하나의 상황에 대해서만 행동을 학습한다. 즉, 이전, 이후 상황과 연관되어 있지 않다. 
  
  - 이 장의 마지막에서 선택 문제가 연합적으로 바뀔 경우에 대해 다루겠다. 

-----

-----

### 다중 선택 문제(k-armed bandit problem)

1. <mark>k개의 서로 다른 옵션이나 행동 중 하나를 반복적으로 선택해야 할 때, </mark>

2. <mark>시간 간격동안 주어지는 보상의 총량에 대한 기댓값을 최대화하자! </mark>

###### 용어 정리

> 가치(value) : k개의 행동 각각에 기대할 수 있는 평균 보상값
> 
> $A_t$ : 시간 단계 t에서 선택되는 행동
> 
> $R_t$ : $A_t$에 따른 보상 
> 
> $q_*(a) =$ $E[R_t |A_t =a]$ : 행동 a가 선택되었을 때 얻는 보상의 기댓값
> 
> $Q_t(a) = $ 시간 단계 t에서 추정한 행동 a의 가치
> 
> 탐욕적(Greedy) 행동 : 각 시간 단계마다 최대의 가치를 갖는 행동
> 
> 활용(Exploiting) : 지금까지의 정보를 기반으로 탐욕적 행동을 하는 것 
> 
> 탐험(Exploring) : 탐욕적 행동이 아닌 다른 행동을 선택함으로써 비탐욕적 행동의 추정 가치를 상승시키는 것

###### 전제 및 바탕

- 행동의 가치를 추정할 수 있더라도 확실히 알지는 못한다.
  
  - 큰 수의 법칙에 따라 t가 증가함에 따라 추정값 $Q_t(a)$는 기댓값 $q_*(a)$ 에 수렴한다.  

- 하나의 행동을 선택할 때 활용과 탐험을 동시에 할 수 없다. 따라서 활용과 탐험의 적절히 채택하는 것이 중요하며, 이를 종종 활용과 탐험의 **갈등(conflict)** 로 지칭한다. 
  
  > 활용 : 지금 당장의 보상을 증가하나, 장기적인 관점에서 최적의 선택이 아닐 수 있음
  
  > 탐험 : 지금 보상이 좋지 않지만, 장기적으로 보다 최적의 선택을 찾는데 도움이 된다. 
  
  - 활용과 탐험 사이의 균형을 찾는 정교한 방법이 많이 있다. 하지만, 대부분 사전지식이 고정(정적)되어 있다 전제 아래에 가능하나, 이는 현실의 상황과 괴리가 있다. 

##### 행동 가치 방법

- 의미 : 행동의 가치를 추정하고 추정값으로부터 행동을 선택하도록 결정하는 방법 
  
  > 표본평균 방법 : $Q_t(a) = \frac{시간 t 이전에 취해지는 행동 a에 대한 보상의 합}{시각 t 이전에 행동 a를 취하는 횟수} = \frac{\sum^{t-1}_{i=1} R_i * 1|_{A_i =a}} {\sum^{t-1}_{i=1}  1|_{A_i =a}}$
  > 
  > $1|_{A_i=a}$ : $A_i = a$ 인 경우 1의 값을, 그 외에는 0의 값을 가짐. 

- **행동 선택 규칙** 
  
  - 탐욕적 방법 : 추정 가치가 최대인 행동 중 하나를 선택한다. 
    
    > $A_t = argmax_aQ_t(a)$
  
  - 입실론 탐욕적 방법 : 상대적 빈도수 $\epsilon$ 을 작은 값으로 유지하면서, 모든 행동을 대상으로 랜덤한 선택을 한다. 
    
    - 장점 : 단계의 개수가 무한으로 커지면 $Q_t(a)가 q_*(a)$로 수렴한다. 
    
    - 

##### 성능 검증 방법 - 10중 선택 테스트(10-armed testbed)

- 열 번의 선택을 하는 다중 선택 문제 2000개를 무작위로 생성한다. 

- 각 다중 선택 문제에 대한 행동 가치 $q_t(a)$ 는 평균이 0이고 분산이 1인 정규 분포로 부터 선택된다.  ![](./picture/1-1.png)
  
  - 시간 단계 t에서 행동 $A_t$를 선택할 때, 실제 보상값 $R_t$ 은 평균이 $q_*(A_t)$ 이고 분산이 1인 정규 분포로부터 선택된다. 

- ![](./picture/1-2.png)
  
  - 탐욕적 방법
    
    - 시작 직후에는 빠르게 성능 향상이 되나, 준최적(suboptimal) 행동을 수행하는 상황에 걸리기 때문에 장기적으로는 다른 방법보다 낮은 성능을 보인다.  
  
  - 입실론 탐욕적 방법 
    
    - 최초에는 느릴지라도, 계속된 탐험을 통해 최적 행동을 식별할 확률을 증가시켰기 때문에 결국에는 더 좋은 성능을 보인다. 
    
    - $\epsilon$ 을 크게 잡으면 최적의 선택지를 빨리 알지만 지속적으로 $\epsilon$ 만큼 무작위 선택하여 손실이 있다.
    
    - 반대로 $\epsilon$을 작게 잡으면 최적의 선택지를 늦게 알지만, 무작위 선택으로 인한 손실이 적다.
  
  - => **보상의 분산이 작은 경우, 탐험의 중요성이 낮아져 탐욕적 방법이 더 좋다. 하지만 분산이 클 경우 입실론 탐욕적 방법이 더 좋다.** 

- 추가 고려점 
  
  - **분산이 적더라도, 다중 선택 문제가 비정상적이라면 탐험으로 큰 이득을 볼 수 있다.**
    
    > 비정상적(nonstationary) : 행동 가치의 참값이 시간에 따라 변한다. 
  
  - 또한 대부분의 강화학습에서는 비정상성인 경우가 많다. 

##### 점증적 구현

- 컴퓨터의 메모리 상 표본 평균이 구하기 어려운 경우, 점증적 구현을 적용한다. 

- 표본 평균으로 구한 가치 추정값 $Q_n$은 아래와 같다. 
  
  > $Q_n = \frac{R_1 + R_2 + ... + R_{n-1}}{n-1}$

- **점증적 구현을 통해 구한 $Q_n$ 은 아래와 같다.**
  
  > $Q_{n+1} = \frac{1}{n} \sum^n_{i=1} R_i$
  > 
  >             = $\frac{1}{n} [R_n + (n-1) * \frac{1}{n-1} \sum^{n-1}_{i=1} R_i ]$
  > 
  >             = $\frac{1}{n} [R_n + (n-1)Q_n$)]
  > 
  >             = $\frac{1}{n}[R_n + nQ_n - Q_n]$
  > 
  >             =<mark> $Q_n + \frac{1}{n} [R_n - Q_n]$</mark>
  
  > n=1 인 경우에는, $Q_1$에 상관없이 $Q_2$ = $R_1$이 된다.
  
  - 위의 식을 일반적으로 표현하면 다음과 같다. 
    
    > 새로운 추정값 <- 이전 추정값 + 시간 간격의 크기 [목푯값 - 이전 추정값]
    > 
    > 추정 오차(error) : 목푯값 - 이전 추정값

###### 비정상(nonstationary) 전제 하 점진적 구현

- 보상값의 확률 분포가 시간이 지남에 따라 변화하는 **'비정상 상황'** 속에선 최근의 보상에 더 큰 가중치를 주고, 오래된 보상일수록 낮은 가중치를 주는 것이 타장당하다. 

- 또한 점증적 계산 방법에서 $\frac{1}{n}$ 로 사용된 '시간 간격의 크기'는 사실 시간 단계마다 다를 수 있다. 
  
  - 따라서 매 단계마다 시간 단계를 다를 것을 고려해, **시간 간격의 크기를 $\alpha$  또는 $\alpha_t(a)$ 로 표현하겠다.**

- **기하급수적 최신 가중 평균** 
  
  * 시간 간격의 크기 $\alpha \in (0,1]$ 에 속한다 가정했을 때의 점진적 구현
  
  > $Q_{n+1} = Q_n + \alpha[R_n - Q_n]$
  > 
  >             = $\alpha R_n + (1-a)Q_n$ 
  > 
  >             = $\alpha R_n + (1-\alpha)\alpha R_{n-1} +(1-\alpha)^2\alpha R_{n-2} $
  > 
  >                 $ ... + (1-\alpha)^{n-1}\alpha R_1 + (1-\alpha)^n Q_1$
  > 
  >             = $(1-\alpha)^nQ_1 + \sum^n_{i=1} \alpha(1-\alpha)^{n-i} R_i$
  
  > 가중치의 합은 "$(1-\alpha)^n + \sum^n_{i=1} \alpha(1-\alpha)^{n-i}$  =1" 을 만족한다.   

- 또는 $\alpha_n(a)$ 을 적용할 수 있다. 단, **$\sum \alpha_n(a) =1$ 을 보장하기 위해서 아래 조건을 만족시켜야 한다.** 
  
  1. $\sum^\infin_{n=1} \alpha_n(a) = \infin$  
     
     > 어떠한 초기 조건이나 확률적 변동성(random fluctuation)도 극복할 만큼 충분히 큰 시간 간격을 보장하기 위함.  
  
  2. $\sum^{\infin}_{n=1} \alpha^2_n(a) < \infin$
     
     > 시간 간격이 감소하여 결국에는 수렴성을 확신할 만큼 충분히 작아질 것을 보장하기 위함. 
  - => 위의 조건으로 비춰볼때, $\alpha_n(a) = \alpha$ 을 적용하면 조건 2를 만족하지 못한다. 
    
    - 이는 <u>추정값이 결코 완전히 수렴하지 않고 가장 최근에 받은 보상에 반응하여 연속적으로 변한다는 것을 의미</u>한다. 
  
  - 단, 강화학습에 있어 **비정상적인 상황이 매우 많기 때문에**, 위와 같이 확률의 수렴 조건은 실제 문제에 적용하거나 실증적 연구를 수행할 때에는 거의 사용되지 않는다.






