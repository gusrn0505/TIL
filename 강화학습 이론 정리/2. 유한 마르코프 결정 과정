## Chapter 3 : 유한 마르코프 결정 과정(Finite Markow decision Process)

출처 : 단단한 강화학습 - Part 1 표 형태의 해법 속 챕터 3



#### 서론

- MDP에서는 각 상태 s에 있는 각 행동 a에 대한 $q_*(s,a)$ 를 추정하거나 최적의 행동 선택을 가정한 채로 각 상태의 가치 $v_*(s)$ 를 추정한다. 

- 또한 MDP는 지연된 보상을 포함하여, 지연된 보상과 즉각적인 보상 사이에서 균형을 잡고자 한다. 

- MDP는 강화학습 문제를 이론적으로 정교하게 설명할 수 있도록 해주는 이상적인 수학적 형태이다. 



##### 에이전트-환경 인터페이스

- ###### 용어
  
  > 에어전트(Agent) : 학습자와 의사결정자 
  > 
  > - 에이전트는 행동을 선택한다. 
  > 
  > 환경(Environment) : 에이전트와 상호작용하는 에이전트 이외의 모든 것 
  > 
  > - 환경은 에이전트의 행동에 반응하여 새로운 환경과 보상을 제공한다. 



![](./picture/2-1.png)

- 모든 시간 단계 t에서 에이전트는 환경의 상태(State) $S_t \in S$ 를 나타내는 어떤 것을 받으며, 이를 기반으로 행동(Action) $A_t \in A(s)$ 을 선택한다. 

- 시간 단계 t+1 에서 에어전트는 부분적으로는 이전 행동의 결과로서 숫자로 된 보상(Reward) $R_{T+1} \in R$ 을 받고 스스로가 새로운 상태 $S_{t+1}$ 에 있다고 인식한다. 
  
  - 그 결과 MDP와 에이전트는 다음과 같은 궤적(trajectory)을 만들어 낸다. 
    
    > $S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3 ...$





- <mark>$p(s', r |s,a) = Pr(S_t=s', R_T =r|S_{t-1}=s, A_{t-1} =a)$</mark>
  
  > 모든 $s \in S, a \in A(s)$ 에 대해 $\sum_{s' \in S}\sum_{r \in R}p(s',r|s,a) =1$
  
  - 함수 p는 MDP의 동역학을 정의하며, 4개의 변수(s', r, s,a) 로 정해지는 결정론적 함수다.
  
  - <u>$S_t, R_T$ 각각이 어떤 값을 가질 확률은 바로 직전 상태와 행동인 $S_{t-1}, A_{t-1} $ 에만 의존하며, 그 이전(t-2)의 상태와 행동에는 전혀 영향을 받지 않는다. </u>
    
    - <u></u>상태는 과거의 모든 에이전트-환경 상호작용에 대한 정보를 포함해야 한다. (상태에 대한 제약)
  
  - 이와 같은 조건을 만족하는 상태는 <mark>마르코프 특성(Markow property)</mark>을 가졌다고 한다. 



- ###### 추가 관련 함수
  
  - 상태 전이 확률 (State-transition probability) : $p(s'|s,a) = Pr(S_t = s' | S_{t-1}=s, A_{t-1}=a) = \sum_{r\in R}p(s',r|s,a)$
  
  - 보상의 기댓값 : $r(s,a) = E[R_t|S_{t-1} =s , A_{t-1} =a] = \sum_{r \in R} r \sum_{s' \in S} p(s',r|s,a)$
  
  - '상태-행동-다음 상태' 에 대한 보상의 기댓값 : $r(s,a,s') = E[R_t| S_{t-1}=s, A_{t-1}=a, S_t=s'] = \sum_{r \in R} r \frac{p(s', r|s,a)}{p(s'|s,a)}$



- MDP 구조는 목표 지향적인 상호작용으로부터의 학습 문제를 상당히 추상화한 것이다.
  
  - 목표 지향적인 행동을 학습하는 문제는 에이전트와 호나경 사이에서 움직이는 3가지 신호(a,r,s)로 축약할 수 있다.
  
  - 시간 단계가 유동적이여도 된다. 
  
  - 에이전트와 환경에 대한 경계를 비교적 자유롭게 잡을 수 있다. 
    
    - 일반적인 규칙은 에이전트에 의해 임의로 변경될 수 없는 모든 것은 에이전트 밖에 있다고 간주한다. 
    
    - 보상을 계산하는 일은 항상 에이전트 밖에서 이루어진다. (에이전트가 개입되면 안되기 때문)
    
    - => 에이전트 - 환경 경계는 에이전트의 지식을 제한하기보다는 <u>에이전트의 '절대적 제어'의 환계</u>를 나타낸다. 
      
       

##### 목표와 보상

- 에이전트의 목표는 <u>장기적으로 자신이 받는 보상의 총합을 최대화하는 것</u>이다 **(보상가설)**
  
  - 즉, 보상은 곧 이루고자 하는 목표를 나타낸다. 
  
  - 단, 보상은 에이전트에게 '무엇을' 이루어야 하는지를 알려주는 방법이지, 그것을 '어떻게' 이루어야 하는지를 알려주는 것은 아니다. 
    
    - ex)- 목표를 이루기 위한 중간 단계에도 보상을 설정한다면, 에이전트는 중간 단계만 빠르게 반복할 수도 있다. 



##### 보상과 에피소드

- 에이전트는 일반적으로 **기대되는 이득(expected return  - Gain)** 을 최대화하고자 한다. 
  
  > $G_t = R_{t+1} + R_{t+2} + R_{t+3} ... R_T$
  > 
  > T : 최종 시간 단계.



-  에이전트 - 환경 상호작용이 종단 상태(Terminal State)이 있는 에피소드(Episode)로 구성되어 있을 때, 최종 시간 단계 T가 생긴다. 
  
  - 각 에피소드는 이전 에피소드가 종료된 방식과는 상관없이 독립적으로 시작한다. 
    
    - 종단 시각 T는 보통 에피소드마다 다른 값을 갖는 확률변수다. 
  
  - 이러한 종류의 에피소드를 다루는 작업을 <u>에피소딕 작업</u>  이라고 한다. 
  
  - 때론 모든 비종단(notterminal) 상태 S를 모든 상태와 종단 상태의 합 $S^+$ 와 구분할 필요가 있다. 



- 반면 많은 경우에 있어 에이전트-환경 상호작용은 식별 가능한 에피소드로 자연스럽게 나누어지지 않고 한계 없이 계속 이어진다. **(연속적인 작업)**
  
  - 이런 경우 매 순간의 보상을 합할 경우 이득이 $\infin$ 로 수렴하여, **할인(discounting)** 개념이 필요하다. 
  
  - 에이전트는 할인된 이득의 기댓값을 최대화하기 위해 $A_t$ 를 선택한다. 
    
    > <mark>$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum^\infin_{k=0} \gamma^K R_{t+k+1} $</mark>
    > 
    > <=> $G_t = $ $R_{t+1} + \gamma [R_{t+2} + \gamma R_{t+3} + ... ] = R_{t+1} + \gamma G_{t+1}$
    > 
    > $\gamma :$ 할인율. 0 <= $\gamma$ <=1 의 범위를 가짐. 
  
  - 할인율은 미래 보상의 현재 가치를 결정한다. 



###### 에피소딕 작업과 연속적인 작업을 위한 통합 표기법

> $G_t = \sum^T_{k=t+1} \gamma^{k-t-1} R_k$ 

- T = $\infin$ 의 경우, $\gamma$ =1 이 될 가능성을 고려한 표기법 



##### 정책과 가치 함수 - Monte Carlo Method

- ###### 용어
  
  - 가치 함수 : 에이전트가 주어진 상태에 있는 것이 '얼마나 좋은가'를 추정하는 상태의 함수 
    
    - '얼마나 좋은가' 라는 개념은 미래의 보상(이득의 기댓값)을 기준으로 정의된다. 
  
  - 정책($\pi(a|s)$) : 상태로부터 각각의 가능한 행동을 선택하는 확률로 대응되는 관계 
    
    > $\pi(a|s) :$ $S_t =s $ 일 경우 $A_t=a$ 일 확률 



- 가치 함수($v_\pi(s)$) 는 상태 s에서 시작한 이후로 정책 $\pi$을 따랐을 경우 얻게 되는 이득의 기댓값이다. 
  
  > 모든 $s \in S$ 에 대해 <mark>$v_\pi(s) = E_\pi[G_t | S_t =s] = E_\pi[\sum^\infin_{k=0} \gamma^K R_{t+k+1}| S_t =s ]$</mark>
  
  - 함수 $v_\pi$ 를 <u>정책 $\pi$에 대한 <mark>상태</mark> 가치 함수</u> 라고 부른다. 



-  $q_\pi(s,a)$ 를 정책 $\pi$ 를 아래에서, 상태 s에서 행동 a를 취할 때의  이득의 기댓값으로 정의한다. 
  
  > $q_\pi(s,a) = E_\pi[G_t|S_t = s, A_t =a] = E_\pi[\sum^\infin_{k=0} \gamma^k R_{t+k+1} | S_t =s, A_t =a]$
  > 
  > 함수 $q_\pi(s,a)$ 를 <u>정책 $\pi$ 에 대한 <mark>행동</mark> 가치 함수</u>라고 부른다. 



- 가치 함수의 근본적인 특성은 재귀적인 관계식을 만족한다는 것이다. 
  
  > 모든 s $\in S$ 에 대해, 
  > 
  > $v_\pi(s) = E_\pi[G_t|S_t=s]$
  > 
  >             $ = E_\pi[R_{t+1} + \gamma G_{t+1} | S_t =s]$
  > 
  >             $ = \sum_a \pi(a|s) \sum_{s'} \sum_{r} p(s',r|s,a) [r+\gamma E_\pi[G_{t+1}|S_{t+1}=s']]$
  > 
  >            <mark> $ = \sum_a\pi(a|s) \sum_{s', r} p(s', r|s,a)[r+\gamma v_{\pi}(s')]$ </mark>**(벨만 방정식)**
  > 
  > *$\pi(a|s) * p(s',r|s,a)$  : 상태 s일 때 새로운 상태 s'와 r가 주어질 확률 
  
  - 가치 함수 $v_\pi$ 는 벨만 방정식의 유일한 해다. 





- **보강 다이어그램**
  
       ![](./picture/2-2.png)  
  
  - 최상위에 있는 루트 노드인 상태 s로 시작하여 에이젠트는 정책 $\pi$ 에 따라 주어진 행동 중 어떤 것이라도 취할 수 있다. 각각에 행동에 대해 환경이 반응하여 다음 상태 s'와 보상 r이 함께 도출된다.  
    
    > 빈 원 : 상태를 의미 
    > 
    > 검은 원 : 상태 - 행동 쌍 을 의미


