### Chapter 8 : 표에 기반한 방법을 이용한 계획 및 학습

출처 : 단단한 강화학습 - Part 2 근사적 해법 



##### 서론

- 앞으로 근사적 가치 함수를 표 형태가 아닌, 가중치 $w \in R^d$ 를 갖는 파라미터화된 함수 형태로 표현한다. 
  
  > $\hat v(s,w)$ ~= $v_\pi(s)$ 
  > 
  > > w : (일반적으로) 모든 층에서의 연결 가중치 벡터
  > > 
  > > $\hat v$ : (일반적으로) 다층 인공 신경망에 의해 계산된 함수   
  
  - 일반적으로 가중치의 개수(w의 차원)은 상태의 개수보다 훨씬 적고, 하나의 값을 바꾸면 많은 상태의 가치 추정값이 바뀐다. 
  
  - 상태가 갱신으로 인한 변화는 일반화되어 다른 많은 상태의 가치에 영향을 준다.

- **일반화는 학습을 더 강력하게 만들면서, 한편으론 관리/이해를 어렵게만든다.**
  
  - +강화학습을 <u>함수 근사로 확장함에 따라 부분 관측 가능한 문제에 적용가능</u>해진다.



##### 가치 함수 근사

- 예측은 특정 상태에서의 가치를 그 상태에 대한 **'보강된 가치'** 또는 **갱신 목표**(update target)<u>를 향해 이동시키기 위한 가치 함수 추정값의 갱신</u>으로 설명된다. 
  
  > $s |→ u $
  > 
  > > s : 상태 (갱신의 대상)
  > > 
  > > u : s의 가치 추정값이 도달해야 하는 갱신의 목표 
  > 
  > ex)- MC : $S_t |→ G_t$, TD(0) : $S_t |→ R_{t+1} + \gamma \hat v(S_{t+1}, w_t)$



- 함수 근사 방법은 근사하려고 하는 함수의 바람직한 입출력 관계를 나타내는 예제가 주어진다는 것을 전제로 한다.
  
  > 함수값이 목표$u$ 에 유사할 때, 종종 **함수근사**(function approximat)라고 한다.  
  
  - 즉, 입출력 예제를 모사하는 방법으로 <u>지도 학습의 일환</u>으로 분류된다. 
    
    - 원칙적으론 지도학습의 방법론(ex-인공 신경망, 결정 트리 등)을 사용할 수 있다.
    
    - 하지만 일반적으로 비정상적 / 온라인 상황 속 학습을 강조하는 강화학습 특성 상, 정적 훈련 데이터를 전제로 하는 지도 학습의 방법이 적합하지 않을 수 있다.



---

##### 예측 목적($\bar {VE}$)

- 표 형태의 데이터 속에서는 예측의 분명한 목적을 명시하지 않아도 괜찮다. 
  
  - 표 형태에선, 학습된 가치 함수는 실제 가치 함수와 정확하게 같아질 수 있다. 
  
  - 표 형태에선, 각 상태에서 학습된 가치는 분리되어 있다.



- 반면, <u>근사에선</u> 주어진 상태 수에 보다 적은 가중치 수로 인해 <u>목적을 선택</u>해야 한다. 
  
  - 한 상태의 가치 추정값을 더 정확하게 만드는 것은 언제나 다른 상태를 덜 정확하게 만드는 것을 의미한다. 즉, 모든 상태에 대해 정확하게 추정하는 건 불가능하다. 
  
  - 또한 한 상태의 갱신은 다른 많은 상태에 영향을 미친다. 
  
  - => <mark>어떤 상태를 가장 우선시할 것인지를 명시하기 위해 상태 분포를 도입한다.</mark> 
    
    > 상태 분포 : $\mu(s) >= 0 , \sum_a \mu(s) =1$
  
  - 또한 **에피소딕과 연속적인 문제를 분리**하여 살펴 봐야 한다.



- **Ex)-에피소딕 문제의 활성 정책 분포**
  
  - 에피소딕 문제에서는 초기 상태가 어떻게 선택되는지에 따라 활성 정책 분포가 영향을 받는다는 점에서 다른 문제의 활성 정책 분포와 다르다.
    
    > 모든 $s \in S$ 에 대해 $\eta(s) = h(s) + \sum_s \eta(\bar s) \sum_a\pi(a|\bar s)p(s|\bar s,a)$
    > 
    > > $h(s)$ : 상태 s에서 시작할 확률
    > > 
    > > $\eta(s)$ : 단일 에피소드의 상태 s에서 평균적으로 소비된 시간 단계의 수
  
  - 이때 <u>활성 정책 분포는 각 상태의 소비된 시간의 비율을 정규화한 것</u>이 된다.
    
    > 모든 $s \in S$에 대해 $\mu(s) = \frac{\eta(s)}{\sum_{s'}\eta(s')}$ (할인이 없는 경우)



- 가중치 w에 대한 성능을 판단하기 위해 가치추정오차(**V**alue **E**rror)을 도입한다.
  
  > $\bar {VE}$ = $\sum_{s \in S} \mu(s) [v_\pi(s) - \hat v(s,w)]^2$    (평균 제곱 가치 오차)
  > 
  > - 대체로 $\mu(s)$ 는 '상태 s에서 소비된 시간의 비율' 로 선택된다.  
  > 
  > - 활성 정책 훈련의 경우, 이것은 **활성 정책 분포**라고 불린다. 
  
  - 제곱근 $\bar {VE}$는 <u>근사 가치가 실제 가치와 얼마나 차이를 갖는지에 대한 지표</u>가 된다.
    
    -  $\bar {VE}$ 가 최적의 지표란 점은 명확하지 않다. 
    
    - 더 나은 대안이 뭔지 또한 명확하지 않기에, 지금으로선 $\bar {VE}$ 에 초점을 둔다.



- $\bar {VE}$ 측면에서 이상적인 목표는 **전역 최적값**(global optimum) $w^*$을 찾는 것이다. 
  
  > 전역 최적값 $w^*$ : $\bar {VE}(w^*) <= \bar {VE}(w)$ for $\forall w \in W$
  
  - 하지만 위의 목표는 인공 신경망 등 복잡한 함수의 근사에선 보통 가능하지 않다. 
  
  - 또한 일부 문제에선 **지엽 최적값**(local optimum)으로 조차 수렴성을 보장못한다. 



---

##### 확률론적 경사도(Stochastic-Gradient Descent, SGD)와 준경사도 방법

- SGD은 함수 근사 방법 중 가장 폭넓게 사용되며, 특히 온라인 강화학습에 잘 맞는다. 

- SGD는 각 예제에 대해 <u>오차를 가장 많이 감소시키는 방향으로 가중치 벡터를 조금씩 조정</u>함으로써 관측된 <u>예제의 오차를 최소화</u>한다. 
  
  > $w_{t+1} = w_t - \frac{1}{2} \alpha \nabla [v_\pi(S_t) - \bar v(S_t, w_t)]^2 $
  > 
  >            $= w_t + \alpha[v_\pi(S_t) - \hat v(S_t,w_t)]\nabla \hat v(S_t, w_t)$
  > 
  > > $w_t$ : t번째 시간 단계에서의 가중치 벡터($w_1, w_2, ..., w_d$)
  > > 
  > > $\hat v(s,w)$ : 모든 $s \in S$에 대해 미분가능한 w의 함수 
  > > 
  > > $\alpha$ : 양의 시간 간격 파라미터 
  > > 
  > > $\nabla \hat v(S_t, w_t)$ = ($\frac {\theta f(w)}{\theta w_1},\frac {\theta f(w)}{\theta w_2}, ... , \frac {\theta f(w)}{\theta w_d} $) . 함수 f의 경사도를 의미. 
  > > 
  > > $f(w)$ : 벡터 w를 변수로 하는 임의의 스칼라 함수 
  
  - SGD 말고도 다른 방향으로 예제와의 오차를 제거할 수 있지만, 바람직하진 않다. 
  
  - SGD는 $\alpha$가 표준적인 확률론적 근사 조건을 만족 시, 지역적 최적값으로 수렴한다. 
    
    > 표준적인 확률론적 근사 조건 : $\sum_{n=1}^\infin \alpha_n = \infin $ & $\sum_{n=1}^\infin \alpha_n ^2 < \infin$



- 위의 식에서 구할 수 없는 $v_\pi(S_t)$을 근사값 $U_t(S_t)$로 대체한다. 
  
  > $w_{t+1} = w_t + \alpha[U_t - \hat v(S_t,w_t)]\nabla \hat v(S_t, w_t)$
  > 
  > - 이때 $U_t$의 값은 $v_\pi$ 값에 오차가 섞인 값일 수도 있다. 
  
  - <u>$U_t$가 각각의 t에 대해 편차 없는 추정값</u>이라면, **$w_t$ 는 지엽적 최적값으로 수렴**한다.
    
    - 즉, 부트스트랩을 사용하는 n단계 이득이나 DP의 경우, $U_t$에 대해 편차를 부여하여 $w_t$가 수렴할 것을 보장하지 못한다.



- 부트스트랩은 **준경사도 방법**로 불리며 특정 상황(선형 근사)하 안정적으로 수렴한다.
  
  - $v_\pi(s)$와는 독립적인 목표를 목표를 잡음으로써, 가중치 $w_t$를 변경하는 것이 추정값에 미치는 효과는 고려하되 목표에 미치는 효과를 무시한다. 
    
    > $U_t = R_{t+1} + \gamma \hat v(S_{t+1}, w)$     [준경사도 방법의 원형 - TD(0)]
  - 부트스트랩은 보통 <mark>빠른 학습이 가능하며, 연속적인 학습과 온라인 학습이 가능</mark>하다.



- **상태 결집**(state aggregation)은 함수 근사를 일반화하는 간단한 형태다. 
  
  - 상태 결집 하에서 상태는 함께 묶이고, 한 묶음당 하나의 가치 추정값($w_i$)를 갖는다.
  
  - 상태의 가치는 상태가 속한 묶음의 성분으로서 추정된다. 
  
  - 상태가 갱신되면 그 성분 하나만 갱신된다. 



---

##### 선형 방법

- <u>근사함수 $\hat v(., w)$가 가중치 벡터 w에 대해 선형 함수인 경우</u>를 의미하며, 함수 근사의 가장 중요하고 특별한 경우이다.
  
  > $\hat v(s,w) = w^Tx(s) = \sum^d_{t=1} w_ix_i(s)$
  > 
  > > $x(s) :$ 상태 s를 나타내는 **특징 벡터**라고 부름. 
  > > 
  > > - 한 함수의 특징(feature)은 그 함수의 전부를 나타내는 것으로 간주
  > > 
  > > - 어떤 상태에 대한 함수의 가치를 **s의 특징**(feature of s) 이라고 부름
  
  - 선형 방법의 경우, <u>특징은 근사 함수 집합에 대한 선형 기저(linear basis)형성</u>하기 때문에, <mark>특징은 **기저 함수**(basis function)이 된다</mark> .



- 선형 함수의 경우 일반적인 SGD 갱신은 다음과 같다. 
  
  > $w_{t+1} = w_t + \alpha[U_t - \hat v(S_t, w_t)] x(S_t)$
  > 
  > > $\nabla \hat v(s,w) = x(s)$



- **선형 SGD는 여러 측면에서 선호받는다.**
  
  - 식이 매우 간단하여 수학적 분석이 용이하다.
  
  - 오직 하나의 최적값이 존재하여, 지엽적 최적값을 보장하는 모든 방법은 전역 최적값으로 수렴함을 보장한다.
    
    - 또는 더욱 간단한 함수가 있을 경우 동등하게 좋은 최적값의 집합이 존재한다.



- 선형 함수 근사 하<u> TD(0) 알고리즘의 수렴성은 다음 식에 따라 보장</u>된다. 
  
  > $w_{t+1} = w_t + \alpha(R_{t+1} + \gamma w_t^Tx_{t+1} - w^T_t x_t)x_t$
  > 
  >            $= w_t + \alpha(R_{t+1}x_t - x_t(x_t - \gamma x_{t+1}^T w_t)$
  > 
  > $E[w_{t+1}|w_t] = w_t + \alpha(b-Aw_t)$ 
  > 
  > > $x_t = x(S_t)$
  > > 
  > > $b = E[R_{t+1}x_t] \in R^d $
  > > 
  > > $A = E[x_t(x_t - \gamma x_{t+1})^T] \in R^d$ x $R^d$
  > 
  > - $b- Aw_t =0$ 일 경우, 가중치 벡터는 수렴한다. 
  
  > $b - Aw_{TD} =0$
  > 
  >                   $b = Aw_{TD}$
  > 
  >             $w_{TD} = A^{-1}b$
  
  - $A^{-1}$ 의 존재 증명은 책 참고할 것 :) 



- TD 고정점에서, (연속적인 문제의 경우) 발생 가능한 가장 작은 오차를 유한하게 확장한 값에 의해 $\bar {VE}$가 제한된다 
  
  > $\bar {VE}(w_{TD}) <= \frac {1} {1-\gamma} min_w \bar {VE}(w) $
  > 
  > *Why? $\alpha$ 의 값이 그렇게 되야한다는 건 대략 감이 오긴 하는데 정확히는 모르겠다.*
  
  - 즉, TD 방법을 적용한 결과가 점근적으로 수렴했을 때의 오차는, 발생 가능한 가장 작은 오차에 $\frac {1}{1-\gamma}$ 을 곱한 값보다 크지 않다. 



- **MC 방법과 TD 방법은 어떤 방법이 최선**인가는 <u>근사 및 문제의 특성, 그리고 얼마나 오랫동안 학습이 지속되는 지에 따라 다르다. </u>
  
  ![](picture/8-1.png)
  
  - TD 방법은 MC 보다 분산이 크게 감소해서 MC 보다 더 빠르다, 
  
  - 단, $\gamma$ 가 종종 1에 가깝기 때문에, TD 방식의 점근적 수렴 시점이 늦춰질 수 있다. 








