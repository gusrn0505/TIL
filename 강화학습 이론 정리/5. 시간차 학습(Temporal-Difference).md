### 

### Chapter 6 : 시간차 학습(Temporal-Difference)

출처 : 단단한 강화학습 - Part 1 표 형태의 해법 속 챕터 6

##### 서론

- TD(**T**emporal **D**ifference)는 몬테카를로 방법과 동적 프로그래밍 방법을 결합한 것이다. 
  
  - TD 학습은 MC 방법처럼 <u>환경의 동역학에 대한 모델 없이도 가공하지 않은 경험으로부터 직접 학습</u>할 수 있다. 
  
  - TD 방법은 DP 방법처럼 <u>최종 결과를 얻을 때까지 기다리지 않고, 부분적으로는 다른 학습된 추정값을 기반으로 추정값을 갱신</u>한다. 
  
  - TD, MC, DP는 다양한 방법으로 서로 섞이고 결합될 수 있다. 



--- 

##### TD 예측

- TD와 MC 방법 모두 예측 문제를 풀기 위해 **경험을 활용**한다. 
  
  - MC는 상태를 마주친 이후에 발생하는 이득을 알 수 있을 때까지 기다린다. 
  
  - 반면 TD는 시각 t+1 에서 즉각적으로 목표를 형성하고 관측된 보상 $R_{t+1}$ 과 추정값 $V(S_{t+1})$ 을 이용하여 유용한 갱신을 수행한다.  



- 두 방법 모두 **비정상(nonstationary) 환경에 적용가능**하다.
  
  - **고정 $\alpha$ MC 방법** 
    
    > $V(S_t)$ <- $V(S_t) + \alpha[G_t - V(S_t)]$
    > 
    > > $\alpha$ : 고정 시간 간격 
    > > 
    > > $G_t$ : 시각 t 이후의 실제 이득. 에피소드가 끝나야 할 수 있다.  
    
    - 갱신의 목표 : $G_t$
  
  - **TD(0) / 단일 단계(one-step) TD**
    
    > $V(S_t)$ <- $V(S_t) + \alpha[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]$
    
    - 갱신의 목표 : $R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$
    
    - 이 갱신은 $R_{t+1}$ 을 받는 즉시 $S_{t+1}$ 로 전이된다. 



- **MC, DP, TD의 추정값 특징** 
  
  > $v_\pi(s) = E_\pi[G_t|S_t =s ]$  <mark>(식 1)</mark>
  > 
  >             $= E_\pi[R_{t+1} + \gamma G_{t+1}|S_t=s]$
  > 
  >             $= E_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t =s]$  <mark>(식 2)</mark>
  
  - <u>DP 방식은 식 2의 추정값</u>을, <u>MC는 식 1의 추정값을 목표</u>로 한다.
    
    - DP 방법은 동역학을 알아 기댓값을 구할 순 있으나, $v_\pi(S_{t+1})$을 몰라 추정값 $V(S_{t+1})$ 이 사용된다. (부트스트탭)
    
    - MC 방법은 표본 이득이 사용되어 기댓값을 모르기 때문에 추정값이 된다. 
  
  - TD는 위의 두가지 경우 모두 해당한다. 
    
    - 식 2의 기댓값에 대한 표본을 추출한다. (표본추출)
      
      - TD,MC 모두 이후 상태의 가치($V(S_{t+1})$) 와 이득($R_{t+1}$)에 의해 원래 상태($V(S_t)$) 를 갱신하므로 **표본 갱신** 이라고 부른다.  
    
    - 실제 $v_\pi$ 대신에 현재 추정값 V를 사용한다. (부트스트랩)



- TD(0) 방식은 **TD오차**에 의해 갱신된다. 
  
  > TD 오차($\delta_t$) : $R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$
  > 
  > - TD 오차는 그 시각에 만들어진 추정값의 오차이다.  
  > 
  > - $R_{t+1}, V(S_{t+1})$을 알아야 하기에 t+1 단계가 되어야 계산할 수 있다. 
  
  - 몬테카를로 오차는 TD 오차의 합으로 표현될 수 있다. 
    
    > 전제 : $V(s)$ 의 값은 에피소드 동안 변하지 않는다. 
    > 
    > $G_t - V(S_t) = R_{t+1} + \gamma G_{t+1} - V(S_t) + \gamma(S_{t+1}) - \gamma(S_{t+1})$
    > 
    >                         $= \delta_t + \gamma (G_{t+1} - V(S_{t+1}))$
    > 
    >                         $= \delta_t + \gamma \delta_{t+1} + \gamma^2(G_{t+2}-V(S_{t+2}))$
    > 
    >                         $= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + ... + \gamma^{T-t}(G_T - V(S_T))$
    > 
    >                         $=  \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + ... + \gamma^{T-t}(0 - 0)$
    > 
    >                         $= \sum^{T-1}_{k=t} \gamma^{k-t}\delta_k$
    
    - 만약 V가 에피소드 동안 갱신된다면 등식은 성립하지 않는다. 
      
      - 하지만 시간 간격이 작으면 근사적으로 여전히 유효할 수 있다. 

---

##### TD 예측 방법의 좋은 점

- <mark>환경에 대한 모델과 보상, 다음 상태의 확률 분포를 필요로 하지 않는다</mark>. (DP 대비 강점)

- <mark>점증적인 방식으로 갱신</mark>하여 온라인(ex- 에피소드 x)에도 적용가능하다. (MC 대비 강점)
  
  - MC는 에피소드가 끝나야만 갱신할 수 있으나, TD는 1단계만 기다리면 된다. 

- MC 방법은 실험적인 행동이 취해지는 에피소드를 무시하거나 할인해야 한다. 반면 TD는 <mark>어떤 후속 행동이 취해지는가 관계없이</mark> 각각의 전이로부터 학습하여 <mark>훨씬 덜 민감하다</mark>. 

- TD의 부트스트랩 방식은 <mark>확실성 동등 추정값으로 수렴함을 보장</mark>한다.  
  
  - 모든 고정된 정책 $\pi$ 에 대해, 고정 시간 간격 파라미터 $\alpha$를 적용하고, 그 값이 충분히 작고 <u>확률론적 근사 조건</u>을 따르다면 100%의 확률로 $v_\pi$ 로 수렴함이 증명되었다. 
    
    > 확률론적 근사 조건 : $\sum^\infin_{n=1} \alpha_n(a) = \infin$  and $\sum^\infin_{n=1} \alpha^2(a) < \infin$

- DP, MC, TD 중 어떤 방법이 다른 것보다 더 빨리 수렴한다고 <u>수학적으로 증명된 건 없다</u>.
  
  - 일반적으로 확률론적 문제에서는 TD 방법이 고정 $\alpha$ MC 방법보다 더 빨리 수렴한다. 

---

##### TD(0)의 최적성

- 점증적 학습 방법을 이용하는 흔한 접근법은 **학습 방법이 정답으로 수렴할 때까지 반복적으로 경험을 제시하는 것**이다. 
  
  - 가치 함수의 근삿값 V 가 주어지면, <u>TD 오차($\delta_t$) 또는 MC 오차는 매순간 갱신</u>된다.
  
  - 반면 <u>가치 함수</u>는 시간 단계가 끝난 뒤 <u>모든 증가량의 총합만큼 단 한 번 갱신</u>한다. 
    
    > 훈련 데이터에 대한 각각의 완전한 일괄(batch) 처리 이후에만 가치 함수가 갱신되기 때문에 이것을 **일괄 갱신(batch-updating)** 이라 부른다.
  
  - 수렴할 때끼지 갱신된 가치 함수를 통해 다시 오차, 가치함수 순으로 갱신한다. 

- 일괄 갱신 하에서 $\alpha$가 충분히 작은 값으로 선택되는 한, <u>TD(0)는 시간 간격 파라미터 $\alpha$에 따라 달라지는 하나의 정답에 결정론적으로 수렴</u>한다.
  
  - 고정<u> $\alpha$ MC 방법</u> 또한 같은 조건 하에서 결정론적으로 수렴하지만<u> 다른 정답</u>이다. 
  
  > 일괄 MC 방법은 훈련 데이터의 평균제곱오차를 최소화하는 추정값을 찾는다.
  > 
  > - 훈련 데이터에 대해 0의 오차를 도출하지만 성능이 비교적 낮을 것
  > 
  > 일괄 TD(0)는 마르코프 과정의 최대우도 모델에 대해 올바른 추정값을 찾는다. 
  > 
  > - 훈련 데이터에 대해 오차가 있지만 <u>'미래의' 데이터에 대해선 더 작은 오차를 만들 것으로 기대됨</u> 
  > 
  > - 모델이 올바를 경우 올바른 가치 함수의 추정값을 정확하게 구할 수 있다. 
  >   
  >   - 이는 추정값이 근사값이 아닌 확실한 값을 안다 가정하는 것으로, 이를 **확실성 동등 추정**(certainty-equivalence estimate)이라 부른다. 
  > 
  > - 일반적으로 <mark>일괄 TD(0)는 확실성 동등 추정값으로 수렴</mark>한다. 

- 일괄적 형식 하,<mark> TD(0)가 실제 확실성 동등 추정값을 계산하기 때문에 MC보다 빠르다.</mark>
  
  - TD 방법은 상태의 개수가 n인 상태에서 <u>n 수준 이하의 메모리와 훈련 데이터에 대한 반복적인 계산을 사용하여 추정값을 근사</u>할 수 있다. 
  
  - 이는 확실성 동등 추정값을 구하는데 전통적인 방법으론 $n^3$ 수준의 계산 단계가 필요하다는 점을 고려할 때, <u>규모가 큰 상태 공간을 다룰 때 TD 방법이 실현 가능한 유일한 방법</u>처럼 여겨지는 이유다. 

--- 

##### 살사(Sarsa) : 활성 정책 TD 제어

- 첫번째 단계는 <u>행동 가치 함수를 학습하는 것</u>이다. 
  
  - 특히, 활성 정책 방법을 적용할 경우 현재의 행동정책 $\pi$와 모든 상태 s 및 행동 a에 대해 $q_\pi(s,a)$ 를 추정해야 한다. 
    
    - *모델을 모를 때는 행동 가치 함수만 있어도 정책을 판단할 수 있음*

- 두번째 단계는 <u>비종단 상태 $S_t$로부터 전이가 일어난 이후 가치 함수를 갱신</u>하는 것이다. 
  
  > $Q(S_t,A_t)$ <- $Q(S_t, A_t) + \alpha [R_{t+1} + \gamma $ <mark>$Q(S_{t+1}, A_{t+1})$</mark> $ - Q(S_t, A_t)]$
  > 
  > 갱신할 때 5개의 사건($S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$)을 필요하여, 앞글자를 따 이 알고리즘을 **Sarsa** 라고 부른다.  

- 정책은 위의 두 단계를 반복하면서 점차 수렴할 것이다.  

- 살사 알고리즘의 수렴 특성은 정책이 Q에 의존하는 성질에 따라 달라진다. 

--- 

##### Q 학습 : 비활성 정책 TD 제어

- 강화학습 발전의 큰 축을 맡고 있는 Q-학습(Q-learning)의 알고리즘은 다음과 같다.
  
  > $Q(S_t, A_t)$ <- $Q(S_t, A_t) + \alpha[R_{t+1} +  \gamma$ <mark>$max_a Q(S_{t+1}, a)$</mark> $ - Q(S_t, A_t)] $
  
  - <mark>함수 Q는 <u>정책에 상관없이</u> 최적 행동 가치 함수 $q_\pi$에 직접적으로 근사한다.</mark> 
    
    - 정책은 여전히 어떤 상태-행동 쌍을 마주치고 갱신할까에 영향을 미친다.
    
    - 수렴을 위한 조건은 <u>'모든 쌍이 계속해서 갱신되어야 한다'</u>는 것이 전부다. 

--- 

##### 기댓값 살사

- Q 학습에서의 $max_aQ(S_{t+1}, a)$ 대신, **각 행동이 현재의 정책 하에 있을 것 같은 기댓값을 사용**한다. 
  
  > $Q(S_t,A_t) $ 
  > 
  > <- $Q(S_t, A_t) + \alpha [R_{t+1} +$ $\gamma$ <mark>$E_\pi[Q(S_{t+1}, A_{t+1})|S_{t+1}]$</mark> - $Q(S_t, A_t)]$
  > 
  > <- $Q(S_t, A_t) + \alpha [R_{t+1} + \gamma$ <mark>$\sum_a \pi(a|S_{t+1})Q(S_{t+1}, a)$ </mark>$- Q(S_t, A_t)]$

- 살사가 **기댓값을 기준으로** 이동하는 방향과 같은 방향으로 **결정론적으로 이동**한다. 
  
  > 따라서 이 알고리즘을 **기댓값 살사**(expected Sarsa)라고 부른다. 

- 기댓값 살사는 살사보다 계산이 복잡하지만, 그 대신 <u>$A_{t+1}$에 대한 무작위 선택 때문에 발생하는 분산을 없애준다</u>. 
  
  - 같은 양의 경험이 주어졌을 때 기댓값 살사가 일반적으로 살사보다 성능이 좋다. 
  
  - 또한 시간 간격 파라미터 $\alpha$ 가 1에 근접해도 성능이 악화되지 않는다. 단, $\alpha$ 가 작으면 단기 성능은 악화된다. 

- 일반적으로 기댓값 살사는 행동을 만들어 내기 위해 정책 $\pi$ 와는 다른 정책(b)을 사용할 것이고, 이 경우 비활성 정책 알고리즘이 된다. 
  
  - 정책 $\pi$가 탐욕적 정책이고 행동 정책(b)이 좀 더 탐험적이라면 <u>기댓값 살사는 정확히 Q-학습이 된다</u>. 
    
    > *$\pi$ 가 탐욕적일 수록 $max_a Q(S_{t+1},a)$ 에 근접하며, b가 탐험적일수록 모든 a에 대해 고려할 수 있기 때문*
    
    > 기댓값 살사는 Q학습의 바탕이며, Q학습을 일반화한 것으로 볼 수 있다. 
  
  - 계산 능력을 좀 더 필요로 한다는 점을 제외하면, 기댓값 살사는 Q-학습과 살사를 능가할지도 모른다. 

---

##### 최대화 편차 및 이중 학습

- 모든 제어 알고리즘은 목표 정책을 만드는 데 있어 최대화를 포함한다. 
  
  - 하지만 각 <mark>최대화 과정에서 최대화 편차를 만들어 낼 수 있다</mark>. 
    
    > 행동 a의 실제 가치 $q(s,a)$ 가 0이라고 할 때, 추정값 Q(s,a)는 불안정하여 일부는 양수이고 일부는 음수의 분포를 가질 것이다. 
    > 
    > 이때 가치의 최댓값은 0이나 추정값의 최대값은 양수로, 편차가 발생한다.
    > 
    > 이러한 편차를 **최대화 편차**(Maximization bias)라고 부른다. 

- 최대화 편차 문제를 바라보는 하나의 관점은 <u>'동일한 표본이 행동의 가치를 추정하는 데와 가치를 최대화하는 행동 둘다 사용되기 때문'</u> 이다. 
  
  > 기댓값에 대한 추정값 $\neq$ max(실제 가치의 추정값)  
  
  - 따라서 **이중 학습**과 같이 각각을 2개의 표본 세트로 추정한다면 편차 없는 추정값을 구할 수 있다.
    
    > $Q_1(S_t, A_t) $ <- $Q_1(S_t,A_t) + \alpha[R_{t+1} + \gamma Q_2(S_{t+1}, argmax_a Q_1(S_{t+1}, a)) - Q_1(S_t,A_t)] $
    
    - 이 때, <u>두 개의 추정값을 학습하지만 오직 하나의 추정값만 매번의 다중 선택에서 갱신된다</u>. 
    
    - 즉, 이중 학습은 두 배의 메모리를 필요로 하지만 단계별로 필요한 계산량이 증가하지는 않는다.  

--- 

##### 게임, 이후상태, 그 밖의 특별한 경우

- 특정 문제는 **이후 상태**를 기준으로 판단하는게 효율적일 때가 있다.
  
  > 이후 상태(afterstate) : Agent가 행동한 이후의 상태
  > 
  > 이후상태 가치함수(afterstate value function) : 이후 상태의 가치 함수 
  
  - 이후 행동이 반복해서 겹치는 경우는 이후 상태를 통해 판단하는게 효율적임.
    
    - ex)- 틱택토 게임.
