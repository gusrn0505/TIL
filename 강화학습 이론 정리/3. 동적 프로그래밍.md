### Chapter 4 : 동적 프로그래밍(Dynamic Program)

출처 : 단단한 강화학습 - Part 1 표 형태의 해법 속 챕터 4

##### 서론

- <mark>동적 프로그래밍</mark>(DP)란, <u>마르코프 결정과정(MDP) 같은 환경 모델이 완벽하게 주어졌을 때</u> **최적 정책을 계산하기 위해 사용될 수 있는 일군의 알고리즘**을 가리킨다. 

- DP의 핵심 개념, 일반적으로는 강화학습의 핵심 개념은 **좋은 정책을 찾는 과정을 체계적으로 구조화하기 위해** **가치 함수를 사용한다.** 
  
  - 최적 가치 함수 $v_*$ 또는 $q_*$ 를 구하고 나면 최적 정책은 쉽게 구할 수 있다. 
  
  > $v_*(s) = max_a E[R_{t+1} + \gamma v_*(s)(S_{t+1})|S_t=s, A_t=a ] $
  > 
  >             $= max_a \sum_{s',r} p(s',r|s,a)[r+\gamma v_*(s')]$
  
  > $q_*(s,a) = E[R_{t+1} + \gamma max_{a'} q_*(S_{t+1}, a') | S_t=s, A_t=a]$
  > 
  >                 $= \sum_{s', r} p(s',r|s,a)[r+\gamma max_{a'} q_*(s',a')$
  
  - 각 $v_*(s), q_*(s,a)$ 는 최적 벨만 방정식을 만족한다. 
  
  - 벨만 방정식을 목표 가치 함수에 대한 근사를 향상시키기 위한 할당(assignment)의 형식, 즉 갱신 규칙으로 변환함으로써 DP 알고리즘을 얻을 수 있다. 
    
    - => Check. 해당 문구를 다 이해하진 못함. T단계, T+1 단계 등으로 표현하여 점차 개선하는 형태로 표현했다는 것인가? 



##### 정책 평가(예측)

- ###### 용어
  
  > **정책평가**(Policy evaluation) 또는 **예측 문제**(Prediction Problem) : 임의의 정책 $\pi$에 대해 상태 가치 함수 $v_\pi$를 계산하는 것 
  > 
  > **반복 정책 평가**(Iterative policy evaluation) : 갱신 횟수 k을 $\infin$ 까지 보냄으로서 $v_k$가 최적 정책 가치함수  $v_\pi$ 에 수렴하도록 하는 알고리즘
  > 
  > **기댓값 갱신** : 이전 가치와 즉각적인 보상의 기댓값을 이용하여 새로운 가치를 구하는 것 
  > 
  > - 상태, 상태-행동 쌍의 갱신 여부나 이후 상태들의 기대 가치가 결합되는 방법에 따라 다양한 종류의 기댓값 갱신이 있음. 
  > 
  > - 갱신은 상태 공간에 대해 일괄처리하는 방식으로 이뤄진다. 

 

- 가치 함수 $v_*(s)$ 에 대해서 벨만 방정식이 성립한다. 
  
  > 모든 $s \in S$ 에 대해,
  > 
  > $v_\pi(s) = E_\pi[G_t|S_t=s]$ 
  > 
  > $             = E_\pi[R_{t+1} + \gamma G_{t+1} | S_t =s]$
  > 
  > $             = \sum_a \pi(a|s) \sum_{s'} \sum_{r} p(s',r|s,a) [r+\gamma E_\pi[G_{t+1}|S_{t+1}=s']]$
  > 
  > $             = \sum_a\pi(a|s) \sum_{s', r} p(s', r|s,a)[r+\gamma v_{\pi}(s')]$ **(벨만 방정식)**



- 벨만 방정식을 갱신 규칙으로 하여 모든 $s \in S$에 대해 다음과 같이 구한다.
  
  > <=> <mark>$v_{k+1}(s)$</mark> = $E_\pi [R_{t+1} + \gamma v_k(S_{t+1})|S_t=s] $
  > 
  >                         <mark>$= \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)[r+\gamma v_k(s')]$</mark>
  
  - $\gamma <1$을 만족하거나, 정책 $\pi$ 를 따르는 모든 상태가 종국적으로 더 이상 변하지 않는 상태에 도달하는 것이 보장된다면, $v_\pi$ 의 존재와 유일성(Uniqueness)가 보장된다.
  
  - 환경의 동역학을 완전히 안다면, 벨만 방정식은 집합 |S|의 원소 개수만큼의 미지수($v_\pi(s), s \in S)$)를 갖는 선형 연립 방정식이 되며, 이때 방정식의 개수도 집합 |S|의 원소 개수와 같다. 
    
    - $\pi(a|s), p(s',r|s,a)$ 을 모두 계산할 수 있으니, s에 대해서만 변하는 선형 연립 방정식이 된다.  

##### 정책 향상

- **정책 향상 정리** : <u>상태 s에서 행동 a를 선택했을 때의 가치($q_\pi(s,a)$)가  $v_\pi(s)$ 보다 클 때</u>, a를 선택하는 새로운 정책 $\pi'$ 가 더 좋은 선택이며 더 좋은 결과를 가져올 것이라 기대한다.
  
  - 임의의 결정론적 정책 $\pi$ 와 $\pi'$ 가 아래를 성립한다면, 정책 $\pi'$ 는 $\pi$ 만큼 좋거나 그 이상 좋다고 한다. 
  
  >  $q_\pi(s,\pi'(s)) >= v_\pi(s)$ for $\forall s \in S$
  
  - 만약 임의의 상태 s에서 위의 부등식을 만족할 경우, 최소한 하나의 상태는 아래 식을 만족해야 한다. 
  
  > <=> $v_\pi'(s) >= v_\pi(S)$ 
  
  - 이때 변경된 정책 $\pi'$ 는 $\pi'(s)=a \neq \pi(s)$ 외에는 모두 동일한 정책이다. 



- **정책 향상 정리 증명 과정**
  
  >  <mark>$v_\pi(s)$</mark>
  > 
  > $< =q_\pi(s,\pi'(s))$ 
  > 
  >     $= E[R_{t+1} + \gamma v_\pi(S_{t+1}) | s_t =s, A_t=\pi'(s)]$
  > 
  > $<= E_{\pi'}[R_{t+1} + \gamma q_\pi(S_{t+1}, \pi' (S_{t+1})) | s_t =s]$       
  > 
  >    $  = E_{\pi'}[R_{t+1} + \gamma E_{\pi'}[R_{t+2} + \gamma v_\pi(S_{t+2})|S_{t+1}, A_{t+1} = \pi'(S_{t+1})]|S_t=s]$ 
  > 
  >    $= E_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 v_\pi'(S_{t+3}) | s_t =s]$
  > 
  >     ...
  > 
  > $<= E_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + ... | S_t=s]$
  > 
  >     <mark>$= v_{\pi'}(s)$</mark>



- **정책 향상** : <u>기존 정책의 가치에 대해 탐욕적이 되게 함으로써</u> <mark>기존 정책을 능가하는 새로운 정책을 만드는 과정 </mark>
  
  > $\pi'(s) = argmax_aq_\pi(s,a)$
  > 
  >             $= argmax_a E[R_{t+1} + \gamma v_\pi(S_{t+1})|S_t=s, A_t=a]$
  > 
  >             $= argmax_a \sum_{s',r} p(s',r|s,a)[r+\gamma v_\pi(s')]$
  
  - 만약 $v_\pi$ 와 $v_{\pi'}$ 가 같다면, 최적 벨만 방정식과 같게 된다. 



---- 

##### 정책 반복

- 가치 함수 $v_\pi$를 이용(계산)하여 더 좋은 정책 $\pi'$으로 <u>향상</u>시킬 수 있으며, 

- 새로운 정책 $\pi'$ 로 새롭게 $v_{\pi'}$ 를 <u>평가</u>할 수 있다. 

- 위의 과정을 반복하여 최정 정책을 갖는 것을 **정책 반복(Policy Itration)** 이라 한다.
  
  > $\pi_0 \rightarrow^E v_{\pi_0} \rightarrow^I \pi_1 \rightarrow^E ... \rightarrow^I \pi_* \rightarrow^E v_* $
  > 
  > $\rightarrow^E$ : 정책 평가(Evaluation)
  > 
  > $\rightarrow^I$ : 정책 향상(Improvement) 



##### 가치 반복

- 정책 반복의 단점은 정책 반복의 주기마다 정책 평가를 수행한다는 것이다. (계산 양이 많다)

- 정책 반복 과정에서 <u>정책 평가 단계의 반복적 계산 과정을 중간에 멈출 수 있는 방법은 여러가지가 있다</u>. 
  
  - 더불어 계산 과정이 중간에 중단된다 하더라도 정책 반복의 수렴성은 보장된다. 
  
  - 중요히 다뤄야 할 특수한 경우는 **가치 반복(Value Iteration)** 이다.
  
  > 가치 반복 : 정책 평가의 계산 과정이 오직 한 번의 일괄 계산 이후에 중단되는 것 
  > 
  > $v_{k+1}(s) = max_aE[R_{t+1} + \gamma v_k(S_{t+1})|S_t=s, A_t=a]$
  > 
  >                 $= max_a \sum_{s',r} p(s', r|s,a) [r + \gamma v_k(s')]$



- 정책 평가와 마찬가지로, 가치 반복이 정확히 $v_*$로 수렴하기 위해서 형식적으로 무한 번의 반복이 필요하다. 
  
  - 실제 적용 간에는 가치 함수의 변화가 아주 작은 값 이내로 들어오면 반복을 멈춘다. 



- 가치 반복은 매번의 일괄 계산 과정에서 정책 평가의 일괄 계산과 정책 향상의 일괄 계산을 효과적으로 결합한다. 
  
  - 정책 향상의 일괄 계산 사이사이에 정책 평가의 일괄 계산을 여러 번 삽입함으로써 좀 더 빨리 수렴하게 할 수 있다.
