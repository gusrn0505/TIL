### Chapter 5 : 몬테카를로 방법(Monte Carlo)

출처 : 단단한 강화학습 - Part 1 표 형태의 해법 속 챕터 5



##### 서론

- **몬테카를로 방법(이하 MC)은 오로지 경험만을 필요로 한다.**
  
  - MC는 <u>표본 이득의 평균값</u>을 기반으로 강화학습 문제를 푸는 방법이다. 
  
  > 경험 : 환경과의 상호작용으로부터 발생한 상태, 행동, 보상의 표본을 나열한 것

- **실제 경험, 또는 실제를 모사한 경험으로부터 학습하는 것**은 <mark>환경의 동역학에 대한 사전 지식 없이도 최적 행동을 할 수 있다는 점</mark>에서 매력적이다. 

- 이번 챕터에선 오직 에피소딕 문제에 한정하며, 오직 하나의 에피소드가 완료된 후에만 가치 추정값과 정책이 변화한다 가정한다. 
  
  - 따라서 MC는 실시간 관점이 아니라, 에피소드에서 에피소드로 진행되는 관점에서 점증적일 수 있다. 

- 이 책에서 MC는 <u>온전한 이득에 대한 평균값을 기반으로 하는 방법</u>만을 의미하겠다.  
  
  - MC는 폭넓게 사용되어 상당한 무작위성을 포함하는 추정 과정을 나타내곤 한다.



##### 몬테카를로 예측

- **MC 기본 개념**
  
  - **어떤 상태의 가치**는 <u>그 상태를 시작점으로 하여 계산된 이득의 기댓값</u>이다. 
    
    - 따라서 경험으로부터 상태의 가치를 추정하는 방법은, 그 상태 이후에 관측되는 모든 이득에 대해 평균을 계산하는 것이다. 
    
    - 더 많은 이득이 관측됨에 따라 그 평균값은 기댓값으로 수렴해야 한다. 



- 용어 
  
  > 접촉(visit) : 한 에피소드에서 상태 s가 발생할 때, s 와의 접촉이라 부르다 
  > 
  > 최초 접촉 : 한 에피소드에서 s를 처음 마주치는 것 



- **주요 MC 방법** 
  
  - 최초 접촉 MC 방법(first-visit MC method) : s와의 최초 접촉 이후에 발생하는 이득의 평균을 구함으로써 $v_\pi(s)$를 구한다. 
  
  - 모든 접촉 MC 방법(every-visit MC method) : s와의 모든 접촉 이후에 발생하는 이득의 평균을 구함으로써 $v_\pi(s)$를 구한다. 
  
  - => 이 장에서는 좀더 단순한 최초 접촉 MC 방법에 초점을 잡는다. 



- **특징**
  
  - 최초 접촉 / 모든 접촉 MC 방법 모두 큰 수의 법칙에 따라 접촉의 개수가 무한으로 갈수록 $v_\pi(s)$ 로 수렴한다.
  
  - 가치의 추정값은 2차 곡선의 변화율과 유사한 속도로 $v_\pi(s)$를 향해 수렴한다. 
    
    - 자세한 내용은 Singh and Sutton 의 1996 년 논문 확인할 것 
  
  - 동역학을 모르고 있어도 적용할 수 있으며, 알아도 상당한 도움이 될 수 있다. 
    
    - DP를 적용할 경우 동역학을 미리 알고 있어야 하며, 계산또한 복잡하며 오류가 있을 확률이 높다. 
  
  - 각 상태에 대한 추정이 서로 독립적이며, 계산 비용 또한 상태의 개수와 상관이 없다. 
    
    - 하나의 상태에 대한 추정은 다른 어떤 상태에 대한 추정으로부터도 영향을 받지 않는다. 



- <mark>**DP 대비 MC 장점**</mark>
  
  - (1) 환경에 대한 동역학 모델 없이 환경과 직접적으로 상호작용하여 최적 행동을 학습하는데 사용될 수 있다. 
  
  - (2) 시뮬레이션 또는 표본 모델(sample model)과 함께 사용될 수 있다. 
  
  - (3) 상태 집합의 작은 부분집합에 대한 몬테카를로 방법에 초점을 두는 것은 쉬우면서도 효율적이다. 
  
  - +마르코프 특성에 위배되는 상황에서는 MC 방법이 다른 방법에 비해 더 강인하다. 



##### 몬테카를로 행동 가치 추정

- 모델을 활용할 수 없다면 상태 가치 대신에 행동 가치를 추정하는 것이 특히 유용하다. 
  
  - 모델이 있다면 상태 가치만으로 정책을 결정하는 데 충분하다. 
  
  - 모델이 이 없다면 각 행동의 가치를 분명하게 추정해야, 유의미한 정책을 제안할 수 있다. 



- **행동 가치에 대한 정책 평가 문제**는 <u>'초기 상태 s에서 행동 a를 선택하고 이후에는 정책 $\pi$를 따를 때 얻는 이득의 기댓값 $q_\pi(s,a)$를 추정하는 것'</u>이다
  
  - MC 방법은 상태-행동 쌍을 마주치는 것에 대해 다룬다. (s에서 a가 선택되는 상황)
    
    - 최초 접촉 MC 방법 : 상태를 마주치고 행동을 선택했던 최초의 선택 이후에 발생하는 이득의 평균값을 계산한다.
    
    - 모든 접촉 MC 방법 : 상태 - 행동 쌍에 대한 모든 접촉 이후에 따라왔을 이득의 평균값으로서 상태-행동 쌍의 가치를 추정한다. 



- 단, 서로 다른 행동들을 비교하기 위해서는 <u>현재 선호하는 행동만이 아니라, 각 상태로부터 선택할 수 있는 모든 행동의 가치를 추정할 필요</u>가 있다. **(탐험 유지의 필요성)**
  
  - 많은 상태-행동 쌍에 대해 접촉이 발생하지 않을 수 있어 조치가 필요하다.
  
  - 이를 위한 하나의 방법은 <mark>시작 탐험</mark>의 가정을 하는 것이다. 
    
    > 시작 탐험의 가정 : 에피소드가 하나의 상태-행동 쌍에서 시작하고, 모든 상태-행동 쌍이 에피소드가 시작하는 쌍으로서 선택될 0이 아닌 확률을 갖는다. 
    > 
    > 몬테카를로 ES(Exploring starts) : 시작 탐험의 가정을 한 MC 방법  
  
  - 하지만 실제 환경과의 상호작용으로부터 학습하는 경우 등 시작 탐험의 가정을 적용할 수 없을 때가 많아 <u>대안적 방법이 필요</u>하다. 



##### 몬테카를로 제어

- MC 방법을 통해 최적 정책을 근사하는 것은 일반화된 정책 반복(-GPI) 개념을 따른다. 



- 정책이 현재 가치 함수에 대해 탐욕적이 되도록 함으로써 정책 향상을 수행할 수 있다. 
  
  - 이 경우 행동 가치 함수가 있기 때문에 탐욕적 정책을 만드는 데 있어 모델이 필요하지 않다. 
    
    > $\pi(s) = argmax_a q(s,a)$
    
    > $q_{\pi_k}(s, \pi_{k+1}(s)) = q_{\pi_k}(s, argmax_a q_{\pi_k}(s,a)) $
    > 
    >                               $= max_a q_{\pi_k}(s,a)$
    > 
    >                               $>= q_{\pi_k}(s,\pi_k(s))$
    > 
    >                                $>= v_{\pi_k}(s)$                    <mark>[정책 향상 정리]</mark>
  
  - Q. 행동 가치 함수가 없다면? 
  
  - 위 정리는 전체 과정이 최적 정책과 최적 가치 함수로 수렴한다는 것을 보장한다. 
    
    - 또한 동역학에 대한 정보 없이, 표본 샘플만 있어도 최적 정책을 찾을 수 있음을 보장한다. 



- MC의 수렴성을 쉽게 보장받기 위해 <mark>2가지 가정</mark>이 들어간다. 
  
  1. **에피소드가 시작 탐험을 한다.**
     
     - 활성 정책 / 비활성 정책을 적용하여 가정을 없앨 수 있다. 
  
  2. **정책 평가가 무한 개의 에피소드에 대해 행해질 수 있다.**
     
     - 샘플의 무한성은 아래의 2가지 방법으로 해결할 수 있다. 
       
       - (1) 각각의 정책 평가에서 $q_{\pi_k}$를 <u>근사</u>하는 개념을 고수한다.
       
       - (2) 정책 향상으로 가기 전에 <u>정책 평가를 완료하려는 노력을 포기</u>한다. (ex- 가치 반복)
         
         - 매번의 평가 단계에서 가치 함수는 $q_{\pi_k}$를 '향해' 움직이지만, 바로 수렴하지는 않는다. 
     
     - Q. 정말 2개뿐일까? 



----

#### 시작 탐험 없는 몬테카를로 제어

- 모든 행동을 보장하기 위한 일반적인 방법은 **정책을 통해 행동을 선택하게 하는 것**이다. 
  
  > 활성 정책(on-policy) :  결정을 내리는 데 사용하는 정책을 평가하고 향상시킴
  > 
  > 비활성 정책(off-policy) : 자료를 생성하는 데 사용되지 않는 정책을 평가하고 향상시킴



- **활성 정책 제어 방법**에서는 일반적으로 <mark>부드러운(soft) 성질</mark>을 가진다. 
  
  > 부드러움(soft) : 모든 $s \in S$와 모든 $a \in A(s)$ 에 대해 $\pi(a|s) >0$ 을 마족하지만 결정론적인 최적 정책에 가깝게 이동한다.
  > 
  > ex)- Epsilon-greedy 정책 
  
  - $q_\pi$ 에 대한 입실론 탐욕적 정책은 정책 향상 정리를 통해 정책 향상을 보장한다. 
    
    > <mark>$q_\pi(s, \pi'(s)) $</mark>
    > 
    > $= \sum_a \pi'(a|s)q_\pi(s,a)$ 
    > 
    > $= \frac{\epsilon}{|A(s)|} \sum_aq_\pi(s,a) + (1-\epsilon)max_aq_\pi(s,a)$
    > 
    > $>= \frac{\epsilon}{|A(s)|} \sum_a(q_\pi(s,a) + (1-\epsilon) \sum_a\frac{\pi(a|s) - \frac{\epsilon}{|A(s)|}}{1-\epsilon}q_\pi(s,a) $
    > 
    > - *마지막 부분의 $\sum_a$ 은 총합이 1이 되는 음이 아닌 가중치를 적용한 평균*
    > 
    > $= \frac{\epsilon}{|A(s)|} \sum_aq_\pi(s,a) - \frac{\epsilon}{|A(s)|} \sum_aq_\pi(s,a) + \sum_a\pi(a|s)q_\pi(s,a)$
    > 
    > <mark>= $v_\pi(s)$</mark> $for \forall s \in S$
  
  - 입실론 소프트 정책 중에서는 최선의 정책이 발견되는 경우를 제외하곤 매 단계에서 정책이 향상되고 있다. 
    
    - 단, 이러한 분석은 매 단계에서 행동 가치 함수가 정확히 계산된다는 가정이 전제되어야 한다. 

---

#### 중요도 추출법을 통한 비활성 정책 예측

- 모든 학습 제어 방법은 '어떻게 탐험적 정책에 따라 행동하면서 최적 정책을 학습할 수 있을까' 에 대한 갈등에 부딪힌다. 

- Epsilon-greedy 또한 이에 대한 절충안이나, 좀 더 명확한 접근법은 <u>2가지 정책(목표 정책, 행동 정책)을 모두 사용하는 것</u>이다 **(비활성 정책 학습).** 
  
  > 목표 정책($\pi$) : 학습 대상이자 최적 정책
  > 
  > 행동 정책(b) : 좀 더 탐험적이고 행동을 생성하기 위해 사용되는 정책 



- **특징** 
  
  - 데이터가 서로 다른 정책으로부터 비롯되기 때문에 분산이 더 크고 수렴 속도가 더 느리다
  
  - 반면 좀 더 강력하며 더 일반적이다. 
  
  - 실제 적용하는데 활용도가 높다. 



- 정책 b로부터 에피소드를 활용하여 정책 $\pi$의 가치를 추정하기 위해서는 정책 $\pi$하에서 취해지는 모든 행동이 이따금씩 정책 b 하에서 취해질 필요가 있다. **(보증의 가정)**
  
  > $\pi(a|s)>0 $ 이면, $b(a|s)>0 $ 또한 성립해야 한다. 
  > 
  > *MC 방법에선 상태-행동 쌍으로 계산하기 때문*
  
  - 보증의 가정에 따르면 정책 b가 정책 $\pi$ 와 동일하지 않은 상태에서 정책 b는 틀림없이 확률론적이다. 반면에 목표 정책 $\pi$는 결정론적일 수 있다. 
    
    > *정책 b는 정책 $\pi$ 의 행동 값과 그 외의 행동 모두 포괄해야 하기 때문* 
  
  

- 거의 모든 비활성 정책은 **중요도 추출비율**에 따라 <u>이득에 가중치를 부여하는 방식</u>을 적용한다.
  
  > 중요도 추출법 : A 분포로 부터 얻어진 표본을 기반으로 B 분포에서의 기댓값을 추정하는 일반적인 방법  
  
  > 중요도 추출비율($\rho_{t:T-1}$) : 목표 정책과 행동 정책 하에서 발생하는 상태-행동의 궤적에 대한 상대적 확률 
  > 
  > $\rho$ : 상태 전이 확률 함수 
  
  - 중요도 추출 비율은 결과적으로 <u>두 정책과 그들의 나열에만 영향을 받고 MDP에는 영향을 받지 않는다</u>.
    
    - 비록 상태-궤적의 확률이 일반적으로 그 값을 알지 못하는 MDP의 전이 확률에 따라 결정되지만, 그 확률이 분모와 분자에 동일하게 나타나기 때문에 약분되어 사라진다.
    
    - 중요도 추출비율 계산 과정 
      
      > $\rho_{t:T-1} = \frac{\prod^{T-1}_{k=t} \pi(A_k|S_k) p(S_{k+1}|S_k, A_k)}{\prod^{T-1}_{k=t} b(A_k|S_k) p(S*{k+1}|S_k, A_k)} = \prod^{T-1}_{k=t} \frac{\pi(A_k|S_k)}{b(A_k|S_k)}$
      > 
      > $Pr${$A_t, S_{t+1}, A_{t+1}, ..., S_T | S_t, A_{t=T-1}$~$\pi$} 
      > 
      > $= \pi(A_t|S_t)p(S_{t+1}|S_t,A_t)\pi(A_{t+1}|S_{t+1}) ... p(S_T|S_{T-1}, A_{T-1})$ 
      > 
      > $= \prod^{T-1}_{k=t}\pi(A_k|S_k)p(S_{k+1}|S_k, A_k)$
  
  - 중요도 추출 비율은 행동 정책(b)으로부터 만들어진 이득 $G_t$을 목표 정책($\pi$) 하의 이득의 기댓값을 추정하는 데 도움을 준다. 
    
    > $v_b(s) = E[G_t|S_t=s]$ 는 $v_\pi$ 에 근사하지 않는다. 
    > 
    > 반면, $E[\rho_{t:T-1} G_t|S_t=s] $ 는 $v_\pi$ 에 근사한다.   



- 에피소드 방식의 상태 가치 함수는 **기본 / 가중치 중요도 추출법**에 따라 계산할 수 있다. 
  
  > 기본 중요도 추출법 : $V(s) = \frac{\sum_{t \in \tau(s)} \rho_{t:T(t)-1} G_t} {|\tau(s)}$
  > 
  > 가중치 중요도 추출법 : $V(s) = \frac{\sum_{t \in \tau(s)}{\rho_{t:T(t) -1}}_{t \in \tau(s)} G_t}{\sum_{t \in \tau(s)}{\rho_{t:T(t) -1}}_{t \in \tau(s)}}$
  > 
  > > $\tau(s)$ : 상태 s와 마주치는 모든 시간 단계의 집합
  > > 
  > > $T(t)$ : 시각 t 이후에 나타나는 최초의 종단 시각 
  > > 
  > > $G_t$ : 시각 t부터 $T(t)$까지의 이득 
  > > 
  > > $\{G_t\}_{t\in\tau(s)}$ : 상태 s와 관련된 이득 
  > > 
  > > $\{\rho_{t:T(t) -1}\}_{t \in \tau(s)}$ : 상태 s와 관련된 중요도 추출비율 
  
  - 기본 가중치 중요도 추출법은 항상 기댓값이 <u>편차없이 $v_\pi(s)$</u>지만 극단적인 값이 될 수 있다. 
    
    - $\rho$ 가 10이라고 할 때, 추정값은 관측된 이득의 10배가 될 것이다. 
    
    - 즉, 에피소드의 상태-행동 궤적이 목표 정책을 매우 잘 대표하는 것으로 생각될지라도 <mark>이득의 추정값과 관측된 이득 사이에는 매우 큰 차이가 있을 것</mark>이다. 
  
  - 가중치 중요도 추출법은, 단일 이득에 대한 비율 ${\rho_{t:T(t) -1}}$  이 분자와 분모에서 약분되어 추정값($v_b(s)$)은 <u>편차가 있으나</u> 관측된 이득과 같아진다. 
    
    - 기본 가중치와 다르게 단일 이득에 부여되는 최대 가중치는 1로, <u>추정치의 분산은 0으로 수렴</u>한다. 
    
    - **=> 분산의 크기가 작아 가중치를 적용한 추정 방법이 인기가 많다.**



---

##### 점증적 구현

- MC 방법은 에피소드가 단계적으로 진행됨에 따라 점증적으로 구현할 수 있다. 
  
  - MC 방법에는 이득을 평균 낸다. 
  
  - 활성 정책은 앞서 진행한 점증적 구현과 방법이 동일하다. 
  
  - 비활성 정책 MC의 경우, 기본 / 가중치 중요도 추출법을 구분하여 생각해야 한다.



- 가중치 중요도 추출법 점증적 구현 
  
  > let $V_n = \frac{\sum^{n-1}_{k=1} W_k G_k}{\sum^{n-1}_{k=1}W_k}$
  > 
  > $V_{n+1} =  \frac{\sum^{n-1}*{k=1} W_k G_k}{\sum^{n-1}*{k=1}W_k} $
  > 
  >            $= \frac{1}{C_n} (C_{n-1} V_n + W_nG_n)$
  > 
  >             $= (1-\frac{W_n}{C_n})V_n + \frac{W_n}{C_n} G_n$
  > 
  >             $=V_n + \frac{W_n}{C_n}[G_n-V_n]$
  > 
  > > let $C_n = \sum^n_{k=1} W_n$ 



---

##### 비활성 몬테카를로 제어

- 활성 정책은 1) 정책의 가치를 추정하면서, 2) 동시에 그것을 제어에 활용한다.

- 반면 비활성 몬테카를로 정책은 이 두가지 기능이 분리되어 있따. 
  
  - 행동을 생성하기 위해 사용되는 행동 정책은 목표 정책과는 아무런 관계가 없을 수 있다. 
  
  - 이러한 분리의 좋은 점은 <u>행동 정책이 모든 가능한 행동에 대해 표본을 추출하는 반면, 목표 정책은 결정론적일 수 있다</u>는 것이다. 
    
    - *나중에 목표 정책의 비중을 100%로 하면 손실이 없겠네*



- 비활성 MC 정책은은 <u>목표 정책에 대해 학습하고 향상시키는 동안 행동 정책을 따른다.</u>
  
  - 이를 위해 모든 행동이 행동 정책에 의해서도 o이 아닌 확률로 선택되야 한다. 
  
  - 즉, 모든 가능성을 탐험하기 위해 <u>행동 정책은 부드러워야 한다.  </u>



- 행동 정책 b는 어떠한 정책이든 될 수 있지만, 정책 $\pi$가 최적 정책으로 수렴함을 보장하기 위해서는 상태와 행동의 각 쌍에 대해 무한대의 이득이 얻어져야 한다. 
  
  - Why? 
  
  - 이것은 정책 b를 입실론 소프트가 되도록 선택함으로써 보장된다. 
    
    

- 이 방법의 잠재적인 문제는 이 방법이 오직 에피소드의 잔여물(ex- $\epsilon)$로부터 학습한다는 것이다. 
  
  - 에피소드의 남아 있는 모든 행동이 탐욕적일 때, 비탐욕적인 행동이 많을수록 학습 속도는 느릴 것이다. 
  
  - 그 결과 학습을 크게 지연시킬 수 있다. 
  
  - => 이 방법을 해결하기 위한 방법중 하나는 <u>시간차 학습을 도입</u>하는 것이다. 






