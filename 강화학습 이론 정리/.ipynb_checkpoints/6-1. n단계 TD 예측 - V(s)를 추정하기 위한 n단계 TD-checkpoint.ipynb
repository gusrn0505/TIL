{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4118eb",
   "metadata": {},
   "source": [
    "### <$V_\\pi$를 추정하기 위한 n단계 TD> \n",
    "\n",
    "입력 : 정책 $\\pi$ \n",
    "\n",
    "알고리즘 파라미터 : 시간 간격 $\\alpha \\in (0,1] $, 양의 정수 $\\alpha$  \n",
    "\n",
    "모든 $s \\in S$ 에 대해 V(s)를 임의의 값으로 초기화 \n",
    "\n",
    "($S_t$와 $R_t$에 대한) 모든 저장과 접근은 n+1로 인텍스를 설정하여 행해질 수 있음. \n",
    "\n",
    "각 에피소드에 대한 루프 : \n",
    "- $S_0$를 초기화하고 저장(단, $S_0 \\neq$ 종단) \n",
    "- $ T <- $ 무한대\n",
    "- t= 0,1,2, ... 에 대한 루프 : \n",
    "\n",
    "> t < T 이면 : \n",
    " > -  $\\pi(.|S_t)$에 따라 행동을 취함\n",
    " > - 다음 보상을 $R_{t+1}$, 다음 상태를 $S_{t+1}$로 측정하고 저장함 \n",
    " > - $S_{t+1}$ 이 종단이면, T <- t+1 \n",
    " \n",
    "> r <- t-n+1(r은 상태의 추정값이 갱신되는 시각) \n",
    "\n",
    "> R>=0 이면 : \n",
    "> - G <- $\\sum^{min(r+n,T)}_{i = r+1} \\gamma^{i-r-1} R_i$ \n",
    "> - r+n < T 이면, G <- $G+ \\gamma^n V(S_{r+n})$\n",
    "> - $V(S_r)$ <- $V(S_r) + \\alpha [G-V(S_r]$ \n",
    "\n",
    "- r = T-1 이면 종료\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e154776c",
   "metadata": {},
   "source": [
    "**<구현해야 하는 것>**\n",
    "- V(S) : 상태 s에 대한 가치 함수 계산. 챕터 4의 코드 참고 \n",
    "- pi(s) : 상태 s에 따른 행동들의 확률 값들을 list로 반환 \n",
    "- return(s', a, s) : R 값 반환 \n",
    "\n",
    "- next_S(s,a) : 상태 s와 행동 a를 했을 때의 s'. \n",
    "- make_episode : 챕터 4의 코드 참고\n",
    "\n",
    "**<필요한 것>** \n",
    "- $\\alpha$ : class 제작시 입력 값으로 부여 \n",
    "- n : 몇단계 TD를 할 것인가? \n",
    "\n",
    "\n",
    "**<함수 / 데이터 형식>** \n",
    "- class evaluate_v_n_TD : a# alpha 값 추가 \n",
    "> def __init__(self, S, A, n, pi, alpha, reward_func, epsilon = 0.001, gamma = 0.9, num_episode = 10, len_episode = 20) : \n",
    "\n",
    "\n",
    "**<외부함수>** \n",
    "- $\\pi(s,a) $ : 상태 s 하에서 선택할 a. 확률론적으로 구현할 것  \n",
    "- R(s',a,s) : 보상함수\n",
    "\n",
    "\n",
    "**<고민점>** \n",
    "- \"($S_t$와 $R_t$에 대한) 모든 저장과 접근은 n+1로 인텍스를 설정하여 행해질 수 있음.\" 을 어떻게 구현할 것인가? \n",
    "> t=0 부터 n까지의 사례를 다루겠다는 것으로 보여짐. make_episode의 구조와 까지 dic의 value 값들을 list로 설정한 것으로 해결가능할 듯.\n",
    "\n",
    "- r과 n의 값이 의미하는 것은 무엇인가? \n",
    "> r 이 양수이면 갱신가능 의미. n은 n단계 TD의 n을 의미함 . \n",
    "\n",
    "- r <- 't-n+1' 인 이유는? 그리고 r >=0 일때만 값들을 갱신하는 이유는 무엇인가? \n",
    "> 갱신 가능 여부를 확인하는 것. n단계 TD에서 적어도 +n 단계의 정보가 필요함. \n",
    "\n",
    "- $S_{t+1}$ 이 종단이다는 어떻게 표현할 것인가? \n",
    "> make_episode 함수를 통해서 길이를 미리 정해줌. 이를 통해서 확인 가능할 것 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82efc2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 용 임시 데이터 \n",
    "S = list(range(100)) \n",
    "A = list(range(-5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b584850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "553bc2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외부용 함수 reward_func 간략 구현 (이전 예시 활용)\n",
    "def reward_func(next_s, a, s) : \n",
    "    # next_s 와 s의 차이가 짝수이면 +1, 홀수면 -1 \n",
    "    # 단, a의 크기에 반비례함. \n",
    "    if abs(next_s - s) %2 == 0 : reward = 1 \n",
    "    else : reward = -1\n",
    "    \n",
    "    if a == 0 : \n",
    "        return 0 \n",
    "    else : \n",
    "        return reward / a # 즉, a가 양수이며 짝수이며, 가능한 작을 때 (=2) 일 때 최대의 보상이 주어지도록 설정 \n",
    "    \n",
    "    \n",
    "def pi(s,a) : #s 상황에서 a를 선택할 확률. 전체 합은 1이여야 한다. \n",
    "    # 확률은 모두 동일하게 설정 \n",
    "    \n",
    "    return 1/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bf3fbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간차 학습 - 가치 상태 추정 TD(0) 코드 참고 \n",
    "\n",
    "class evaluate_n_TD :  \n",
    "    def __init__(self, S, A, n, pi, reward_func, alpha=0.1,  epsilon = 0.001, gamma = 0.9, num_episode = 10, len_episode = 20) : \n",
    "        self.S = S \n",
    "        self.A = A \n",
    "        self.n = n \n",
    "        self.pi = pi \n",
    "        self.reward_func = reward_func\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon \n",
    "        self.gamma = gamma \n",
    "        self.num_episode = num_episode\n",
    "        self.T = len_episode\n",
    "\n",
    "#        self.b, self.C = self.initiate_b()\n",
    "#        self.Q, self.return_lst, self.s_prob = self.initiate() \n",
    "        self.V = self.initiate_V() \n",
    "    \n",
    "    def initiate_V(self) : # V 값을 초기화 \n",
    "        return [0]*len(self.S)\n",
    "    \n",
    "\n",
    "    def choice_action(self, s, policy) : #수정 - 일반화. 정책 기반으로 상황 s에 있을 때 선택할 행동 a 산출 \n",
    "        policy_a_list = [] \n",
    "        for _ in self.A :\n",
    "            policy_a_list.append(policy(s,_)) \n",
    "\n",
    "        a= random.choices(self.A, weights = policy_a_list)\n",
    "        a = a[0]\n",
    "        return a\n",
    "    \n",
    "    def next_s(self, s,a) : # 상태 s에서 a 행동을 했을 때 다음 상태 s'. 정책, S,A 에 따라 달라짐. \n",
    "        return min(max(s+a, 0), max(self.S)) \n",
    "\n",
    "\n",
    "    def make_episode(self, start_s, T) :\n",
    "        s = start_s\n",
    "        episode = {\"S\" : [], \"A\" : [], \"R\" : []} # R은 0~T 까지,  총 T+1 경우가 있음. \n",
    "        episode[\"R\"].append(0) # R_0 값 부여\n",
    "        episode[\"S\"].append(start_s) # 추가.S_0 값 부여. 이로 인해서 추가 코드 수정 필요. \n",
    "        for _ in range(T) : \n",
    "            episode[\"S\"].append(s)\n",
    "            a = self.choice_action(s, self.pi) \n",
    "            next_s = self.next_s(s, a)\n",
    "            r = self.reward_func(next_s, a, s)\n",
    "            episode[\"A\"].append(a)\n",
    "            episode[\"R\"].append(r)\n",
    "            s = next_s\n",
    "        return episode \n",
    " \n",
    "    def update_returns(self) : # TD(0) 형식에 맞춰 대거 수정 \n",
    "        \n",
    "        for _ in range(self.num_episode) : \n",
    "            start_s = random.choice(self.S) \n",
    "            T = self.T # 어차피 T는 self.T로 갱신되기 때문에 앞에서 설정함. \n",
    "            episode = self.make_episode(start_s, self.T) \n",
    "            S,R,A = episode['S'],episode['R'], episode['A'] \n",
    "            \n",
    "            for t in range(self.T + self.n +1) : # t는 아무리 커도 self.T + n 보다 커질 수 없기 때문 \n",
    "                # make_episode 에서 이미 a,r를 계산해 두었기 때문에 V(s)만 갱신하겠음. \n",
    "                \n",
    "                # r 또한 현재 t와 인덱스로 사용되는 n이 중복되기 때문에, t값이 T 값일 때 순환문을 종료하는 것으로 \n",
    "                \n",
    "                r = t - self.n +1 \n",
    "                if r >= 0 : \n",
    "                    G = sum([self.gamma **(i-r-1) * R[i] for i in range(r+1, min(r+self.n, self.T)+1)])\n",
    "                    if r+self.n < self.T : G = G + self.gamma**self.n * self.V[S[r+self.n]]\n",
    "                    self.V[S[r]] = self.V[S[r]] + self.alpha * (G - self.V[S[r]])\n",
    "                if r == self.T -1 : break                  \n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7544362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.29540680525, -0.4121530145643473, 0, -0.3303780521045176, -0.23634118278961452, -0.06992652500000002, -0.30675673369693623, 0, -0.07060400857000002, 0.030412589499999997, -0.052805000000000005, -0.04903995912216452, 0, -0.1, 0, -0.022833824450000005, 0, -0.065, -0.15045486800000002, -0.27487150000000005, 0, 0, 0.00802846533333333, 0, -0.09137150000000002, -0.19117161145250006, -0.252500439922, -0.22372346746225003, -0.05178167473000002, 0, -0.16123646505500003, -0.08112533492254051, 0.006981999999999994, 0.012654197666535952, -0.10822244313333335, -0.00581590077570001, -0.142, 0, -0.12190991738666668, -0.09547202482000001, 0.05755720928, 0.3098353276293522, 0.015766697475000015, 0.0795843284876667, 0.06763117703744413, -0.07545959741733133, 0.10658997995808334, -0.01107707166977872, 0.014796881507919801, 0, 0.21353937424959973, 0.023261219335833305, 0, 0.011131174906467525, -0.14853563397347005, 0.04316359272500001, 0, -0.0892641571370802, 0.14547370424489037, 0, 0, 0, -0.015206550370950023, 0.061016656533333324, 0, -0.028923562854283352, 0.20477931361568263, 0.08605535385980206, 0.21679290657980207, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1, -0.05899999999999999, -0.17301362710673168, -0.27448761207370004, -0.3883956305180255, -0.10091230739012169, -0.17544868949501907, 0.11904155397981231, -0.38882958486269653, -0.039971622168517684, -0.1914784836530871, -0.11833610596322629, 0.1350493, -0.056667850782484076, -0.026240090367804114, 0.01887609565270864, 0.08377063963627168, -0.023474122488865462, 0.157255078025, 0.14282013002500002, 0.06852489500000002]\n",
      "[-1.2127420565018852, -0.9052505206068049, -0.8827630487844542, -0.4557894787357969, -0.9634386269102545, -0.46385566261347794, -1.1323475444455324, -0.27447303662029515, -0.6069028076578896, -1.0118010276418812, -0.29554225936803474, -0.2407141546594761, -0.3402048605475125, 0.11575048973966226, -0.40230860049122497, -0.304664101606516, -0.09014365145820125, -0.15812953673317737, 0.28231030718331823, 0.30668325123742546, 0.3057211870815947, 0.09239083552193764, -0.25922107448037723, -0.03716470433680569, 0.2410606304269426, 0.1608701278941734, -0.591563205211912, -0.2507608702739431, 0.2061154953417335, 0.29967782395355835, -0.20420163279283032, 0.05867180593106219, -0.18246820584022366, 0.4234278163216537, 0.12738240743208895, 1.0081380612922695, 0.5593462509266468, 0.7683263316410269, 0.35549775128803796, 0.29939383592684343, 0.024755703124227577, 0.3494131837713843, 0.460889113660274, 0.27827932617957996, 0.29275728188118055, 0.4035460048183818, 0.22937318863138875, 0.33241694642521324, 0.5682896080910517, -0.3119786568316568, 0.6261851029797292, 0.7907893870725519, 0.5243514819381955, -0.30793007296688857, 0.2531026874237707, -0.0540009843532803, -0.20977668666816118, 0.03774644076903502, -0.24592115866393782, -0.2778434056397147, 0.2774783612548697, -0.228760899310879, 0.17965755349000134, -0.060696316987552346, 0.41427583514257865, 0.40524730548152943, 0.20816533320567895, 0.2157554453327272, 0.5221954365359336, 0.4607481749207308, 0.6191533577788502, 0.12156608505511347, 0.07337473270478889, 0.4131056587203169, -0.23126984576687432, -0.23976447156561506, 0.1659429852410231, -0.29913336445013416, -0.2908138652019057, 0.12188617161733414, 0.22067500934783152, -0.1012618819789565, -0.6427273777324355, 0.01255950586166987, 0.1370774408850859, 0.08712904696585903, 0.05264828633776388, 0.3409357393460559, -0.028486957580567226, 0.04357212446220793, -0.17674907698283093, -0.002534348019617355, 0.1006027129784182, -0.3880716395138176, -0.3362514176772472, 0.3265373517153477, 0.1897665074886949, 0.03651277332973614, 0.09359953897308361, 0.749126161048425]\n"
     ]
    }
   ],
   "source": [
    "test = evaluate_n_TD(S,A, 10, pi, reward_func)\n",
    "test.update_returns() \n",
    "print(test.V)\n",
    "\n",
    "for _ in range(10) : test.update_returns()  \n",
    "print(test.V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b0b1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40808c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddc8335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d3bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c073f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf68007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
