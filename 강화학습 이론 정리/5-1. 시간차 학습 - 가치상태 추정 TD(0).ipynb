{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06af560e",
   "metadata": {},
   "source": [
    "### <$v_\\pi$를 추정하기 위한 표 형태로 된 TD(0)>\n",
    "\n",
    "입력 : 평가 대상이 될 정책 $\\pi$\n",
    "\n",
    "알고리즘 파라미터 : 시간 간격 $\\alpha \\in (0,1] $ \n",
    "\n",
    "모든 $s \\in S^+$ 에 대한 V(s)를 임의의 값으로 초기화. 단, V(종단) = 0 \n",
    "\n",
    "각 에피소드에 대한 루프 : \n",
    "- S를 초기화 \n",
    "- 에피소드의 각 단계에 대한 루프 : \n",
    "\n",
    "> A ← S에 대해 $\\pi$에 따라 도출된 행동 \n",
    "\n",
    "> 행동 A를 취하고 $R,S'$ 을 관측 \n",
    "\n",
    "> V(s) ← $V(S) + \\alpha[R+ \\gamma V(S') - V(S)] $\n",
    "\n",
    "> S ← S' \n",
    "\n",
    "S가 종단이면 종료 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e42c33",
   "metadata": {},
   "source": [
    "**<구현해야 하는 것>**\n",
    "- V(S) : 상태 s에 대한 가치 함수 계산. 챕터 4의 코드 참고 \n",
    "- next_S(s,a) : 상태 s와 행동 a를 했을 때의 s'. \n",
    "- make_episode : 챕터 4의 코드 참고\n",
    "\n",
    "**<필요한 것>** \n",
    "- $\\alpha$ : class 제작시 입력 값으로 부여 \n",
    "\n",
    "\n",
    "**<함수 / 데이터 형식>** \n",
    "- class evaluate_TD : # alpha 값 추가 \n",
    "> def __init__(self, S, A, pi, alpha reward_func, epsilon = 0.001, gamma = 0.9, num_episode = 10, len_episode = 20) : \n",
    "\n",
    "\n",
    "**<외부함수>** \n",
    "- $\\pi(s,a) $ : 상태 s 하에서 선택할 a. 확률론적으로 구현할 것  \n",
    "- R(s',a,s) : 보상함수. 챕터 4의 코드 참고 \n",
    "\n",
    "\n",
    "**<고민점>** \n",
    "- V를 어떻게 임의의 값으로 초기화할 것인까? 단순히 0 값으로 부여하는 것보다 더 좋은 방법이 있나?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75543447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 용 임시 데이터 \n",
    "S = list(range(100)) \n",
    "A = list(range(-5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29d1a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13dd9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외부용 함수 reward_func 간략 구현 (이전 예시 활용)\n",
    "def reward_func(next_s, a, s) : \n",
    "    # next_s 와 s의 차이가 짝수이면 +1, 홀수면 -1 \n",
    "    # 단, a의 크기에 반비례함. \n",
    "    if abs(next_s - s) %2 == 0 : reward = 1 \n",
    "    else : reward = -1\n",
    "    \n",
    "    if a == 0 : \n",
    "        return 0 \n",
    "    else : \n",
    "        return reward / a # 즉, a가 양수이며 짝수이며, 가능한 작을 때 (=2) 일 때 최대의 보상이 주어지도록 설정 \n",
    "    \n",
    "    \n",
    "def pi(s,a) : #s 상황에서 a를 선택할 확률. 전체 합은 1이여야 한다. \n",
    "    # 확률은 모두 동일하게 설정 \n",
    "    \n",
    "    return 1/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f61c2f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비활성 MC 정책 코드 참고\n",
    "# 기존 코드를 전반적으로 일반화시킴\n",
    "\n",
    "class evaluate_TD :  \n",
    "    def __init__(self, S, A, pi, reward_func, alpha=0.1,  epsilon = 0.001, gamma = 0.9, num_episode = 10, len_episode = 20) : \n",
    "        self.S = S \n",
    "        self.A = A \n",
    "        self.pi = pi \n",
    "        self.reward_func = reward_func\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon \n",
    "        self.gamma = gamma \n",
    "        self.num_episode = num_episode\n",
    "        self.T = len_episode\n",
    "\n",
    "#        self.b, self.C = self.initiate_b()\n",
    "#        self.Q, self.return_lst, self.s_prob = self.initiate() \n",
    "        self.V = self.initiate_V() \n",
    "    \n",
    "    def initiate_V(self) : # V 값을 초기화 \n",
    "        return [0]*len(self.S)\n",
    "    \n",
    "\n",
    "    def choice_action(self, s, policy) : #수정 - 일반화. 정책 기반으로 상황 s에 있을 때 선택할 행동 a 산출 \n",
    "        policy_a_list = [] \n",
    "        for _ in self.A :\n",
    "            policy_a_list.append(policy(s,_)) \n",
    "\n",
    "        a= random.choices(self.A, weights = policy_a_list)\n",
    "        a = a[0]\n",
    "        return a\n",
    "    \n",
    "    def next_s(self, s,a) : # 상태 s에서 a 행동을 했을 때 다음 상태 s'. 정책, S,A 에 따라 달라짐. \n",
    "        return min(max(s+a, 0), max(self.S)) \n",
    "\n",
    "\n",
    "    def make_episode(self, start_s, T) :\n",
    "        s = start_s\n",
    "        episode = {\"S\" : [], \"A\" : [], \"R\" : []}\n",
    "        episode[\"R\"].append(0) # R_0 값 부여 \n",
    "        for _ in range(T) : \n",
    "            episode[\"S\"].append(s)\n",
    "            a = self.choice_action(s, self.pi) \n",
    "            next_s = self.next_s(s, a)\n",
    "            r = self.reward_func(next_s, a, s)\n",
    "            episode[\"A\"].append(a)\n",
    "            episode[\"R\"].append(r)\n",
    "            s = next_s\n",
    "        return episode \n",
    " \n",
    "    def update_returns(self) : # TD(0) 형식에 맞춰 대거 수정 \n",
    "        \n",
    "        for _ in range(self.num_episode) : \n",
    "            start_s = random.choice(self.S) # 시작 탐험 가정 \n",
    "            episode = self.make_episode(start_s, self.T) \n",
    "            S,R,A = episode['S'],episode['R'], episode['A'] \n",
    "            # make_episode 에서 이미 a,r를 계산해 두었기 때문에 V(s)만 갱신하겠음. \n",
    "            for index, s in enumerate(S[:-1]) :  \n",
    "                index_origin_s = self.S.index(s)\n",
    "                index_origin_next_s = self.S.index(S[index+1])\n",
    "                self.V[index_origin_s] = self.V[index_origin_s] + self.alpha*(R[index+1] +self.gamma * self.V[index_origin_next_s] - self.V[index_origin_s] )    \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6dba93a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-1.0493505055941106, -0.43038595743146524, -0.4169946350929829, -0.5877144751062008, -0.25694432747784957, -0.4720657832093836, -0.2639135784257529, -0.3220448175060952, -0.03826029381856259, -0.22811732853311537, 0.07114026421328888, -0.031585929277619405, 0.12330239095191457, 0.03155697016806558, -0.04166150204111681, 0.06276496680687738, 0.1576356438373627, 0.00908920916935058, 0.08607635605511552, 0.1787902954937421, 0.31653495890584277, 0.008272978683048843, 0.269545528176297, 0.006074211442367562, 0.019513235378787158, -0.0020004820785587962, -0.09530570428342128, -0.055898457718203676, 0.01328969478283342, -0.06999968850104638, 0.2569413505085248, 0.022524517793944633, 0.13145100505563587, 0.08114479024210318, 0.13250639921637195, 0.09794314940193949, 0.0835469083757119, -0.15459911139505914, 0.10228833684257299, 0.22230603439663119, 0.28734201883990723, 0.176379307736621, -0.05461817700105476, 0.19725094398441495, 0.0663278457088923, -0.008554181806395153, -0.14664944311963934, 0.37613322207859745, 0.06386108863701541, 0.09847954199144868, 0.17676831525442135, 0.25582087585566604, 0.2760229638676468, 0.19567273337564567, 0.14973128008552056, 0.05366068933849079, 0.2217911439528203, 0.09669979860683017, 0.3185104002291627, 0.3233807387382086, 0.2764603641077492, 0.10238183685631849, 0.01746696140027742, 0.42991996810616784, 0.14385279759522165, 0.18301381399846686, 0.2117402563988281, 0.07368596880383999, 0.12012399192801786, 0.21292601523811064, 0.1570391574648568, 0.3257494440747525, 0.06705098536097909, 0.3076403713699503, 0.07861893182905245, 0.09425734388415122, -0.047777532321393264, 0.14292643155951618, 0.3300284736534277, -0.1600476923120463, 0.01077261667219772, -0.022349839684858923, 0.11564029429566827, 0.04946838006039948, 0.3648054542049568, -0.04065402985940632, 0.3182666080370156, 0.31475577978207675, 0.17154568300394352, 0.10246607349861298, 0.3663320203515294, 0.450285367451816, 0.4457095583257094, 0.2851644041180493, 0.45572017921311686, 0.2766751136626687, 0.13011939582883864, 0.626521015737867, 0.14460028661262633, 0.5434692416973425]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\na = test.V\\ntest.update_returns()\\nb = test.V\\ntest.update_returns() \\nc = test.V\\n\\n위의 코드의 경우 a,b,c 모두에 test.V 라는 동일한 객체를 할당한 것임. \\n따라서 최초에 a의 값이 다른 것이었다 하더라도, test.V를 업데이트 하면 a,b,c 모든 값이 바뀌는 것임. \\n\\n즉, V의 값이 변하는 것을 따로 저장해두고 싶으면 test.V[:] 을 통해 값을 \"복사\" 해야한다. \\n\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = evaluate_TD(S,A, pi, reward_func)\n",
    "lst = [] \n",
    "\n",
    "for _ in range(50) : \n",
    "    lst.append(test.V[:])\n",
    "    test.update_returns()\n",
    "\n",
    "print(lst[0])\n",
    "print(lst[-1])\n",
    "\n",
    "\n",
    "\n",
    "# Q. 왜 업데이트를 한번 한 것과, 여러번 한 것의 결과가 같지? 이미 최적의 가치 함수라서 그런건가? \n",
    "# 값 할당 실수가 있었음. \n",
    "\n",
    "\"\"\"\n",
    "a = test.V\n",
    "test.update_returns()\n",
    "b = test.V\n",
    "test.update_returns() \n",
    "c = test.V\n",
    "\n",
    "위의 코드의 경우 a,b,c 모두에 test.V 라는 동일한 객체를 할당한 것임. \n",
    "따라서 최초에 a의 값이 다른 것이었다 하더라도, test.V를 업데이트 하면 a,b,c 모든 값이 바뀌는 것임. \n",
    "\n",
    "즉, V의 값이 변하는 것을 따로 저장해두고 싶으면 test.V[:] 을 통해 값을 \"복사\" 해야한다. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4e6ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3,4,5,6,7]\n",
    "\n",
    "print(a[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ed6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d97e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b9173e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194a9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9024c4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511af3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dcc579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d7380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
